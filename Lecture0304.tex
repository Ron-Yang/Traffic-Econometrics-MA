%#############################################
% New pdflatex format,
% only png, jpg (or svg?) images but positioning now ok
% template in ~/tex/inputs/template_folien.tex
% [TODO now specialized to Vkoem_Ma; make general]
%#############################################


%\documentclass[mathserif]{beamer}
\documentclass[mathserif,handout]{beamer}
%\usepackage{beamerthemeshadow}
\input{$HOME/tex/inputs/defsSkript}%$
\input{$HOME/tex/inputs/styleBeamerVkoekMa}%$
\usepackage{graphicx}


%##############################################################

\begin{document}

\section{3. Classical Inferential Statistics}

%###################################################
\frame{  %title other layout
%###################################################
\placebox{0.50}{0.50}{
  \figSimple{1.70\textwidth}{figsRegr/regression3dFig.png}}

\makePale{0.85}{0.50}{0.55}{1.20}{1.50}

\placebox{0.50}{0.46}{\parbox{0.6\textwidth}{
\bi
\item[3.1] Expectation and Covariance Matrix of the Ordinary Least
  Squares (OLS) Estimator
\item[3.2] Confidence Intervals
\item[4] Significance Tests
\item[4.1] General Four-Step Procedure
\bi
\item[4.1.1] Step 1: Chosing $H_0$: Type I and II errors
\item[4.1.2] Steps 2 and 3: Test statistics
\item[4.1.3] Steps 4: Decision
\item[4.1.4] Step 4a: The $p$-value
\ei
\item[4.2] Dependence on the True Parameter Value: Power Function
\item[4.3] Model Selection Strategies
\item[4.4] Logistic Regression
\ei
}}

\placebox{0.50}{0.90}{\myheading{
 Lectures 03 and 04: Classical Inferential Statistics}}
}


\subsection{3.1. OLS Expectation and Covariance}

%###################################################
\frame{\frametitle{3.1. Ordinary Least Squares (OLS) Estimator:\\
 Expectation and Covariance}
%###################################################

\bi
\item Only stochasticity: residual errors
  $\veceps$ according to  $\vec{y}=\m{X}\vec{\beta}+\veceps$
\pause \item The OLS estimator is linear in $\vec{y}$:
\bdma
\hatvecbeta 
  &=&
    \left(\m{X}\tr\m{X}\right)^{-1}\m{X}\tr\vec{y} \\ 
 \visible<3->{ &=&
  \left(\m{X}\tr\m{X}\right)^{-1}\m{X}\tr(\m{X}\vec{\beta}+\veceps)\\}
 \visible<4->{ &=&
  \uu{\vec{\beta}+\left(\m{X}\tr\m{X}\right)^{-1} \m{X}\tr\veceps}}
\edma
\ei

\visible<5->{\mysubheading{Expectation value}

\bdm
E(\hatvecbeta)=E(\vecbeta)
 +\left(\m{X}\tr\m{X}\right)^{-1} \m{X}\tr E(\veceps)=\vecbeta
\edm
}


\visible<6->{\maintextbox{0.8\textwidth}{
The OLS estimator of parameter-linear models is \bfdef{unbiased} under
the mild condition $E(\veceps)=\vec{0}$ for all the data points}}

}

 %###################################################
\frame{\frametitle{OLS estimator: variances and covariances}
%###################################################

\bi
\item Gau\3-Markow conditions $\to \epsilon \sim
  \text{i.i.d} N(0,\sigma^2) \to \hatvecbeta $ is normal distributed
\pause \item In this case, the complete error characteristics are
specified by the 
  expectation value and the \bfdef{variance-covariance matrix}
$\m{V}_{\hatvecbeta}$

\ei

\bdma
  \visible<2->{\m{V}_{\hatvecbeta} &\stackrel{\text{def}}{=}&
    E\left( (\hatvecbeta-\vecbeta) (\hatvecbeta-\vecbeta)\tr\right)\\}
  \visible<3->{\remark{insert
      $\hatvecbeta=\vec{\beta}
+\left(\m{X}\tr\m{X}\right)^{-1}\m{X}\tr\veceps$} 
     &=& 
    E\left( (\m{X}\tr\m{X})^{-1}\m{X}\tr\veceps
     \left( (\m{X}\tr\m{X})^{-1}\m{X}\tr\veceps\right)\tr\right) \\}
  \visible<4->{\remark{transpose and inversion rules} &=& 
     E\left( (\m{X}\tr\m{X})^{-1}\m{X}\tr \veceps\veceps\tr
     \m{X}(\m{X}\tr\m{X})^{-1}\right)\\}
  \visible<5->{\remark{$E(.)$ acts only on $\veceps$} &=& 
     (\m{X}\tr\m{X})^{-1}\m{X}\tr E(\veceps\veceps\tr)
     \m{X}(\m{X}\tr\m{X})^{-1}\\}
  \visible<6->{\remark{Gau\3-Markow} &=& 
     (\m{X}\tr\m{X})^{-1}\m{X}\tr\sigeps^2\m{X}(\m{X}\tr\m{X})^{-1}\\}
  \visible<7->{\remark{def inverse matrix} &=& 
     \sigeps^2(\m{X}\tr\m{X})^{-1} }
\edma
\visible<8->{The variance-covariance matrix depends only on the values of the
exogenous factors!}


}

%###################################################
\frame{\frametitle{Results}
%###################################################

{\small
\bi
\item Ordinary least squares (OLS) estimator:
\maindm{\hatvecbeta =\left(\m{X}\tr\m{X}\right)^{-1}\m{X}\tr\vec{y}}

\pause \item \bfdef{Variance-Covariance matrix} of the estimation errors
(provided the errors are i.i.d.) can be written in terms of the
\bfdef{Hesse matrix $\m{H}$} of the  
objective function SSE:

\maindm{
\begin{array}{rcl}
\m{V}_{\hatvecbeta}
 &=&  E\left( (\hatvecbeta-\vecbeta)(\hatvecbeta-\vecbeta)\tr\right)
 =\sigma^2 \left(\m{X}\tr\m{X}\right)^{-1}= 2\sigma^2 \m{H}^{-1},\\
H_{jk} &=& \left. \ablpartmix{S}{\beta_j}{\beta_k}\right|_{\vecbeta=\hatvecbeta}
=2(\m{X}\tr\m{X})_{jk}
\end{array}
}

\pause \item Variances of estimation errors: $V(\hatbeta_j)=V_{jj}$

\pause \item Correlation of estimation errors: $\text{Corr}(\hatbeta_j, \hatbeta_k)=\frac{V_{jk}}{\sqrt{V_{jj}V_{kk}}}$

\pause \item Distribution of the normalized estimation errors:
$
\frac{\hatbeta_j-\beta_j}{\sqrt{V_{jj}}} \sim N(0,1)
$
\ei
}

}
 
%###################################################
\frame{\frametitle{Estimation of the residual variance}
%###################################################
The above cannot be applied directly since the residual variance $\sigma^2$ is
unknown and must be estimated by the minimum SSE $S(\hatbeta)$: 
\maindm{
\hatsig^2 = \frac{1}{n-J-1}\sum_i \left(y_i-\hat{y}(\vec{x}_i)\right)^2
 = \frac{S(\hatbeta)}{n-J-1}
}
\pause
\bdm
\text{Alternative formulation:} \quad \hatsig^2 =\frac{\vec{y}\tr
  \left(\m{1}-\m{X}(\m{X}\tr\m{X})^{-1}\m{X}\tr\right)\vec{y}} {n-J-1}
\Rightarrow \text{\parbox{0.15\textwidth}{\scriptsize{ sum of\\squared Gaussians}}}
\edm
\vspace{0.2ex}

{\small
\pause This results in following \emph{estimated} error statistics:

\bi
\item Estimated variance-covariance matrix:

\maindm{\hat{\m{V}}_{\hatvecbeta}= 2\hatsig^2 \m{H}^{-1}
=\hatsig^2 \left(\m{X}\tr\m{X}\right)^{-1}
}

\pause \item The normalized 
approximate estimation errors are student-t distributed:
\bdm
\frac{\hatbeta_j-\beta_j}{\sqrt{\hat{V}_{jj}}} \sim T(n-1-J)
\edm
\ei
}

}
 
%###################################################
\frame{\frametitle{Multivariate distribution function of $\hatvecbeta$}
%###################################################

{\small
The distribution of the errors $\Delta
\hatvecbeta=\hatvecbeta-\vecbeta$ obeys a multivariate normal
distribution: 
\bdm
f_{\hatvecbeta}(\Delta \hatvecbeta)
 \propto \exp\left[-\frac{1}{2} 
 \Delta \hatvecbeta\tr \ \m{V}^{-1}\ \Delta \hatvecbeta\right]
   = \exp\left[-\frac{\Delta \hatvecbeta\tr \ \m{X}\tr\m{X}\ 
 \Delta \hatvecbeta}
  {2\sigeps^2}\right].
\edm

\pause
\mysubsubheading{Relation to the maximum-likelihood-method ($\to$
  Lecture 07:)}

Expand the SSE $S(\vecbeta)$ around $\hatvecbeta$ to second order:
\bdm
S(\vecbeta)-S(\hatvecbeta)
\approx \frac{1}{2}\Delta \hatvecbeta\tr \ \m{H} \ \Delta \hatvecbeta
= \Delta \hatvecbeta\tr \ \m{X}\tr\m{X} \ \Delta \hatvecbeta
\edm
\vspace{1em}

\pause
\hspace{5em} $\Rightarrow \quad$ 
\vspace{-3em}

\maindm{f_{\hatvecbeta}(\Delta \hatvecbeta)
 \propto \exp\left[-\frac{S(\vecbeta)-S(\hatvecbeta)}{2\sigeps^2}\right] 
}

\pause and with the estimated residual variance
$\hatsigeps^2=S(\hatvecbeta)/(n-J-1)$  
\bdm
\hat{f}_{\hatvecbeta}(\vecbeta) \propto \exp \left[
-\frac{(n-J-1)}{2}\left(\frac{S(\vecbeta)}{S(\hatvecbeta)}-1\right)
\right]
\edm
}

}

%###################################################
\frame{\frametitle{Example of correlated errors:
modeling the demand for hotel rooms}
%###################################################

\placebox{0.26}{0.64}
{\figSimple{0.46\textwidth}{figsRegr/hotel_scatterplot_x1x2_eng.png}}


\placebox{0.26}{0.22}
{\figSimple{0.46\textwidth}{figsRegr/hotel_scatterplot_x1y_simple_eng.png}}

\placebox{0.74}{0.22}
{\figSimple{0.46\textwidth}{figsRegr/hotel_scatterplot_x2y_simple_eng.png}}

\placebox{0.74}{0.61}
{\parbox{0.53\textwidth}{\scriptsize
\bi
\item The example of Lecture 02: \\[-1ex]
  \bdm
  y=\beta_0+\beta_1x_1+\beta_2x_2+\epsilon
\edm \\[-1ex]
\pause \item Exogenous factors: 
  $x_0=1$, $x_1$: proxy for quality [\# stars]; $x_2$: price 
  [\euro{}/night].
\pause \item Endogenous variable: booking rate [\%]
\pause \item The demand is positively correlated with both the quality and
  the price
\ei
}}

}

%###################################################
\frame{\frametitle{Residual errors for fitted parameters} 
%###################################################

\fig{0.9\textwidth}{figsRegr/hotel_scatterplot_x2ycondx1_eng.png}
}


%###################################################
\frame{\frametitle{Effect of mis-fit parameters I: small effect 
   if $\bfbeta_1$ and $\bfbeta_2$ have opposite misfits}
%###################################################
\fig{0.9\textwidth}{figsRegr/hotel_scatterplot_x2ycondx1_notCal2_eng.png}
}

%###################################################
\frame{\frametitle{Effect of mis-fit parameters II: small effect 
   if $\bfbeta_1$ and $\bfbeta_2$ have opposite misfits}
%###################################################
\fig{0.9\textwidth}{figsRegr/hotel_scatterplot_x2ycondx1_notCal4_eng.png}
}

%###################################################
\frame{\frametitle{Effect of mis-fit parameters III: large effect 
   if $\bfbeta_1$ and $\bfbeta_2$ have both positive misfits}
%###################################################
\fig{0.9\textwidth}{figsRegr/hotel_scatterplot_x2ycondx1_notCal1_eng.png}
}

%###################################################
\frame{\frametitle{Effect of mis-fit parameters IV: large effect 
   if $\bfbeta_1$ and $\bfbeta_2$ have both negative misfits}
%###################################################
\fig{0.9\textwidth}{figsRegr/hotel_scatterplot_x2ycondx1_notCal3_eng.png}
}


%###################################################
\frame{\frametitle{All this results in a negative correlation\\
    between the estimation errors for $\bfbeta_1$ and $\bfbeta_2$}
%###################################################

\vspace{1em}

\fig{0.90\textwidth}{figsRegr/hotel_f2_hatbeta1_hatbeta2_simple_eng.png}

}

%###################################################
\frame{\frametitle{Special case 1: No exogenous variables}
%###################################################
{\small
\bi
\item Model: $y=\beta_0+\epsilon := \mu+\epsilon$
\pause \item System matrix: $\m{X}=(1,1, ..., 1)\tr$
\pause \item OLS estimator: 
\bdma 
\left(\m{X}\tr\m{X}\right)^{-1}=\frac{1}{n}, \quad 
\m{X}\tr \vec{y}=\sum_iy_i=n\bar{y}, &&\\
\hatbeta_0=\hat{\mu}=\left(\m{X}\tr\m{X}\right)^{-1}\m{X}\tr \vec{y}=\bar{y}
\edma
\pause \item Variance:
  $V_{00}=V(\hat{\mu})=\sigma^2\left(\m{X}\tr\m{X}\right)^{-1}
=\frac{\sigma^2}{n}$, \quad
$\hat{V}_{00}=\frac{\hatsig^2}{n}$
\vspace{1em}

\pause \item Distribution of the estimator
 \red{(if $\epsilon \sim i.i.d N(\mu,\sigma^2)$)}

\bdma
\frac{\hatbeta_0-\beta_0}{\sqrt{V_{00}}}
&=& \frac{\bar{y}-\mu}{\sigma}\ \sqrt{n} \sim N(0,1), \\
\frac{\hatbeta_0-\beta_0}{\sqrt{\hat{V}_{00}}}
&=& \frac{\bar{y}-\mu}{\hatsig}\ \sqrt{n} \sim T(n-1)
\edma
\ei
}

}

%###################################################
\frame{\frametitle{Special case 2: Simple linear regression}
%###################################################
\bi
\item Model (with $x_1=x$): $y=\beta_0+\beta_1x+\epsilon$

\pause \item System matrix: 
\bdm \m{X}=\myMatrixTwo{1 & x_1 \\ \vdots & \vdots \\ 1 & x_n},\quad
\m{X}\tr\m{X}=\myMatrixTwo{n & n\bar{x}\\n\bar{x} & \sum x_i^2}
\edm

\pause \item OLS estimator (with $s_x^2=1/n(\sum x_i^2-n\bar{x})$): 
\bdm
\left(\m{X}\tr\m{X}\right)^{-1}
 =\frac{1}{ns_x^2}
\myMatrixTwo{\frac{\sum x_i^2}{n} & -\bar{x} \\ -\bar{x} & 1}, \quad
\m{X}\tr \vec{y}=\myVector{n\bar{y}\\ \sum x_iy_i} 
\edm
\bdma
\hatbeta_1 &=& \left(-\frac{\bar{x}}{ns_x^2}, \frac{1}{ns_x^2}\right)
\myVector{n\bar{y}\\ \sum x_iy_i}
=\frac{\sum_ix_iy_i-n\bar{x}\bar{y}}{\sum x_i^2-n\bar{x}}
=\frac{s_{xy}}{s_x^2}, \\
\hatbeta_0 &=& \bar{y}-\hatbeta_1 \bar{x}
\edma
\ei
}

%###################################################
\frame{\frametitle{Simple linear regression (ctnd)}
%###################################################

\bi
\item Variance-covariance matrix (assuming w/o loss of generality
$\bar{x}=0$): 
\bdm
\m{V}(\hatvecbeta)=\sigma^2 \left(\m{X}\tr\m{X}\right)^{-1}
=\sigma^2 \myMatrixTwo{\frac{1}{n} & 0 \\ 0 & \frac{1}{ns_x^2}}
\edm

\pause \item Variance of the estimator $\hat{y}(x)$ ($x$ is deterministic):
\bdm
V(\hat{y}(x))=V(\hatbeta_0+\hatbeta_1x)
 = V_{00}+x^2  V_{11} +2xV_{01}
 = \frac{\sigma^2}{n}\left(1+\frac{x^2}{s_x^2}\right)
\edm

\pause \item Distribution of the estimator for $y(x)$:
\bdm
\hat{y}(x)\sim N\big(y(x), V(\hat{y}(x))\big)
\edm
If $\sigma^2$ has to be estimated by $\hatsig^2$, the normalized
estimators for $\beta_0$, $\beta_1$ and $y(x)$ are $\sim T(n-2)$.

\ei
}



%###################################################
\frame{\frametitle{Probability density for $\hat{y}(x)$ for simple 
linear regression}
%###################################################

\vspace{-0.5em}
\fig{0.8\textwidth}{figsRegr/regression3dFig.png}
\vspace{-2em}

\bi
\item If the Gau\3-Markov assumptions apply, the model
  estimation errors $\hat{y}(x)-y(x)$ are
  Gaussian distributed
\item The expectation and variance depends on $x$; the standard error
  is hyperbola-shaped.
\ei

}

\subsection{3.2 Confidence Intervals}

%###################################################
\frame{\frametitle{3.2. Confidence Intervals:\\
 where the
Student-t distribution comes from}
%###################################################

\visible<1>{\placebox{0.489}{0.468}{
  \figSimple{1.02\textwidth}{figsRegr/KI_veranschaulichung_eng1.png}}}

\visible<2>{\placebox{0.48}{0.45}{
  \figSimple{1.0\textwidth}{figsRegr/KI_veranschaulichung_eng2.png}}}

\visible<3>{\placebox{0.502}{0.453}{
  \figSimple{1.05\textwidth}{figsRegr/KI_veranschaulichung_eng3.png}}}

\visible<4>{\placebox{0.502}{0.453}{
  \figSimple{1.05\textwidth}{figsRegr/KI_veranschaulichung_eng4.png}}}


}

%###################################################
\frame{\frametitle{Densities of standard normal 
\textit{vs.} Student-t distribution}
%###################################################

\fig{0.90\textwidth}{figsRegr/f_gaussStudent_eng.png}
}


%###################################################
\frame{\frametitle{Distributions of standard normal 
\textit{vs.} Student-t-distribution}
%###################################################

\fig{0.90\textwidth}{figsRegr/F_gaussStudent_eng.png}
}

%###################################################
\frame{\frametitle{Calculation of the confidence intervals (CI)}
%###################################################

\placebox{0.50}{0.81}{\parbox{1.0\textwidth}{
\maindm{\text{CI}_{\beta_j}^{(\alpha)}: \beta_j \in \left[
 \, \hatbeta_j-\Delta \hatbeta_j,  \hatbeta_j+\Delta \hatbeta_j \,
\right], \quad
\Delta \hatbeta_j =t_{1-\alpha/2}^{(n-J-1)}\hatsig_{\hatbeta_j}.
}}}

\placebox{0.50}{0.70}{\parbox{1.0\textwidth}{
 {\small
 \bi
  \pause \item $t_{1-\alpha/2}$: Quantile (inverse of) the distribution
    function\\[-1ex]
  \pause \item CI ``uncertainty principle'': Higher sensitivity implies
    higher $\alpha$ error.
  \ei
}
}}

\visible<3>{\placebox{0.50}{0.31}
{\figSimple{0.70\textwidth}{figsRegr/f_student_KI_eng.png}}}

\visible<2>{\placebox{0.50}{0.31}
{\figSimple{0.70\textwidth}{figsRegr/F_student_KI_eng.png}}}



}


%###################################################
\frame{\frametitle{Hotel example: CI for the
    appraisal for ``stars'' 
    $\beta_1$\\ \hspace*{0.65\textwidth}(full model)}
 %###################################################
\vspace{-2em}
\begin{tabular}{cc}
\parbox{0.5\textwidth}{
  \visible<1->{\fig{0.48\textwidth}{figsRegr/hotel_f_hatbeta1_eng.png}}
  \vspace{-2em}
  \visible<2->{\fig{0.48\textwidth}{figsRegr/hotel_F_hatbeta1_eng.png}}
}
&
\hspace{1em}
\parbox{0.48\textwidth}{

Model: $y(\vec{x})=\sum_j\beta_jx_j+\epsilon$
\vspace{1em}

Factors:\\[-0.5ex]
{\small $x_0=1$, $x_1$: \#stars, $x_2$: price}
\vspace{1em}

Confidence interval (CI):
\vspace{0.5em}

{\small 
$\beta_1\in \left[\hat{\beta}_1-\Delta \hat{\beta}_1^{(\alpha)},
 \hat{\beta}_1+\Delta \hat{\beta}_1^{(\alpha)}\right]$

\pause $\Delta \hat{\beta}_1^{(\alpha)}=t_{1-\alpha/2}^{(n-3)}
\sqrt{\hat{V}(\hat{\beta}_1)}$

\pause $\hat{V}(\hat{\beta}_1)
  =\hatsigeps^2\left[\left(\m{X}\tr\m{X}\right)^{-1}\right]_{11}$

\pause $\hatsigeps^2=\frac{1}{n-3}\sum\limits_{i=1}^n
 \left(\hat{y}_i-y_i\right)^2$
}
}
\end{tabular}
}



\section{4. Significance tests}
\subsection{4.1 General Four-Step Procedure}

%###################################################
\frame{\frametitle{4. Significance Tests\\
4.1 General Four-Step Procedure}
%###################################################
\benum
\item Formulate a \bfdef{null hypothesis $H_0$} such that their rejection
  gives insight, e.g. $\beta_j=\beta_{j0}$ (point hypothesis) or
  $\beta_j \le \beta_0$ (interval hypothesis): Notice: \emph{One cannot confirm $H_0$}
\pause \item Select a \bfdef{test function} or \bfdef{statistics $T$}
\bi
\item whose distribution is known provided the parameters are at the 
\bfdef{margin $H^*_0$ of the null hypothesis} (of course, $H^*_0=H_0$ for a point
null hypothesis) 

\pause
\colAsk{\scriptsize{What if the estimator has a known
    distribution but the variance is unknown?}}
\\[-0.5ex]
\pause
\colAnswer{\scriptsize{Test function in units of the estimated standard deviation}}
\pause \item which has distinct \bfdef{rejection regions $R(\alpha)$} which
  are reached rarely 
  (with a probability $\le \alpha$) if $H_0$ but more often if
  $H_1=\overline{H_0}$ 
\ei

\pause \item Evaluate a realisation $t\sub{data}$ of $T$ from the data

\pause \item Check if $t\sub{data} \in R(\alpha)$. If yes, $H_0$ can be
  rejected at an error probability or \bfdef{significance level
  $\alpha$}. Otherwise, \emph{nothing can be said} \colAsk{(mask
    example with $H_0$: ``mask useless'')}.
\pause \item[4a] Alternatively, calculate the \bfdef{$p$-value} as
the minimum $\alpha$ at which $H_0$ can be rejected.
\eenum
}

%###################################################
\frame{\frametitle{4.1.1 Step 1: Choosing $\mathbf{H_0}$: 
Type I and II errors}
%###################################################

\visible<1>{\placebox{0.5}{0.69}{
 \figSimple{0.59\textwidth}{figsRegr/fehlerErsterZweiterArt1_eng.png}}}

\visible<2>{\placebox{0.5}{0.69}{
 \figSimple{0.59\textwidth}{figsRegr/fehlerErsterZweiterArt2_eng.png}}}

\visible<3>{\placebox{0.5}{0.69}{
 \figSimple{0.59\textwidth}{figsRegr/fehlerErsterZweiterArt3_eng.png}}}

\visible<4->{\placebox{0.5}{0.69}{
 \figSimple{0.59\textwidth}{figsRegr/fehlerErsterZweiterArt4_eng.png}}}

\placebox{0.5}{0.25}{\parbox{\textwidth}{
{\small
\bi
\visible<1->{\item A significance test reduces reality to a ``binary
  in-binary out'' setting.}
\visible<2->{There are two combinations corresponding to a
  correct test result}
\visible<3->{\item We can control the \bfdef{type I or $\alpha$-error}
  probability 
  $P(\text{$H_0$ rejected}|H_0) \le \alpha$ in \bfdef{significance tests}}
\visible<4->{\item Since the \bfdef{type II or $\beta$-error} probability 
  $P(\text{$H_0$ not rejected}|\overline{H_0})$ is unknown, the more serious
error type should be the $\alpha$ error}
\visible<5->{\item \maintextbox{0.85\textwidth}{Fundamental problem: I want $P(H_0|\text{rejected})$ and
$P(H_0|\overline{\text{rejected}})$ while I get control over 
$P(\text{rejected}|H_0)\le P(\text{rejected}|H_0^*)$  $\Rightarrow$
\bfdef{Bayesian statistics}} }
\ei
}}}

}


\subsubsection{4.1.2 Test statistics}
%###################################################
\frame{\frametitle{4.1.2 Steps 2 and 3: Test statistics I}
%###################################################

{\small
\bi
\item Testing  \red{parameters} such as $H_0$: $\beta_j=\beta_{j0}$ or
  $\beta_j\ge \beta_{j0}$ or  $\beta_j\le \beta_{j0}$:

  The test
  function is the estimated deviation from the boundary of $H_0$ in
  units of the estimated error standard deviation. It is
  \bfdef{student-t} distributed with \#dataPoints- \#parameters
  \bfdef{degrees of 
  freedom (df)}:  
\bdm 
T =\frac{\hatbeta_j-\beta_{j0}}{\sqrt{\hat{V}_{jj}}} \sim T(n-1-J)
\edm
\pause \item Testing \red{functions of parameters} such as
  $H_0$: $\beta_1/\beta_2=2$, $\le 2$ or $\ge 2$: Transform into a linear
  combination. Then, the normalized estimated deviation is student-t
  distributed under $H_0^*$. Here, the linear combination is
  $b=\beta_1-2\beta_2=0$: 
\bdma
\hat{b} &=& \hatbeta_1-2\hatbeta_1, \\
\hat{V}(\hat{b}) &=& \hat{V}_{11}+4\hat{V}_{22}-4\hat{V}_{12}, \\
T &=& \frac{\hat{b}}{\sqrt{\hat{V}(\hat{b})}} \sim T(n-1-J)
\edma
\ei
}

}

%###################################################
\frame{\frametitle{Test statistics II}
%###################################################

\bi
\item Testing the \red{correlation coefficient} in an $xy$ scatter plot:
\bdm
\hat{\rho}=\frac{s_{xy}}{s_xs_y}, \quad H_0: \rho=0, \quad
T=\frac{\hat{\rho}}{\sqrt{1-\hat{\rho}^2}}\sqrt{n-2} \sim T(n-2)
\edm

\pause \footnotesize{\colAnswer{
\emph{Derivation:} $\rho \neq 0$ if, and only if, 
in a simple linear regression
$y=\beta_0+\beta_1x+\epsilon$, the slope parameter $\beta_1=0$, so
test for $\beta_1=0$: Under $H_0$, the test statistics
\bdm
T=\hatbeta_1/\sqrt{\hat{V}_{11}}=\frac{s_{xy}}{\hat{\sigma} \ s_x}\sqrt{n}
 \sim T(n-2)
\edm
Now insert $\hat{\sigma}$ which can, in the simple-regression case, be
explicitely calculated:
 $\hat{\sigma}^2=n (s_y^2 -s_{xy}^2/s_x^2)/(n-2)$
}}

\vspace{1em}

\pause \item Test for the \red{residual variance}, $H_0$: $\sigma^2=\sigma_0^2$, 
$\sigma^2 \ge \sigma_0^2$, and $\sigma^2 \le \sigma_0^2$:
\bdm
T=\frac{\hatsig^2}{\sigma_0^2} \ (n-1-J) \sim \chi^2(n-1-J)
\edm
The one-parameter \bfdef{chi-squared distribution with $\bfmath{m}$ degrees of
  freedom} $\chi^2(m)=\sum_{i=1}^mZ_i^2$ is the
sum of squares of i.i.d. Gaussians. \emph{Its density is not symmetric,
  so we need to calculate both the $\alpha$ and $1-\alpha$ quantiles} 

\ei
}

%###################################################
\frame{\frametitle{Test statistics III}
%###################################################

{\small
\bi

\item Tests of \red{simultaneous point null hypotheses}, e.g., $H_0$: 
$(\beta_1=0)$ AND $(\beta_2=2)$ using the
\bfdef{Fisher-F test}:
\bdm
T=\frac{ (S_0-S)/(M-M_0)}{S/(n-M)} \sim F(M-M_0,n-M)
\edm
\bi
\pause \item $S$: SSE of the estimated full model with $M=J+1$ parameters
\pause \item $S_0$: SSE of the estimated restrained model under $H_0$ with
  $M_0$ free parameters
\ei

\pause \item The \bfdef{Fisher-F} distribution is essentially the ratio of two
  independent $\chi^2$ distributed random variables,
\bdm 
F(n,d)=\frac{\chi_n^2/n}{\chi_d^2/d},
\edm
with $n$ numerator and $d$ denominator degrees of freedom

\pause \itemAsk  Argue that always $S_0\ge S$ 
\pause \itemAsk  Justify that there is only a left-sided rejection region:
  $H_0$ is rejected if $t\sub{data}>f^{(n,d)}_{1-\alpha}$

\ei
}
}



%###################################################
\frame{\frametitle{Equivalence of the F and T-tests for one parameter}
%###################################################
\vspace{1em}

{\small
With $M-M_0=1$, the F-test is equivalent to a parameter test for the
  parameter $j$ in question:
\bi
\item Parameter test:
$
T=\frac{\hatbeta_j-\beta_{j0}}{\sqrt{\hat{V}(\hatbeta_j)}}\sim T(n-1-J)
$

\item F-test:
$
T=(n-J-1)\frac{S_0-S}{S} \sim F(1,n-1-J)
$
\pause \itemAsk \colAsk{\scriptsize{Regarding the rhs., show following
    general relation between the 
  student-t and the F(1,d) distributions: 
$
F\sim F(1,d) \ \text{and} \ T \sim T(d) \Rightarrow F=T^2
$
}}

\pause \itemAnswer \colAnswer{\tiny{By definition, Fisher's F is a
    ratio of $\chi^2$ distributions. Furthermore, squares of
    standardnormal random variables $Z$ are $\chi_1^2$ distributed:
\bdm
F(1,d)=\chi_1^2/(\chi_d^2/d)=Z^2/(\chi_d^2/d)
\edm
where $Z\sim N(0,1)$ and $\chi_d^2$ and $Z$ are independent from each
other. 
The definition of the student-t distribution is 
$T(d)=Z/\sqrt{\chi_d^2/d}$, so $F(1,d)=T_d^2$.}}

\pause \item One can show (difficult!) that following is exactly valid for
  the lhs.:
 \bdm
 (n-J-1)\frac{S_0-S}{S}
=\frac{(\hatbeta_j-\beta_{j0})^2}{\hat{V}(\hatbeta_j)}
=\frac{(\hatbeta_j-\beta_{j0})^2}{\hat{V}_{jj}}
\edm
where $S_0$ is the (minimum) SSE for the
calibrated restrained model


\ei
}

}

\subsubsection{4.1.3 Step 4: Decision}

%###################################################
\frame{\frametitle{4.1.3 Step 4: Decision}
%###################################################
\bi
\item Once the test statistics $T$ with a certain conditional distribution
  under $H_0^*$ is defined and the data realisation $t\sub{data}$ is
  calculated, we can decide based on the \emph{rejection region}:
\maintextbox{0.7\textwidth}{The \bfdef{rejection region
    $R^{(H_0)}(\alpha)$} contains the fraction $\alpha$ of all
  realisations $t$ of the test statistics $T$ which are most distant
  from $H_0$}

\pause \item Decision: 
\maintextbox{0.80\textwidth}{$H_0$ is rejected at significance level
  $\alpha$ if  $t\sub{data}\in
  R^{(H_0)}(\alpha)$} 

\pause \item A good test statistics allows for a clear definition of what is
  meant by ``distance to $H_0$'' and brings, for a given $\alpha$, 
 the boundary of the rejection
  region as close to $H_0^*$ as possible

\pause \item In contrast to $T$ and the realisation $t\sub{data}$ which only depends
  on $H_0^*$ and therefore is the same for point and interval
  hypotheses of the same kind, the rejection region is different for
  each null hypothesis
\ei

}

%###################################################
\frame{\frametitle{1. Rejection region for $\vec{H}_0$: ``$<$''
or ``$\le$'' (interval hypothesis)}
%###################################################

\placebox{0.5}{0.58}{
 \figSimple{0.7\textwidth}{figsRegr/annahmeAblehn_einZweiseit1_eng.png}}

\pause
\placebox{0.5}{0.16}{\parbox{\textwidth}{
\bi
 \item $H_0$ is rejected on the level $\alpha$ if
\maindm{
t\sub{data}>t_{1-\alpha}
}
\ei
}}

}


%###################################################
\frame{\frametitle{2. Rejection region for $\vec{H}_0$: ``$>$''
or ``$\ge$'' (interval hypothesis)}
%###################################################

\placebox{0.5}{0.58}{
 \figSimple{0.7\textwidth}{figsRegr/annahmeAblehn_einZweiseit2_eng.png}}

\pause \placebox{0.5}{0.16}{\parbox{\textwidth}{
\bi
 \item $H_0$ is rejected on the level $\alpha$ if
\maindm{t\sub{data}<t_{\alpha}=1-t_{1-\alpha}}
\pause \item The equality sign is only valid for symmetric test
  statistics, e.g., not for the $\chi^2$ distributed statistics of the
  variance test
\ei
}}

}

%###################################################
\frame{\frametitle{3. Rejection region for $\vec{H}_0$: ``='' (point hypothesis)
or ``$\le$''}
%###################################################

\placebox{0.5}{0.61}{
 \figSimple{0.7\textwidth}{figsRegr/annahmeAblehn_einZweiseit3_eng.png}}

\placebox{0.5}{0.12}{\parbox{\textwidth}{
\bi
 \item For symmetric test statistics, 
$H_0$ is rejected on the level $\alpha$ if
\maindm{|t\sub{data}|>t_{1-\alpha/2}}
\pause \item If the distribution is not symmetric (as the $\chi^2$
  distribution for the variance test), the definition of what is
  ``most distant'' is not unique. For simplicity, one assumes equal
  statistical weights to both sides:
\bdm
\text{rejected} \quad \Leftrightarrow 
(t\sub{data}<t_{\alpha/2}) \cup (t\sub{data}>t_{1-\alpha/2})
\edm
\ei
}}

}

%###################################################
\frame{\frametitle{Example: modeling the demand for hotel rooms}
%###################################################

{\small 
The already well-known example for $y(\vec{x})$: hotel room occupancy [\%] 
\bdm
y=\beta_0x_0+\beta_1x_1+\beta_2x_2+\epsilon
\edm
where $x_0=1$, $x_1$: proxy for quality [\# stars]; $x_2$: price
  [\euro{}/night], 
\bdm
\hatbeta_0=25.5, \quad
\hatbeta_1=38.2, \quad
\hatbeta_2=-0.952
\edm
and
\bdm
\hat{\m{V}}=\myMatrixThree{
28.0 &	-6.40 &	-0.119\\
-6.40 & 26.0 & -0.941\\
-0.119 & -0.941 & 0.0397}
\edm

\bi
\pause \itemAsk \colAsk{Formulate and test the null hypothesis at
  $\alpha=\unit[5]{\%}$ that the stars
  do not matter}
\pause \itemAnswer \scriptsize{\colAnswer{$H_{01}: \beta_1=0$, point t-test
    with $T=\hatbeta_1/\sqrt{\hat{V}_{11}} \sim T(12-3)$, i.e. df=9
    degrees of freedom, $t\sub{data}=7.49$,
    $t^{(9)}_{0.975}=2.26<|t\sub{data}|$ $\Rightarrow$ $H_0$ rejected,
    stars matter}}
\pause \itemAsk \colAsk{Do people favour more stars (at $\alpha=\unit[5]{\%}$)?}  
\pause \itemAnswer \scriptsize{\colAnswer{$H_{02}: \beta_1<=0$ (use as $H_0$
    what you want to reject!), interval
     test with same $T$ and $t\sub{data}$ as above,
     $t^{(9)}_{0.95}=1.83<t\sub{data} \Rightarrow H_{02}
     \text{ rejected}$, more stars are better}}
\ei
}

}

%###################################################
\frame{\frametitle{Example: modeling the demand for hotel rooms (ctned)}
%###################################################

\bi
\itemAsk \colAsk{\small{Does each \euro{} more per night decrease the
    occupancy 
  by at most \unit[1]{\%}?}}

\pause \itemAnswer \scriptsize{\colAnswer{$H_{03}: \beta_2<-1$ ($H_{03}$ is the
    complement event!),
    $t\sub{data}=(\hat{\beta}_2+1)/\sqrt{\hat{V}_{22}}=0.24
    \stackrel{!}{>} t^{(9)}_{0.95}=1.83$ $\Rightarrow$ $H_{03}$
    not rejected\\[0.5ex]
$\Rightarrow$ the hotel manager might risk losing more than one
    percent point of customers
}}


\pause \itemAsk {\small \colAsk{Is it worth renovating my hotel thereby gaining one
  star so that\\[0.5ex] I can ask for \unit[30]{\euro{}} more per
  night without  losing guests?}}

\pause \itemAnswer \scriptsize{\colAnswer{Again, define the complement event as 
$H_{04}: \beta_1\le -30\beta_2$ or $\gamma=\beta_1+30\beta_2\le 0$
\bdma
\hat{\gamma} &=& \hatbeta_1+30\hatbeta_2=9.63,\\
\hat{V}(\hat{\gamma}) &=&\hat{V}_{11}+900 \hat{V}_{22}+2*1*30\hat{V}_{12}=5.27
\edma
\pause So, $t\sub{data}=\hat{\gamma}/\sqrt{\hat{V}(\hat{\gamma})}=4.20>
t^{(9)}_{0.95}=1.83$
$\Rightarrow \ H_{04}$ rejected at \unit[5]{\%}  
$\Rightarrow$ 
the risk of losing customers is less than \unit[5]{\%}}}

\pause \itemAsk {\small \colAsk{Can it be simultaneously true that
    $\beta_1=30$ and $\beta_2=-1$?}}

\pause \itemAnswer \scriptsize{\colAnswer{Full model: 
$\hatvecbeta=(25.5, 38.2, -0.952)\tr$, $S(\hatvecbeta)=498.2$; \\
Reduced model with fixed $\beta_1=30$, $\beta_2=1$ and $\hatbeta_0=49.0$:
$\hatvecbeta_r=(49.0, 30, -1)\tr$, $S_0=S(\hatvecbeta_r)=1808$;
\pause
$M-M_0=\unit[2]{df}$,
$n-M=\unit[9]{df}$, $T\sim F(2,9)$,
$t\sub{data}=9/2 \ (S_0-S)/S = 11.8>f^{(2.9)}_{0.95}=4.26$ $\Rightarrow$
$H_0$ rejected}} 

\ei

}



\subsubsection{4.1.4 Step 4a: the $p$-value}

%###################################################
\frame{\frametitle{4.1.4 The $\bfmath{p}$-value}
%###################################################
\bi
\item Obviously, it is not very efficient to test $H_0$ for a fixed
significance level $\alpha$ (one does not know \emph{how significant}
the result really is)
\pause \item Instead, one would like to know the \emph{minimum}
$\alpha$ for rejection (notice the \emph{statistical
  reliability-sensitivity uncertainty 
  relation)}  or the \bfdef{$p$-value}. 
\pause \item The most general definition is:
\maindm{p=\text{Prob}(T\in E\sub{data}|H_0^*))} 
where the \emph{extreme region} $E\sub{data}$ contains all
realisations of $T$ that are
further away from $H^*_0$ than $t\sub{data}$.
\pause \colAsk{\small{Rejection region?}}
\pause \colAnswer{\small{$p$ is defined such that the rejection region
just touches $t\sub{data}$: $E\sub{data}=R(p)$}}

\vspace{1ex}

\bi
\pause \item $p\ge\unit[5]{\%}$: not significant (no star at the value for
  $\beta$, sometimes a ``+'' if between \unit[5]{\%} and
\unit[10]{\%}, e.g., $\beta_1=4.2^+$)
\pause \item $p<\unit[5]{\%}$: significant (one star, e.g., $\beta_1=4.2^*$)
\pause \item $p<\unit[1]{\%}$: very significant (two star, $\beta_1=4.2^{**}$)
\item $p<0.001$: highly significant (three stars, $\beta_1=4.2^{***}$)
\ei
\ei
}

%###################################################
\frame{\frametitle{Calculating $\bfmath{p}$ for some basic tests}
%###################################################

\visible<1->{\placebox{0.84}{0.75}{
 \figSimple{0.30\textwidth}{figsRegr/p_intervalLeft_eng.png}}}

\visible<2->{\placebox{0.84}{0.47}{
 \figSimple{0.30\textwidth}{figsRegr/p_intervalRight_eng.png}}}

\visible<3->{\placebox{0.84}{0.12}{
 \figSimple{0.30\textwidth}{figsRegr/p_point_eng.png}}}

\placebox{0.34}{0.42}{\parbox{0.65\textwidth}{
\bi
\item Interval test $H_0: \beta \le \beta_0$ or $\beta < \beta_0$
\vspace{-1em}
 \bdm
p=P(T>t\sub{data}|\beta=\beta_0)
=1-F_T(t\sub{data})
\edm\\[2em]
\pause \item Interval test $H_0: \beta \ge \beta_0$ or $\beta > \beta_0$
\vspace{-1em}
\bdm
p=P(T<t\sub{data}|\beta=\beta_0)=F_T(t\sub{data})
\edm\\[2em]
\pause \item Point test $H_0: \beta=\beta_0$ (symmetry of $f_T$
assumed at the 3$^{\text{rd}}$ equality sign)
\vspace{-1em}
\bdma
p &=& P\big( (T>|t\sub{data}|) \cup (T<-|t\sub{data}|)\big)\\
 &=& \left(1-F_T(|t\sub{data}|)\right)+F_T(-|t\sub{data}|)\\
 &=& 1-F_T(|t\sub{data}|)+1-F_T(|t\sub{data}|)\\
 &=& 2(1-F_T(|t\sub{data}|))
\edma
\ei
}}

}

%###################################################
\frame{\frametitle{$\bfmath{p}$-values for the null hypotheses of the
    hotel example}
%###################################################

\vspace{-0.5em}
{\small
\bdm
y=\beta_0x_0+\beta_1x_1+\beta_2x_2+\epsilon
\edm
where $x_0=1$, $x_1$: proxy for quality [\# stars]; $x_2$: price

\bi
\item $H_{01}$ ``stars do not matter'': 
point hypothesis $\beta_1=0$
  \\ $t\sub{data}=7.49, \  
 p= 2(1-F_T^{(9)})(|t\sub{data}|)=3.7E-5^{***}$
\pause \item $H_{02}$ ``more stars are better'':
interval hypothesis $\beta_1<0$ \\ $t\sub{data}=7.49,\
p= 1-F_T^{(9)}(t\sub{data})=1.9E-5^{***}$
\pause \item $H_{03}$ ``$\Delta$ occupancy $\le \unit[-1]{\%}$ 
per addtl \euro{}'':
interval hypothesis $\beta_2<-1$ \\ $t\sub{data}=0.24,\
p=1-F_T^{(9)}(t\sub{data})=\unit[40]{\%} $
\pause \item $H_{04}$ One star more is worth $\ge$ \unit[30]{\euro{}}'':\\
 function interval hypothesis
  $\gamma=\beta_1+30\beta_2<0$\\ $t\sub{data}=4.20,\ 
p= 1-F_T^{(9)}(t\sub{data})=\unit[0.12]{\%}^{**}$
\pause \item $H_{05}$ ``star and price sensitivity simultaneously given'':\\
compound point hypothesis $(\beta_1=30) \cap
  (\beta_2=-1)$\\ $t\sub{data}=11.8,\ 
p=1-F_F^{(2,9)}(t\sub{data})=\unit[0.30]{\%}^{**}$
\ei
}

}


%###################################################
\frame{\frametitle{Visualization}
%###################################################
\fig{0.82\textwidth}{figsRegr/hotel_f2_hatbeta1_hatbeta2_eng.png}
\vspace{-1em}

{\scriptsize
\bi
\pause \item Simple null hypotheses ($t$-test):
$H_0: \beta_1=45$

\pause \item Correlated null hypotheses ($t$-test), e.g.,
$H_0=H_{04}: \gamma=\beta_1+30\beta_2<0$

\pause \item Compound null hypotheses ($F$-test), e.g.,
$\bullet$: \ $H_0=H_{05}: (\beta_1=30) \cap \beta_2=-1)$,
or
$\triangle$: \ $H_0: (\beta_1=30) \cap (\beta_2=-0.6)$.
\ei
}
}

\subsection{4.1 Dependence on the True Parameter Value}

%###################################################
\frame{\frametitle{4.2 Dependence on the True Parameter Value}
%###################################################
All statistical tests, including the $p$-values, are based on some
\emph{null hypothesis} which is supposed to be \emph{marginally}
fulfilled, $\beta=\beta_0 \in H_0^*$.  What if the true parameter
values take on other values? 
\bi
\pause\item Since regression parameters are continuous, the probability
  $P(H_0^*)=0$ exactly, so the tests and $p$-values \emph{do not
  reflect reality}
\pause\item What happens for other values $\beta \notin H_0^*$? This is
  quantified by following conditional probability called
  \bfdef{statistical power
    function}: 
\maindm{\pi(\beta)=\text{Pr}(\text{test rejected} | \beta)}
\pause\item If $\beta\notin H_0$, then $\pi(\beta)$ indicates the
  \bfdef{statistical power} or \bfdef{specificity} of a test and 
 $1-\pi(\beta)$ its probability for a type-II error
\pause\item If $\beta\in H_0$, then $\pi(\beta)$ is the type-I ($\alpha$)
  error and $1-\pi(\beta)$ the \bfdef{sensitivity} of a test
\pause\item By definition, $\pi(\beta_0)=\alpha$
\ei
}

%###################################################
\frame{\frametitle{Calculating the statistical power function}
%###################################################

\bi
\item If $\beta\neq \beta_0 \in H_0^*$, then the usual test function, e.g.,
$(\hatbeta_j-\beta_{j0})/\sqrt{\hat{V}_{jj}}$ does \emph{no longer} obey
a standard statistical distribution such as standardnormal or
student-t \\[2em]
\pause \item However, $T=(\hatbeta_j-\beta_j)/\sqrt{\hat{V}_{jj}}$ does:
\bdm
T=\frac{\hatbeta_j-\beta_j}{\sqrt{\hat{V}_{jj}}}
 = \frac{\hatbeta_j-\beta_{j0}}{\sqrt{\hat{V}_{jj}}}
 +\frac{\beta_{j0}-\beta_j}{\sqrt{\hat{V}_{jj}}}
 = \frac{\hatbeta_j-\beta_{j0}}{\sqrt{\hat{V}_{jj}}}-\Delta T
\edm \\[2em]
\pause \item $\Rightarrow$ The independent variable of the power
function is the standardized difference 
$\Delta T=(\beta_j-\beta_{j0})/\sqrt{\hat{V}_{jj}}$


\ei

}

%###################################################
\frame{\frametitle{Example I: Interval test for $<$ and $\le$}
%###################################################
\vspace{-1em}

\bdma
\pi^{\le}(\Delta T)
 & \stackrel{\text{def rejection}}{=} &
   P\left(\frac{\hatbeta_j-\beta_{j0}}{\sqrt{\hat{V}_{jj}}}
    >t_{1-\alpha}\right)\\
\visible<2->{& \stackrel{\text{def $\Delta T$}}{=} & P(T+\Delta
  T>t_{1-\alpha}) \\} 
\visible<3->{ &=& P(T>-\Delta T+t_{1-\alpha}) \\}
\visible<4->{&=& 1-P(T<-\Delta T+t_{1-\alpha})\\}
\visible<5->{ & \stackrel{\text{symm.}}{=} & P(T<\Delta T-t_{1-\alpha})\\}
\visible<6->{ & \stackrel{\text{def distr.}}{=} & 
\uu{F_T(\Delta T-t_{1-\alpha})}}
\edma

\bi
\visible<7->{ \itemAsk \colAsk{\small{Test this expression by calculating
    $\pi^{\le}(0)$ and $\pi'^{\le}(0)$}}}
\visible<8->{\itemAnswer \colAnswer{\scriptsize{Just insert $\Delta T=0$:
\bdma
\pi^{\le}(0) &=& F_T(-t_{1-\alpha})\\
  &=& F_T(t_{\alpha})\\
  &\stackrel{\text{def quantile}}{=} & \alpha \ \OK \\
\pi'^{\le}(0) &=& f_T(-t_{1-\alpha})>0 \ \OK
\edma
}}}

\ei
}

%###################################################
\frame{\frametitle{Type I and II errors for ``$<$'' or
	``$\le$''-tests as a function\\of the true value relative to
    $H_0$, known variance} 
%###################################################
\placebox{0.5}{0.55}{
  \figSimple{0.90\textwidth}{figsRegr/fehler12art_leGauss_eng.png}}

\placebox{0.5}{0.15}{\parbox{0.9\textwidth}{
{\small
\bi
\item The maximum type-I error probability of $\alpha$ 
occurs if $\beta=\beta_0$, i.e., at the boundary of $H_0$.
\pause \item The maximum type-II error probability of $1-\alpha$
occurs if $\beta$ is just outside of $H_0$.
\ei
}}}

}

%###################################################
\frame{\frametitle{The same for unknown variance, 
df=2 degrees of freedom}
%###################################################

\placebox{0.5}{0.55}{
  \figSimple{0.90\textwidth}{figsRegr/fehler12art_leStudent_2FG_eng.png}}

\placebox{0.5}{0.25}{\parbox{0.9\textwidth}{
{\small
\bi
\item The increase with $\Delta T$ is steeper but $\pi(0)=\alpha$ is
  unchanged 
\ei
}}}

}


%###################################################
\frame{\frametitle{Example II: Interval test for for $>$ and $\ge$}
%###################################################

\bdma
\pi^{\ge}(\Delta T)
 & \stackrel{\text{def rejection}}{=} &
  P\left(\frac{\hatbeta_j-\beta_{j0}}{\sqrt{\hat{V}_{jj}}}
    <t_{\alpha}\right)\\
 & \stackrel{\text{def $\Delta T$}}{=} & P(T+\Delta T<t_{\alpha}) \\
 &=& P(T<-\Delta T+t_{\alpha}) \\
 & \stackrel{\text{def distr.}}{=} & \uu{F_T(t_{\alpha}-\Delta T)}
\edma

\bi
\itemAsk \colAsk{\small{Test this expression by calculating
    $\pi^{\ge}(0)$ and $\pi'^{\ge}(0)$}}
\itemAnswer \colAnswer{\scriptsize{Just insert $\Delta T=0$:
\bdma
\pi^{\ge}(0) &\stackrel{\text{def quantile}}{=} & \alpha \ \OK \\
\pi'^{\ge}(0) &=& -f_T(0)<0 \ \OK
\edma
}}

\ei
}

%###################################################
\frame{\frametitle{Type I and II errors for ``$>$'' or
	``$\ge$''-tests, known variance}
%###################################################
\vspace{0em}
\fig{0.9\textwidth}{figsRegr/fehler12art_geGauss_eng.png}
\vspace{-1em}
\bi
\item Again, the maximum type I and II error probabilities of
  $\alpha$ and $1-\alpha$, respectively, are obtained if
  the true parameter(s) are at the boundary / very near outside of $H_0$.
\pause \item The maximum type-I error probability is also known as
significance level.
\ei
}

%###################################################
\frame{\frametitle{The same for unknown variance, 
df=2 degrees of freedom}
%###################################################
\vspace{-4em}
\fig{0.9\textwidth}{figsRegr/fehler12art_geStudent_2FG_eng.png}
}


%###################################################
\frame{\frametitle{Example III: Point test for ``=''}
%###################################################
\bdma
\pi\sup{\,eq}(\Delta T) 
 & \stackrel{\text{def rejection}}{=} & 
 P\left(\left|\frac{\hatbeta_j-\beta_{j0}}{\hatsig_{\hatbeta_j}}\right|
      > t_{1-\alpha/2}\right) \\
\visible<2->{
& \stackrel{\text{def $\Delta T$}}{=} & P(|T+\Delta T| > t_{1-\alpha/2}) \\}
\visible<3->{
&=& P(T+\Delta T > t_{1-\alpha/2}) + P(T+\Delta T <-t_{1-\alpha/2})\\}
\visible<4->{
&=& 1-P(T+\Delta T \le t_{1-\alpha/2})+P(T+\Delta T <-t_{1-\alpha/2})\\}
\visible<5->{
& \stackrel{\text{def distr.}}{=} & 
 1-F_T(t_{1-\alpha/2}-\Delta T) + F_T(-t_{1-\alpha/2}-\Delta T)\\}
\visible<6->{
& \stackrel{\text{symm.}}{=} & 
 \uu{2-F_T(t_{1-\alpha/2}-\Delta T) - F_T(t_{1-\alpha/2}+\Delta T)} }
\edma


\visible<7->{
\bi
\itemAsk \colAsk{\small{Test this expression by calculating
    $\pi^{\le}(0)$}}
\itemAnswer \colAnswer{\scriptsize{Just insert $\Delta T=0$:
\bdm
\pi\sup{\,eq}(0) = 2-(1-\alpha/2)-(1-\alpha/2)=\alpha \ \OK
\edm
}}
}

\ei
}

%###################################################
\frame{\frametitle{Type I and II errors for two-sided (point-)tests \\
	(unkown variance, df=2)}
%###################################################
\vspace{0em}
\fig{0.9\textwidth}{figsRegr/fehler12art_eqStudent_2FG_eng.png}
\vspace{-1em}
\bi
\item Since $H_0$ is a point set here, the type-I error probability is always
  given by $\alpha$ (``significance level'')
\ei
}





\subsection{4.3 Model Selection Strategies}

%###################################################
\frame{\frametitle{4.3 Model Selection Strategies\\
Problem Statement}
%###################################################

\placebox{0.50}{0.45}
{\figSimple{0.60\textwidth}{figsRegr/elephantRaisedTrunkPale.jpg}}

\placebox{0.50}{0.45}
{\parbox{0.80\textwidth}{
\bi
\item With every additional parameter, the fit quality in terms of a
  reduced SSE or $\hat{\sigma}^2=\text{SSE}/(n-J-1)$ becomes better
\pause \item However, the risk of overfitting increases. In the words of John
  Neumann: \textit{With four parameters I can fit an elephant, and
    with five I can make him wiggle [its] trunk.}
\pause \item Overfitted models do not validate and can make neither
  statements nor predictions.
\pause \item \red{$\Rightarrow$ we need selection criteria taking care of
  overfitting!}
\ei
}}

}

%###################################################
\frame{\frametitle{Model selection: some standard criteria}
%###################################################
\bi
\item \bfdef{(1) Adjusted $R^2$:}
\bdm
\bar R^2 = 1 - \, \frac{n-1}{n-J-1}\,\left(1-R^2\right), \quad
R^2=1-\frac{S}{S_0}, 
\edm
\vspace{-1em}
{\small
\bdm
S =\text{SSE(calibr. full model)}, \quad S_0=\text{SSE(calibr. constant-only model)}.
\edm
}
\vspace{0.1em}

\pause \item \bfdef{(2) Akaike information criterion AIC:}
\begin{equation*}
  \text{AIC} = \ln \hat\sigma\sub{descr}^2 + J\, \frac{2}{n}, 
\end{equation*}

\pause \item \bfdef{(3) Bayes' Information criterion BIC:}
\begin{equation*}
   \text{BIC} = \ln \hat\sigma\sub{descr}^2 + J\, \frac{\ln n}{n} \,.
\end{equation*}
\ei
\visible<3->{Notice that the descriptive
  $\hat{\sigma}\sub{descr}^2=S/n$ instead of 
  the unbiased 
$\hat{\sigma}^2=S/(n-1-J)$ is taken in these criteria.
}

}

%###################################################
\frame{\frametitle{Model selection: Strategy {\`a} la ``Occam's Razor''}
{\small
\begin{itemize}
\item Identify $J$ possibly relevant exogenous factors (the constant
  is always included) and calculate
  $\bar{R}^2$, AIC, or BIC for all $2^J$ combinations of these
      factors (\emph{brute force}).
\pause \item The best model is that maximizing $\bar{R}^2$ or minimizing AIC
  or BIC. 
\pause \item Since AIC and also $\bar{R}^2$ penalize complex models (with
  many parameters) too little, the BIC is usually the best bet.
\pause \item Besides the \emph{brute-force} approach, there are two
  faster strategies that may not find the
  ``best'' model, however:  
\bi
\item \bfdef{Top-down approach}: Start with all the $J$
  factors. In each round, eliminate a single factor such that the
  reduced model has the highest increase in $\bar{R}^2$ / decrease in
  AIC or BIC. Stop if there is no further improvement.
\item \bfdef{Bottom-up approach}: Start with the constant-only model
  $y=\beta_0$ and successively add factors until there is no further
  improvement.
\ei
\item Standard statistics packages contain all of these strategies. 
\end{itemize}
}
}

\subsection{4.4. Logistic regression}

%###################################################
\frame{\frametitle{4.4. Logistic regression}
%###################################################


{\small
\bi
\item Normal linear models of the form $Y=\vecbeta\tr\vec{x}+\epsilon$
require the endogenous variable to be continuous (discuss!)
\pause \item Using 
model chaining with an unobservable intermediate continuous 
  variable $Y^*$ allows one to model binary outcomes:
 \maindm{
  Y(\vec{x})= \twoCases{1}{Y^*(\vec{x})> 0}{0}{\text{otherwise,}}
  Y^*(\vec{x})= \hat{y}^*(\vec{x})+\epsilon=\vec{\beta}\tr \vec{x}+\epsilon
}
where $\epsilon$ obeys the \bfdef{logistic distribution} with
$F_{\epsilon}(x)=e^x/(e^x+1)$

\pause \item Probability $P_1$ for the outcome $Y=1$ for symmetric
distributions:
\bdm
 P_1 = P(Y^*(\vec{x})> 0)=F_{\epsilon}(\vecbeta\tr \vec{x})
= \frac{e^{\vecbeta\tr \vec{x}}}{e^{\vecbeta\tr \vec{x}}+1}\text{ (logistic)}
\edm

\pause \item Formally, this is a normal linear regression model for
the log of the \bfdef{odds ratio} $P_1/P_0=P1/(1-P_1)$:
\bdm
\hat{y}^*(\vec{x})=\vec{\beta}\tr \vec{x} = \ln\left(\frac{P_1}{P_0}\right)
\edm
\ei
}

}


%###################################################
\frame{\frametitle{Example: naive OLS-estimation (RP student interviews)}
%###################################################
\placebox[center]{0.25}{0.60}
{\figSimple{0.55\textwidth}{figsRegr/regr_logistic_WS1516cum_f_eng.png}}

\placebox[center]{0.75}{0.60}
{\figSimple{0.55\textwidth}{figsRegr/regr_logistic_WS1516cum_eng.png}}

\placebox[center]{0.50}{0.20}
{\parbox{\textwidth}{
\bi
\item Alternatives: $i=1$: motorized and $i=2$ (not)
\item Intermediate variable estimated by percentaged choices: $y^*=\ln(f_1/(1-f_1))$
\item Model: Log. regression, 
$\hat{y}^*(x_1)=\beta_0  + \beta_1 x_1 $
\item OLS Estimation: $\beta_0=-0.58, \quad \beta_1=0.79 $
\ei
}}
}



%###################################################
\frame{\frametitle{Method consistent? added
    5$\sup{th}$ data point with f=0.9999}
%###################################################

\placebox[center]{0.25}{0.60}
{\figSimple{0.55\textwidth}{figsRegr/regr_logistic_WS1516cum_alt_f_eng.png}}

\placebox[center]{0.75}{0.60}
{\figSimple{0.55\textwidth}{figsRegr/regr_logistic_WS1516cum_alt_eng.png}}

\placebox[center]{0.50}{0.20}
{\parbox{\textwidth}{
\bi
\item Same model:
 $\hat{y}^*(x_1)=\beta_0 + \beta_1 x_1$
\item New estimation: $\beta_0=-3.12, \quad \beta_1=2.03 $
\item Estimation would fail if $f_1=0$ or =1 $\Rightarrow$ real
  discrete-choice model necessary!
\ei
}}
}


%###################################################
\frame{\frametitle{Comparison: real Maximum-Likelihood (ML) estimation}
%###################################################

% png File von von ~/vorlesungen/Verkehrsoekonometrie_Ma/discrChoice_cc_Levmar/*.eng.gnu
% setze beta1_levmar=-beta1_LogR_OLS
% setze beta2_levmar=-beta0_LogR_OLS
% Vorzeichen: Da delta_{i1}->delta_{i0} statt delta_{i2}->delta_{i2} 
% Reihenfolge/Bezeichnung: inkonsistent historisch
% Im latex File Bez. wie bei OLS-logist Regr in ../skripts/figsRegr/


\fig{0.7\textwidth}{figsRegr/revealedChoiceWS1516cum_2al_4dataPoints_fProb_r.png}
\bi
\item Model: Logit, $V_i(x_1)=\beta_0 \delta_{i1} + \beta_1 x_1 \delta_{i1}$, 
$V_2=0$.
\item Estimation: $\beta_0=-0.50\pm 0.65, \ \beta_1=+0.71\pm 0.30$
\ei

}

%###################################################
\frame{\frametitle{Comparison: real ML estimation
	with added 5$\sup{th}$ data point}
%###################################################

\fig{0.7\textwidth}{figsRegr/revealedChoiceWS1516cum_2al_fProb_r.png}
\bi
\item Same logit model, 
$V_i(x_1)=\beta_0 \delta_{i1} + \beta_1 x_1 \delta_{i1}$, 
$V_2=0$.
\item New estimation: $\beta_0=-0.55\pm 0.63, \ \beta_1=+0.75\pm 0.27$
\ei
}

\end{document}


