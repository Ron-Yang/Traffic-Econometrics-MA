%#############################################
% New pdflatex format,
% only png, jpg (or svg?) images but positioning now ok
% template in ~/tex/inputs/template_folien.tex
% [TODO now specialized to Vkoem_Ma; make general]
%#############################################


%\documentclass[mathserif]{beamer}
\documentclass[mathserif,handout]{beamer}
%\usepackage{beamerthemeshadow}
\input{$HOME/tex/inputs/defsSkript}
\input{$HOME/tex/inputs/styleBeamerVkoekMa}
%\input{../style/defs}
\usepackage{graphicx}


%##############################################################

\begin{document}


\section{5. Bayesian Statistics}

%###################################################
\frame{  %title layout
%###################################################
\placebox{0.57}{0.36}{
  \figSimple{1.20\textwidth}{figsRegr/mapMatching.png}}

\makePale{0.50}{0.50}{0.55}{1.20}{1.50}

\placebox{0.31}{0.80}{\parbox{0.6\textwidth}{\mysubheading{
Is the $p$ value dead? Frequentist \emph{vs.} Bayesian inference
}}}

\placebox{0.35}{0.46}{\parbox{0.6\textwidth}{
\bi
\item[5.1] Introduction: Frequentist vs. Bayesian inference
\item[5.2] General Methodics
\bi
\item[5.2.1] Continuous parameter
\item[5.2.2] Discrete parameter
\ei
\item[5.3] Example I: Bayesian Inference\\for a Parameter
    with Gaussian Prior Distribution
\item[5.4] Example II: Map matching
\ei
}}

\placebox{0.50}{0.90}{\myheading{
 Lectures 03 and 04: Classical Inferential Statistics}}
}



%###################################################


\subsection*{5.1 Introduction}

%###################################################
\frame{\frametitle{5.1 Introduction: Frequentist vs. Bayesian inference}
%###################################################
\bi
\item The classic \bfdef{frequentist's} approach calculates the
  probability that the test function $T$ is further away from $H_0$,
  (in
  the extreme range $E\sub{data}$)  than the data realisation
  provided $H_0$ is marginally true: 
\bdm
p=P(T\in E\sub{data}|H^*_0) \ge P(T\in E\sub{data}|H_0)
\edm

\item The \bfdef{Bayesian inference} tries to caculate what is
  actually interesting: The probability of $H_0$ given the data. 

\item If
  the unconditional or \bfdef{a-priori probabilities} were known, this
  is easy using 
  \bfdef{Bayes' theorem} (abbreviating $T\in E\sub{data}$ as $E\sub{data}$)

\maindm{
P(H_0|E\sub{data})=\frac{P(E\sub{data}|H_0)P(H_0)}{P(E\sub{data})}
\le p \, \frac{P(H_0)}{P(E\sub{data})}
}

\item For real-valued parameters, this obviously makes only sense for
  interval null hypotheses 
  since, for a point null hypothesis, we have exactly
  $P(H_0|E\sub{data})=P(H_0)=0$. 
\ei
}

\subsection*{5.2 General Methodology}

%###################################################
\frame{\frametitle{5.2 General Methodology}
%###################################################
Something general
}


\subsection*{5.2.1 Continuous parameter}

%###################################################
\frame{\frametitle{5.2.1 Continuous parameter I}
%###################################################

Probability for $H_0$ based on measurements lying in the extreme
region of a given measurement:
\bdma
P(H_0) &=& \frac{P(E\sub{data}|H_0)P(H_0)}{P(E\sub{data})}\\
 & \stackrel{P(H_0)\to \int_{\beta\in H_0} g(\beta)\diff(\beta)}{=} &
   \frac{\int_{\beta\in H_0}P(E\sub{data}|\beta)g(\beta)\diff{\beta}}
        {\int_{\beta\in \IR}P(E\sub{data}|\beta)g(\beta)\diff{\beta}}\\
 & \stackrel{\text{def power function $\pi$}}{=} &
   \frac{\int_{\beta\in H_0}\pi(\beta)g(\beta)\diff{\beta}}
        {\int_{\beta\in \IR}\pi(\beta)g(\beta)\diff{\beta}}
\edma
}

%###################################################
\frame{\frametitle{5.2.1 Continuous parameter II}
%###################################################
Probability for $H_0$ based on a given realisation (measurement) $b$
of $\hatbeta$ itself:

\bi
\item To obtain finite probabilities, define the event 
$B: \hatbeta \in [b-\delta/2, b+\delta/2]$ with arbitrarily small
  $\delta$

\item Assume a fixed distribution of the estimation error $\hatbeta-\beta$ 
independent from the true value (as in the OLS estimator under
Gau\3-Markow conditions) with density
$f(\hatbeta|\beta)=h(\hatbeta-\beta)$
\ei

\bdma
P(H_0) &=& \frac{P(B|H_0)P(H_0)}{P(B)}\\
 & \stackrel{P(H_0)\to \int g(\beta)\diff(\beta)}{=} &
  \frac{\int_{\beta\in H_0}\delta\ f(\hatbeta|\beta)g(\beta)\diff{\beta}}
       {\int_{\beta\in \IR}\delta\ f(\hatbeta|\beta)g(\beta)\diff{\beta}}
\edma
so
\maindm{P(H_0)=
  \frac{\int_{\beta\in H_0}h(\hatbeta-\beta)g(\beta)\diff{\beta}}
       {\int_{\beta\in \IR}h(\hatbeta-\beta)g(\beta)\diff{\beta}}
}
Notice that the denominator is just the convolution $[h*g](\hatbeta)$
}

\subsection*{5.2.2 Discrete parameter}

%###################################################
\frame{\frametitle{5.2.2 Discrete parameter}
%###################################################
}



\subsection*{5.3 Gaussian Prior Distribution}

%###################################################
\frame{\frametitle{5.3 Example I: Bayesian Analysis for a Parameter
    with Gaussian Prior Distribution} 
%###################################################
{\small
\bi
\item Assume a parameter $\beta$
  with Gaussian prior distribution of variance $\sigma_{\beta}^2$
  and an OLS (or other unbiased)
  estimator $\hatbeta$ with an error variance $\sigma^2_{\hatbeta}$.
\item Assume further a null hypothesis with known a-priori probability
  $P(H_0)$ (can be calculated from the prior distribution) and data
  resulting in a certain frequentist's $p$-value for 
  the estimator.
\item Then, the Bayesian inference for $H_0$ reads
\maindm{P(H_0|\hatbeta) = \Phi\left(\frac{\beta_0-\mu}{\sigma}\right),
  \quad 
 \mu=b \frac{\sigma_{\beta}^2}{\sigma_{\beta}^2+\sigma_{\hatbeta}^2}, \quad
\sigma=\frac{\sigma_{\beta}
  \sigma_{\hatbeta}}{\sqrt{\sigma_{\beta}^2+\sigma_{\hatbeta}^2}}
}
where $b=\beta_0+\sigma_{\hatbeta}\Phi^{-1}(1-p)$ and
$\beta_0 = \sigma_{\beta}\Phi^{-1}(P(H_0))$.

\item This result is valid for any hypothesis for a
  single parameter $\beta$, any a-priori expectation $E(\beta)$ and
  any $H_0$ boundary value $\beta_0$ 

\item If $\sigma_{\hatbeta}^2 \ll \sigma_{\beta}^2$ and $H_0$ is an
  interval, we have $P(H_0|\hatbeta)\to p$ \\ 
  \green{$\Rightarrow$ ressurrection of the $p$-value!}
\ei
}
}

%###################################################
\frame{\frametitle{Bayesian inference for a Gaussian prior
    distribution 1: \\ $P(H_0)=0.5$}
%###################################################
\placebox[center]{0.35}{0.45}
{\figSimple{0.7\textwidth}{figsRegr/PH0_PriorGauss_beta0eq0.png}}

\placebox[center]{0.83}{0.45}
{\parbox{0.39\textwidth}{\footnotesize 
Example: Bike modal split $\beta$
\bi
\item Past investigation: $\beta=\unit[(20 \pm 3)]{\%}$
\item New investigation: $\beta=\unit[(26 \pm 3)]{\%}$
\ei
\pause Has biking increased?
\bi
\item Frequentist:\\
$H_0: \beta<\unit[20]{\%}$,
$p=\Phi(-2)=0.0227$ \OK
\item Bayesian:\\
$\sigma_{\beta}=\sigma_{\hatbeta}=\unit[3]{\%}$,\\
 $p=0.0227$, $P(H_0)=0.5$\\
read from graphics:\\
$P(H_0|\hatbeta)=\unit[8]{\%}$ $\Rightarrow$ \red{no!}
(a difference test would give the same)
\ei
}}

}

%###################################################
\frame{\frametitle{Bayesian inference for a Gaussian prior
    distribution 2: \\ $P(H_0)=0.9987$}
%###################################################
\placebox[center]{0.35}{0.45}
{\figSimple{0.7\textwidth}{figsRegr/PH0_PriorGauss_beta0eq3sigbeta.png}}

\placebox[center]{0.83}{0.45}
{\parbox{0.39\textwidth}{\small
\bi
\item $\sigma_{\hatbeta} \ll \sigma_{\beta}$\\
$\Rightarrow P(H_0|\hatbeta) \approx p$\\
$\Rightarrow$ \green{precise a-posteri information changes much.}
\pause \item $\sigma_{\hatbeta} \gg \sigma_{\beta}$
$\Rightarrow P(H_0|\hatbeta) \approx P(H_0)$\\
$\Rightarrow$ \red{fuzzy a-posteri data essentially give no information
  $\Rightarrow$ a-priori
  probability nearly unchanged.}
\ei
}}

}

%###################################################
\frame{\frametitle{Bayesian inference for a Gaussian prior
    distribution 3: \\ $P(H_0)=0.16$}
%###################################################
\placebox[center]{0.35}{0.45}
{\figSimple{0.7\textwidth}{figsRegr/PH0_PriorGauss_beta0eqm1sigbeta.png}}

\placebox[center]{0.84}{0.45}
{\parbox{0.31\textwidth}{\small
Again, new data with 
$\sigma_{\hatbeta} \ll \sigma_{\beta}$ gives \green{much a-posteriori
information (at least if $p$ is significantly different from
$P(H_0)$)},\\[1em]
\pause new data with $\sigma_{\hatbeta} \gg \sigma_{\beta}$ are
\red{tantamount to essentially no new information.}
}}

}


\subsection*{5.4 Bayesian Inference for a Binary-Valued Parameter}


%###################################################
\frame{\frametitle{5.4 Example II: Bayesian Inference
for a Binary-Valued Parameter:\\
    Map matching}% for $P(H_0)=P(\text{freeway})=0.8$}
%###################################################
\placebox[center]{0.35}{0.45}
{\figSimple{0.7\textwidth}{figsRegr/PH0_PriorBinaryProb08_b.png}}

%}\end{document}

\placebox[center]{0.84}{0.45}
{\parbox{0.35\textwidth}{\footnotesize
True vehicle position:\\
$\beta=\twoCases{0}{\text{freeway}}{d}{\text{parallel road}}$
\\[1em]
Lateral GPS measurement:\\[0.5em]
$\hatbeta \sim 
\twoCases{N(0,\sigma_b^2)}{\text{freeway}}{N(d,\sigma_b^2)}{\text{road}}$
\\[1em]
\pause Measured:\\
 $\hatbeta=\unit[30]{m}$, $\sigma_b=\unit[10]{m}$,\\ 
 at a distance $d=\unit[50]{m}$
\\[1em]
\pause Read from graphics:\\
$\frac{\sigma_b}{d}=0.2$, $\frac{\hatbeta}{d}=0.6$ \\
$\Rightarrow P(H_0|\hatbeta)=0.23$\\[0.5em]
\red{$\Rightarrow$ 
you are on the parallel road with a probability of \unit[77]{\%}}
}}

}

\subsection{2.6. Logistic regression}

%###################################################
\frame{\frametitle{2.5. Logistic regression}
%###################################################

{\small
\bi
\item Normal linear models of the form $Y=\vecbeta\tr\vec{x}+\epsilon$
require the endogenous variable to be continuous (discuss!)
\pause \item Using 
model chaining with an unobservable intermediate continuous 
  variable $Y^*$ allows one to model binary outcomes:
 \maindm{
  Y(\vec{x})= \twoCases{1}{Y^*(\vec{x})> 0}{0}{\text{otherwise,}}
  Y^*(\vec{x})= \hat{y}^*(\vec{x})+\epsilon=\vec{\beta}\tr \vec{x}+\epsilon
}
where $\epsilon$ obeys the \bfdef{logistic distribution} with
$F_{\epsilon}(x)=e^x/(e^x+1)$

\pause \item Probability $P_1$ for the outcome $Y=1$ for symmetric
distributions:
\bdm
 P_1 = P(Y^*(\vec{x})> 0)=F_{\epsilon}(\vecbeta\tr \vec{x})
= \frac{e^{\vecbeta\tr \vec{x}}}{e^{\vecbeta\tr \vec{x}}+1}\text{ (logistic)}
\edm

\pause \item Formally, this is a normal linear regression model for
the log of the \bfdef{odds ratio} $P_1/P_0=P1/(1-P_1)$:
\bdm
\hat{y}^*(\vec{x})=\vec{\beta}\tr \vec{x} = \ln\left(\frac{P_1}{P_0}\right)
\edm
\ei
}

}


%###################################################
\frame{\frametitle{Example: naive OLS-estimation (RP student interviews)}
%###################################################
\placebox[center]{0.25}{0.60}
{\figSimple{0.55\textwidth}{figsRegr/regr_logistic_WS1516cum_f_eng.png}}

\placebox[center]{0.75}{0.60}
{\figSimple{0.55\textwidth}{figsRegr/regr_logistic_WS1516cum_eng.png}}

\placebox[center]{0.50}{0.20}
{\parbox{\textwidth}{
\bi
\item Alternatives: $i=1$: motorized and $i=2$ (not)
\item Intermediate variable estimated by percentaged choices: $y^*=\ln(f_1/(1-f_1))$
\item Model: Log. regression, 
$\hat{y}^*(x_1)=\beta_0  + \beta_1 x_1 $
\item OLS Estimation: $\beta_0=-0.58, \quad \beta_1=0.79 $
\ei
}}
}



%###################################################
\frame{\frametitle{Method consistent? added
    5$\sup{th}$ data point with f=0.9999}
%###################################################

\placebox[center]{0.25}{0.60}
{\figSimple{0.55\textwidth}{figsRegr/regr_logistic_WS1516cum_alt_f_eng.png}}

\placebox[center]{0.75}{0.60}
{\figSimple{0.55\textwidth}{figsRegr/regr_logistic_WS1516cum_alt_eng.png}}

\placebox[center]{0.50}{0.20}
{\parbox{\textwidth}{
\bi
\item Same model:
 $\hat{y}^*(x_1)=\beta_0 + \beta_1 x_1$
\item New estimation: $\beta_0=-3.12, \quad \beta_1=2.03 $
\item Estimation would fail if $f_1=0$ or =1 $\Rightarrow$ real
  discrete-choice model necessary!
\ei
}}
}


%###################################################
\frame{\frametitle{Comparison: real Maximum-Likelihood (ML) estimation}
%###################################################

% png File von von ~/vorlesungen/Verkehrsoekonometrie_Ma/discrChoice_cc_Levmar/*.eng.gnu
% setze beta1_levmar=-beta1_LogR_OLS
% setze beta2_levmar=-beta0_LogR_OLS
% Vorzeichen: Da delta_{i1}->delta_{i0} statt delta_{i2}->delta_{i2} 
% Reihenfolge/Bezeichnung: inkonsistent historisch
% Im latex File Bez. wie bei OLS-logist Regr in ../skripts/figsRegr/


\fig{0.7\textwidth}{figsRegr/revealedChoiceWS1516cum_2al_4dataPoints_fProb_r.png}
\bi
\item Model: Logit, $V_i(x_1)=\beta_0 \delta_{i1} + \beta_1 x_1 \delta_{i1}$, 
$V_2=0$.
\item Estimation: $\beta_0=-0.50\pm 0.65, \ \beta_1=+0.71\pm 0.30$
\ei

}

%###################################################
\frame{\frametitle{Comparison: real ML estimation
	with added 5$\sup{th}$ data point}
%###################################################

\fig{0.7\textwidth}{figsRegr/revealedChoiceWS1516cum_2al_fProb_r.png}
\bi
\item Same logit model, 
$V_i(x_1)=\beta_0 \delta_{i1} + \beta_1 x_1 \delta_{i1}$, 
$V_2=0$.
\item New estimation: $\beta_0=-0.55\pm 0.63, \ \beta_1=+0.75\pm 0.27$
\ei
}

\end{document}
