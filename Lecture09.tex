
%\documentclass[mathserif,aspectratio=1610]{beamer}
\documentclass[mathserif,handout,aspectratio=1610]{beamer}
%\usepackage{beamerthemeshadow}

\input{$HOME/tex/inputs/defsSkript}%$
\input{$HOME/tex/inputs/styleBeamerVkoekMa}%$
%\input{styleBeamerVkoekMa}%$

\usepackage{graphicx}

%\newcommand{\pathDiscrChoice}{$HOME/vorlesungen/Verkehrsoekonometrie_Ma/discrChoice_cc_Levmar} %$
\newcommand{\pathDiscrChoice}{figsDiscr} 

%##############################################################

\begin{document}

\section{Chapter 8: Discrete-Choice Theory: the Basics}

%###################################################
\frame{  %title layout
%###################################################
\makePale{1.00}{0.50}{0.70}{1.40}{1.40}

\placebox{0.50}{0.45}{
  \figSimple{1.10\textwidth}{figsDiscr/vierstufenmodellDiscr_eng.png}}
%makePale{opacity}{centerXrel}{centerYrel}{wrel}\{hrel} 
\makePale{0.70}{0.50}{0.70}{1.40}{1.40}

\placebox{0.50}{0.90}{\parbox{0.8\textwidth}{\myheading{
7. Discrete-Choice Theory: the Basics
}}}

\placebox{0.45}{0.40}{\parbox{0.8\textwidth}{
{\large
\bi
\item 7.1 The Nature of Discrete Decisions
\item 7.2 Basic Concepts: Alternatives, Utilities, Homo Oeconomicus
\item 7.3 Deterministic utilities and how to model them
\item 7.4 Random Utilities
\item 7.5 Choice Probabilities
\ei
}
}}

}




\subsection{4.8. Maximum-likelihood estimation}

%###################################################
\frame{\frametitle{4.8. Maximum-likelihood estimation: 
the likelihood function}
%###################################################
\bi
\item The \bfdef{Maximum-likelihood (ML)} estimation is applicable for
general stochastic models where the probabilities only depend on a
 parameter vector
$\vecbeta$.
\pause \item The goal is to maximize the \bfdef{likelihhod function}
  $L(\vecbeta)$, i.e., the probability that the model predicts
  \emph{all} data points $(\vec{y}_n, \vec{x}_n)$, $n=1, ..., N$:
\maindm{
L(\vecbeta)=P\left(\hat{\vec{y}}_1(\vecbeta)=\vec{y}_1, ..., 
\hat{\vec{y}}_N(\vecbeta)=\vec{y}_N\right)
}
where $\hat{\vec{y}}_n=\hat{\vec{y}}(\vec{x}_n)$ 
gives the model estimate for $\vec{x}_n$
\pause \item For continuous endogenous variables, the likelihood function is
  given by the multi-dimensional probability density at the data
  points:
\bdm
L(\vecbeta)=f_{\hat{\vec{y}}_1(\vecbeta), ..., \hat{\vec{y}}_N(\vecbeta)}
(\vec{y}_1, ..., \vec{y}_N)
\edm

\pause \item[\bfAsk{?}] Verify that the density formulation is equivalent to
  the probability definition by requiring the model estimations to
  be in small intervals around the data instead of hitting the data exactly. 
\ei
}

%###################################################
\frame{\frametitle{Maximum-likelihood estimation}
%###################################################
The ML method just maximizes the likelihood function:
\maindm{
\hatvecbeta=\text{arg} \max_{\vecbeta} L(\vecbeta)
}
\bi
\item Equivalently, and often better, one maximizes the
  \bfdef{log-likelihood}:
\maindm{
\hatvecbeta=\text{arg} \max_{\vecbeta} \tilL(\vecbeta), \quad
\tilL(\vecbeta)=\ln L(\vecbeta)}
\vspace{2em}

\pause \item[\bfAsk{?}] Why it does not matter whether to maximize the
  likelihood or the log-likelihood?
\ei
}



%###################################################
\frame{\frametitle{Application 1: Regression models}
%###################################################

The ML can also be used to estimate regression models and gives
  the same result as the OLS method if the statistical Gau\3-Markow
  conditions are  satisfied:
\bdma
L(\vecbeta) &=& \prod_{n=1}^N f_n(y_n)
 = \prod_{n=1}^N\frac{1}{\sqrt{2\pi\sigma^2}}
\exp\left[-\frac{(y_n-\vec{\beta}\vec{x}_n)^2}{2\sigma^2}\right],\\
\tilL(\vecbeta) &=& \sum_{n=1}^N \ln f_n(y_n)
 = \sum_{n=1}^N\left\{
 -\frac{1}{2}(\ln 2\pi+\ln \sigma^2)
 -\left[\frac{(y_n-\vec{\beta}\vec{x}_n)^2}{2\sigma^2}\right]\right\}
 \nonumber\\
 &=& -\frac{N}{2}(\ln 2\pi+\ln \sigma^2)
 -\frac{1}{2\sigma^2}(\vec{y}-\m{X}\vec{\beta})\tr(\vec{y}-\m{X}\vec{\beta}).
\edma
This has the same functional form as the sum of squared errors of the
OLS method and leads to
the same estimator.
\vspace{1em}

\pause \bfAsk{?} Why it is possible 
to express $L(\vecbeta)$ as a product, here?
}

%###################################################
\frame{\frametitle{Application 2: Discrete-choice models}
%###################################################
\bi
{\small
\item Probability to predict the chosen alternative $i_n$ for decision $n$:
\bdma
P(\hatvec{Y}_{n}=\vec{y}_n)
&=& P(\hat{Y}_{n1}=y_{n1}, ..., \hat{Y}_{nI}=y_{nI})\\
&=& \prod_{i=1}^I
\left[P_{ni}(\vecbeta)\right]^{y_{ni}}=P_{ni_n}(\vecbeta)
\edma
\vspace{-1em}

(The rightmost expression relies on the exclusivity/completeness
requirements of ${\cal A}_n$)
% but the product expression is
%more suitable for the further development)
\vspace{1em}

\pause \item Probability to predict all the data assuming independent
  decisions:
\bdma
L(\vecbeta) &=&
P \left(\vec{Y}\!_{1}(\vecbeta)=\vec{y}_1,
...,\vec{Y}\!_{N}(\vecbeta)=\vec{y}_N \right)\\
 &=&  \prod_{n=1}^N \prod_{i=1}^I
\left[P_{ni}(\vecbeta)\right]^{y_{ni}}
\edma
}
\vspace{-1em}

\pause \item ML estimation:
\ei
\vspace{-0ex}
\maindm{\hatvecbeta=\text{arg} \max_{\vec{\beta}}\tilL(\vecbeta), \quad
 \tilL(\vecbeta)
=\sum\limits_{n=1}^N  
\sum\limits_{i=1}^I y_{ni} \ln P_{ni}(\vec{\beta})
=\sum\limits_{n=1}^N \ln P_{ni_n}(\vec{\beta})
}

}

%###################################################
\frame{\frametitle{Exercise: estimating models with only ACs}
%###################################################

If there are no exogenous variables, we are left with just the ACs
reflecting that people prefer certain alternatives over others for
unknown reasons:
\bdm
V_{ni}=\sum_{m=1}^{I-1}\beta_m\delta_{mi}
\edm
This \bfdef{AC-only model} 
will be the ``reference case'' when estimating the
model quality, e.g., by the \bfdef{likelihood-ratio index}.

{\small
\bi
\pause \item[\bfAsk{?}] Show that, in deriving the main ML result
$\tilL=\sum_n\sum_iy_{ni}\ln P_{ni}$, the random utilities need not to
be uncorrelated between alternatives, only between choices (however,
in the general case, not all probabilities can always be expressed in terms
of parameters)
\pause \item[\bfAsk{?}] Show that the estimated models gives probabilities
  $P_{ni}=P_i$ that are equal to the observed choice fractions $N_i/N$.
  (\emph{Hint:} Lagrange multiplicators to satisfy $\sum_iP_i=1$)
\pause \item[\bfAsk{?}] Based on this result $P_i=N_i/N$, give the parameters for the
 AC-only MNL and for the binary i.i.d. Probit model
\ei
}

}



%###################################################
\frame{\frametitle{I. Direct graphical solution}
%###################################################

\placebox[center]{0.70}{0.90}{$V_{ni}=\beta_1\delta_{i1}+\beta_2T_{ni}$}



\placebox[center]{0.25}{0.30}
 {\figSimple{0.60\textwidth}{figsDiscr/kalLogitBinom_beta1beta2.png}}

\placebox[center]{0.75}{0.30}
 {\figSimple{0.60\textwidth}{figsDiscr/kalProbitBinom_beta1beta2.png}}

\placebox[center]{0.50}{0.70}{\parbox{\textwidth}{
{\footnotesize
\begin{center}
\begin{tabular}{|c||c|c|c|c|} \hline
Choice set
 & $T\sub{ped}=T_1$ [min]
 & $T\sub{bike}=T_2$ [min]
 & \# chosen 1
 & \# chosen 2 \\ \hline
1 & 15 & 30 & 3 & 2 \\
2 & 10 & 15 & 2 & 3 \\
3 & 20 & 20 & 1 & 4 \\
4 & 30 & 25 & 1 & 4 \\
5 & 30 & 20 & 0 & 5 \\
6 & 60 & 30 & 0 & 5 \\ \hline
\end{tabular}
\end{center}
}}}

}



%###################################################
\frame{\frametitle{II. Numerical solution}
%###################################################

\bi
\item
Generally, we have a nonlinear optimization problem. 
\item For
parameterlinear utilities, we know for the MNL that a maximum exists
and is unique.
\pause \item Standard methods of nonlinear optimization are possible:
\bi
\item \bfdef{Newton's and quasi-Newton method}: Fast but may be unstable
\item \bfdef{Gradient/steepest descent methods}: slow but reliable
\item \bfdef{Broyden-Fletcher-Goldfarb-Shanno (BFGS)} or 
\bfdef{Levenberg-Marquardt algorithm} combining gradient and Newton
methods. Such methods are used in many software packages
\item \bfdef{genetic algorithms} if the \emph{objective function
  landscape} is complicated (nonlinear utilities).
\ei
\ei
}

%###################################################
\frame{\frametitle{Special case: estimating the MNL}
%###################################################

The special structure of the MNL with parameter-linear utilities,
$V_{ni}=\sum_m \beta_mX_{mni}$ allows for an intuitive formulation of
the estimation problem: 
\maintextbox{0.8\textwidth}{The observed and modelled 
 \bfdef{property sums} sums of the
  factors $X$ for a given parameter $m$ should be the same
\bdma
X_m\sup{MNL} &=& X_m\sup{data}, \\
\sum_{n, i} x_{mni}\, P_{ni}(\hatvecbeta) &=&
\sum_{n, i} x_{mni}\, y_{ni} = \sum_n x_{mni_n}
\edma
\vspace{-1em}
}

}


%###################################################
\frame{\frametitle{Example: four factors, two alternatives}
%###################################################
MNL model,
$V_{ni}=\beta_1T_{ni}+\beta_2C_{ni}+\beta_3 g_i\delta_{i1}+\beta_4\delta_{i1}$,
$g_{\malepng}=0$, $g_{\femalepng}=1$:
\vspace{1em}
{\small


\bi
\item $X_1=T$: Total travel time for the chosen alternatives:
\bdm
T\sup{MNL} = \sum_{n,i} P_{ni}(\vecbeta)T_{ni}, \quad
T\sup{data} = \sum_{n,i} y_{ni} T_{ni}=\sum_nT_{ni_n}
\edm

\pause \item $X_2=C$: Total money spent by the decision makers:
\bdm
C\sup{MNL} = \sum_{n,i} P_{ni}(\vecbeta)C_{ni}, \quad
C\sup{data} = \sum_{n,i} y_{ni} C_{ni}=\sum_nC_{ni_n}
\edm

\pause \item $X_3=N_{1,\femalepng}$: number of woman chosing alternative 1:
\bdm
N_{1,\femalepng}\sup{MNL} = \sum_{n}  P_{n1}(\vecbeta)g_n, \quad
N_{1,\femalepng}\sup{data} = \sum_{n} y_{n1} g_n 
\edm

\pause \item $X_4=N_1$: total number of persons chosing alternative 1:
\bdm
N_1\sup{MNL} = \sum_{n}  P_{n1}(\vecbeta), \quad
N_1\sup{data} = \sum_{n} y_{n1}
\edm

\ei
}


}




\subsection{4.8a. Detailled example 1}
%~/vorlesungen/Verkehrsoekonometrie_Ma/discrChoice_cc_Levmar/SC_WS1819_3alt_globalT_fProb

%###################################################
\frame{\label{sec:SP}\frametitle{4.8a. Detailled example: 
SP survey WS18/19: global time sensitivities, no
weather factor}
%###################################################

\vspace{0.5em}
{\small
\begin{tabular}{|r||c|c|c||r|r|r|}
\hline 
\myBox{2.5em}{Choice\\Set} &
  \myBox{3em}{Alt. 1:\\Ped} &
  \myBox{3em}{Alt. 2:\\Bike} &
  \myBox{3em}{Alt. 3:\\ PT/Car} &
  Alt 1 & Alt 2 & Alt 3 \\ \hline\hline
1  & 30\,min & 20\,min & 20\,min+0\euro{} & 1  & 3 & 7 \\[0.1em] \hline
2  & 30\,min & 20\,min & 20\,min+2\euro{} & 2  & 9 & 2\\[0.1em] \hline
3  & 30\,min & 20\,min & 20\,min+1\euro{} & 1  & 5 & 7\\[0.1em] \hline
4  & 30\,min & 20\,min & 30\,min+0\euro{} & 2  & 9 & 3\\[0.1em] \hline
5  & 50\,min & 20\,min & 30\,min+0\euro{} & 0  & 9 & 4\\[0.1em] \hline
6  & 50\,min & 30\,min & 30\,min+0\euro{} & 0  & 3 & 9\\[0.1em] \hline
7  & 50\,min & 40\,min & 30\,min+0\euro{} & 0  & 2 & 10\\[0.1em] \hline
8  & 180\,min & 60\,min & 60\,min+2\euro{} & 0 & 4 & 11\\[0.1em] \hline
9  & 180\,min & 40\,min & 60\,min+2\euro{} & 0 & 9 & 6\\[0.1em] \hline
\red{10}  & \red{180\,min}  & \red{40\,min} & \red{60\,min+2\euro{}}
  & \red{0}  & \red{1}  & \red{14}                   \\[0.1em] \hline
11 & 12\,min & 8\,min & 10\,min+0\euro{} & 3  & 5 & 6\\[0.1em] \hline
12 & 12\,min & 8\,min & 10\,min+1\euro{} & 5  & 7 & 2\\[0.1em] \hline
\end{tabular}
}
}

%###################################################
\frame{\frametitle{Model specification and calibration}
%###################################################

\placebox[center]{0.35}{0.40}{
  \figSimple{0.75\textwidth}{\pathDiscrChoice/SC_WS1819_3alt_globalT_fProb.png}
}

\placebox[center]{0.36}{0.82}{\parbox{0.7\textwidth}{\small
%  Deterministic utilities:\\[0.3em]
%
\colorbox{orange}{
$
 V_i= \beta_0\delta_{i1} + \beta_1\delta_{i2} + \beta_2 K
 + \beta_3 T
$
}}}

\pause

\placebox[center]{0.79}{0.84}{\parbox{0.5\textwidth}{\small
or \quad
\colorbox{orange}{$
\begin{array}{l}
V_1=\beta_0 + \beta_2 K + \beta_3 T,\\ 
V_2=\beta_1 + \beta_2 K + \beta_3 T,\\ 
V_3=\beta_2 K + \beta_3 T
\end{array}
$
}}}


\pause

\placebox[center]{0.78}{0.54}{\parbox{0.20\textwidth}{\small\fbox{ 
  $
  \begin{array}{l}
  \ln L\sub{init}=-176.9,\\
  \ln L=-141.5, \\
  \beta_0=-0.95 \pm 0.37, \\
  \beta_1=-0.28 \pm 0.24, \\
  \beta_2=+0.17 \pm 0.19, \\
  \beta_3=-0.04 \pm 0.02
  \end{array}
 $
}}}

\pause

\placebox[center]{0.80}{0.18}{{\scriptsize
   $
   \begin{array}{l}
   \frac{\beta_0}{-\beta_3}=\unit[-22.4]{min},\\
   \frac{\beta_1}{-\beta_3}=\unit[-6.6]{min},\\
   \frac{\beta_0}{-\beta_2}=\unit[+5.7]{\text{\euro{}}},\\
   \frac{\beta_1}{-\beta_2}=\unit[+1.7]{\text{\euro{}}},\\
   \frac{60\beta_3}{\beta_2}=\unit[-15]{\text{\euro{}}/h}
   \end{array}
   $
}}

}




%###################################################
\frame{\frametitle{Likelihood and log-likelihood function for varying
    cost ($\beta_2$) and time ($\beta_3$) sensitivities}
%###################################################

\placebox[center]{0.50}{0.76}{\parbox{1.0\textwidth}{\small
  $ 
  V_i= \beta_0\delta_{i1} + \beta_1\delta_{i2} + \beta_2 K
   + \beta_3 T
  $
}}

\placebox[center]{0.25}{0.42}{\parbox{0.55\textwidth}{
  \begin{center}
    \figSimple{0.55\textwidth}{\pathDiscrChoice/SC_WS1819_3alt_globalT_L_beta2_beta3.png}\\
    Likelihood function\\
    $L(\beta_2,\beta_3|\hatbeta_0, \hatbeta_0)$
  \end{center}
}}


\placebox[center]{0.75}{0.42}{\parbox{0.55\textwidth}{
  \begin{center}
    \figSimple{0.55\textwidth}{\pathDiscrChoice/SC_WS1819_3alt_globalT_lnL_beta2_beta3.png}\\
    Log-likelihood function\\
    $\tilde{L}(\beta_2,\beta_3|\hatbeta_0, \hatbeta_1)$
  \end{center}
}}

}
%###################################################
\frame{\frametitle{Log-likelihood function in parameter space}
%###################################################

\placebox[center]{0.50}{0.42}{
 \figSimple{0.90\textwidth}{\pathDiscrChoice/SC_WS1819_3alt_globalT_corrMatrix.png}
}

\placebox[center]{0.50}{0.81}{\parbox{0.9\textwidth}{\small
  $V_i=\beta_0\delta_{i1} + \beta_1\delta_{i2} + \beta_2 K
   + \beta_3 T + \beta_4 W\delta_{i3}
  $
}}

}


%###################################################
\frame{\frametitle{Model II: plus weather factor (\red{bad weather, $W=1$})}
%###################################################

\placebox[center]{0.35}{0.40}{
  \figSimple{0.75\textwidth}{\pathDiscrChoice/SC_WS1819_3alt_globalT_weather_fProb.png}
}

\placebox[center]{0.36}{0.80}{\parbox{0.7\textwidth}{\small
%  Deterministic utilities:\\[0.3em]
%
\colorbox{orange}{
$
\begin{array}{ll}
 V_i= & \beta_0\delta_{i1} + \beta_1\delta_{i2} + \beta_2 K
 + \beta_3 T\\
 & + \beta_4 W\delta_{i3}
\end{array}
$
}}}

\pause

\placebox[center]{0.79}{0.80}{\parbox{0.5\textwidth}{\small
or \quad
\colorbox{orange}{$
\begin{array}{l}
V_1=\beta_0 + \beta_2 K + \beta_3 T,\\ 
V_2=\beta_1 + \beta_2 K + \beta_3 T,\\ 
V_3=\beta_2 K + \beta_3 T + \beta_4 W
\end{array}
$
}}}


\pause

\placebox[center]{0.78}{0.54}{\parbox{0.20\textwidth}{\small\fbox{ 
  $
  \begin{array}{l}
  \ln L\sub{init}=-176.9,\\
  \ln L=-128.5, \\
  \beta_0=-0.65 \pm 0.37, \\
  \beta_1=-0.42 \pm 0.25, \\
  \beta_2=-0.10 \pm 0.20, \\
  \beta_3=-0.09 \pm 0.02, \\
  \beta_4= 4.2  \pm 1.1
  \end{array}
 $
}}}

\pause

\placebox[center]{0.80}{0.18}{{\scriptsize
   $
   \begin{array}{l}
   \frac{\beta_0}{-\beta_3}=\unit[-7.1]{min},\\
   \frac{\beta_1}{-\beta_3}=\unit[-4.6]{min},\\
   \frac{\beta_0}{-\beta_2}=\unit[-6.7]{\text{\euro{}}},\\
   \frac{\beta_1}{-\beta_2}=\unit[-4.3]{\text{\euro{}}},\\
   \frac{60\beta_3}{\beta_2}=\unit[+57]{\text{\euro{}}/h},\\
   \frac{\beta_4}{-\beta_2}=\unit[+44]{\text{\euro{}}}
   \end{array}
   $
}}

}




%###################################################
\frame{\frametitle{Model III: alt-spec time sensitivities
    plus weather factor}
%###################################################
\placebox[center]{0.35}{0.40}{
  \figSimple{0.75\textwidth}{\pathDiscrChoice/SC_WS1819_3alt_altspecT_weather_fProb.png}
}

\placebox[center]{0.36}{0.81}{\parbox{0.7\textwidth}{\small
%  Deterministic utilities:\\[0.3em]
%
\colorbox{orange}{
$
\begin{array}{ll}
 V_i= & \beta_0\delta_{i1} + \beta_1\delta_{i2} + \beta_2 K
 + \beta_3 T_1\delta_{i1}\\
 & + \beta_4 T_2\delta_{i2} + \beta_5 T_3\delta_{i3}
 + \beta_6 W\delta_{i3}
\end{array}
$
}}}

\pause

\placebox[center]{0.79}{0.81}{\parbox{0.5\textwidth}{\footnotesize
or \quad
\colorbox{orange}{$
\begin{array}{l}
V_1=\beta_0 + \beta_2 K + \beta_3 T,\\ 
V_2=\beta_1 + \beta_2 K + \beta_4 T,\\ 
V_3=\beta_2 K + \beta_5 T + \beta_6 W\\[-1ex]
\end{array}
$
}}}


\pause

\placebox[center]{0.78}{0.53}{\parbox{0.20\textwidth}{\small
  \fbox{ $
   \begin{array}{l}
   \ln L\sub{init}=-176.9,\\
   \ln L=-120.5, \\
   \beta_0=+1.03 \pm 0.74, \\
   \beta_1=+0.66 \pm 0.40, \\
   \beta_2=-0.53 \pm 0.25, \\
   \beta_3=-0.14 \pm 0.03, \\
   \beta_4=-0.11 \pm 0.03, \\
   \beta_5=-0.06 \pm 0.03, \\
   \beta_6=+3.6  \pm 1.1
   \end{array}
   $
}}}

\pause

\placebox[center]{0.80}{0.18}{{\scriptsize
   $
   \begin{array}{l}
   \frac{\beta_0}{-\beta_3}=+\unit[7.5]{min},\\
   \frac{\beta_1}{-\beta_3}=+\unit[4.7]{min},\\
   \frac{\beta_0}{-\beta_2}=+\unit[1.9]{\text{\euro{}}},\\
   \frac{\beta_1}{-\beta_2}=+\unit[4.7]{\text{\euro{}}},\\
   \frac{60\beta_5}{\beta_2}=+\unit[6.7]{\text{\euro{}}/h},\\
   \frac{\beta_4}{-\beta_2}=+\unit[6.7]{\text{\euro{}}}
   \end{array}
   $
}}

}




%###################################################
\frame{\label{sec:SPcomparison}\frametitle{Comparison: Model I}
%###################################################

\placebox[center]{0.35}{0.40}{
  \figSimple{0.75\textwidth}{\pathDiscrChoice/SC_WS1819_3alt_globalT_fProb.png}
}

\placebox[center]{0.36}{0.82}{\parbox{0.7\textwidth}{\small
%  Deterministic utilities:\\[0.3em]
%
\colorbox{orange}{
$
 V_i= \beta_0\delta_{i1} + \beta_1\delta_{i2} + \beta_2 K
 + \beta_3 T
$
}}}

\placebox[center]{0.79}{0.84}{\parbox{0.5\textwidth}{\small
or \quad
\colorbox{orange}{$
\begin{array}{l}
V_1=\beta_0 + \beta_2 K + \beta_3 T,\\ 
V_2=\beta_1 + \beta_2 K + \beta_3 T,\\ 
V_3=\beta_2 K + \beta_3 T
\end{array}
$
}}}


\placebox[center]{0.78}{0.54}{\parbox{0.20\textwidth}{\small\fbox{ 
  $
  \begin{array}{l}
  \ln L\sub{init}=-176.9,\\
  \ln L=-141.5, \\
  \beta_0=-0.95 \pm 0.37, \\
  \beta_1=-0.28 \pm 0.24, \\
  \beta_2=+0.17 \pm 0.19, \\
  \beta_3=-0.04 \pm 0.02
  \end{array}
 $
}}}

\placebox[center]{0.80}{0.18}{{\scriptsize
   $
   \begin{array}{l}
   \frac{\beta_0}{-\beta_3}=\unit[-22.4]{min},\\
   \frac{\beta_1}{-\beta_3}=\unit[-6.6]{min},\\
   \frac{\beta_0}{-\beta_2}=\unit[+5.7]{\text{\euro{}}},\\
   \frac{\beta_1}{-\beta_2}=\unit[+1.7]{\text{\euro{}}},\\
   \frac{60\beta_3}{\beta_2}=\unit[-15]{\text{\euro{}}/h}
   \end{array}
   $
}}

\placebox[center]{0.16}{0.08}{\parbox{0.3\textwidth}{\scriptsize
AIC=275,
BIC=303,\\
$\rho^2=0.200$,
$\tilde{\rho}^2=0.177$
}}

}

%###################################################
\frame{\frametitle{Model II}
%###################################################

\placebox[center]{0.35}{0.40}{
  \figSimple{0.75\textwidth}{\pathDiscrChoice/SC_WS1819_3alt_globalT_weather_fProb.png}
}

\placebox[center]{0.36}{0.82}{\parbox{0.7\textwidth}{\small
%  Deterministic utilities:\\[0.3em]
%
\colorbox{orange}{
$
\begin{array}{ll}
 V_i= & \beta_0\delta_{i1} + \beta_1\delta_{i2} + \beta_2 K
 + \beta_3 T\\
 & + \beta_4 W\delta_{i3}
\end{array}
$
}}}


\placebox[center]{0.79}{0.84}{\parbox{0.5\textwidth}{\small
or \quad
\colorbox{orange}{$
\begin{array}{l}
V_1=\beta_0 + \beta_2 K + \beta_3 T,\\ 
V_2=\beta_1 + \beta_2 K + \beta_3 T,\\ 
V_3=\beta_2 K + \beta_3 T + \beta_4 W
\end{array}
$
}}}




\placebox[center]{0.78}{0.54}{\parbox{0.20\textwidth}{\small\fbox{ 
  $
  \begin{array}{l}
  \ln L\sub{init}=-176.9,\\
  \ln L=-128.5, \\
  \beta_0=-0.65 \pm 0.37, \\
  \beta_1=-0.42 \pm 0.25, \\
  \beta_2=-0.10 \pm 0.20, \\
  \beta_3=-0.09 \pm 0.02, \\
  \beta_4= 4.2  \pm 1.1
  \end{array}
 $
}}}



\placebox[center]{0.80}{0.18}{{\scriptsize
   $
   \begin{array}{l}
   \frac{\beta_0}{-\beta_3}=\unit[-7.1]{min},\\
   \frac{\beta_1}{-\beta_3}=\unit[-4.6]{min},\\
   \frac{\beta_0}{-\beta_2}=\unit[-6.7]{\text{\euro{}}},\\
   \frac{\beta_1}{-\beta_2}=\unit[-4.3]{\text{\euro{}}},\\
   \frac{60\beta_3}{\beta_2}=\unit[+57]{\text{\euro{}}/h},\\
   \frac{\beta_4}{-\beta_2}=\unit[+44]{\text{\euro{}}}
   \end{array}
   $
}}

\placebox[center]{0.16}{0.08}{\parbox{0.3\textwidth}{\scriptsize
AIC=247,
BIC=282,\\
$\rho^2=0.273$,
$\tilde{\rho}^2=0.245$
}}

}

%###################################################
\frame{\frametitle{Model III}
%###################################################

\placebox[center]{0.35}{0.40}{
  \figSimple{0.75\textwidth}{\pathDiscrChoice/SC_WS1819_3alt_altspecT_weather_fProb.png}
}

\placebox[center]{0.36}{0.82}{\parbox{0.7\textwidth}{\small
%  Deterministic utilities:\\[0.3em]
%
\colorbox{orange}{
$
\begin{array}{ll}
 V_i= & \beta_0\delta_{i1} + \beta_1\delta_{i2} + \beta_2 K
 + \beta_3 T_1\delta_{i1}\\
 & + \beta_4 T_2\delta_{i2} + \beta_5 T_3\delta_{i3}
 + \beta_6 W\delta_{i3}
\end{array}
$
}}}



\placebox[center]{0.79}{0.84}{\parbox{0.5\textwidth}{\small
or \quad
\colorbox{orange}{$
\begin{array}{l}
V_1=\beta_0 + \beta_2 K + \beta_3 T,\\ 
V_2=\beta_1 + \beta_2 K + \beta_4 T,\\ 
V_3=\beta_2 K + \beta_5 T + \beta_6 W
\end{array}
$
}}}




\placebox[center]{0.78}{0.54}{\parbox{0.20\textwidth}{\small
  \fbox{ $
   \begin{array}{l}
   \ln L\sub{init}=-176.9,\\
   \ln L=-120.5, \\
   \beta_0=+1.03 \pm 0.74, \\
   \beta_1=+0.66 \pm 0.40, \\
   \beta_2=-0.53 \pm 0.25, \\
   \beta_3=-0.14 \pm 0.03, \\
   \beta_4=-0.11 \pm 0.03, \\
   \beta_5=-0.06 \pm 0.03, \\
   \beta_6=+3.6  \pm 1.1
   \end{array}
   $
}}}



\placebox[center]{0.80}{0.18}{{\scriptsize
   $
   \begin{array}{l}
   \frac{\beta_0}{-\beta_3}=+\unit[7.5]{min},\\
   \frac{\beta_1}{-\beta_3}=+\unit[4.7]{min},\\
   \frac{\beta_0}{-\beta_2}=+\unit[1.9]{\text{\euro{}}},\\
   \frac{\beta_1}{-\beta_2}=+\unit[4.7]{\text{\euro{}}},\\
   \frac{60\beta_5}{\beta_2}=+\unit[6.7]{\text{\euro{}}/h},\\
   \frac{\beta_4}{-\beta_2}=+\unit[6.7]{\text{\euro{}}}
   \end{array}
   $
}}

\placebox[center]{0.16}{0.08}{\parbox{0.3\textwidth}{\scriptsize
AIC=227,
BIC=277,\\
$\rho^2=0.319$,
$\tilde{\rho}^2=0.279$
}}


}


\subsection{4.8b. Detailled example 2}
% ~/vorlesungen/Verkehrsoekonometrie_Ma/discrChoice_cc_Levmar/RC_skript_4alt_r_eng

%##############################################################
\frame{\frametitle{4.8b Revealed preferences/choice: the data}
%##############################################################

Distance classes for the trip home to university (cumulated till 2018)
\vspace{0.5em}

Weather: good
\vspace{1em}

{\small\begin{tabular}{|r|r||c|c|c|c|}
\hline 
\myBox{4em}{Distance} & 
  \myBox{4em}{Class-\\center} &
  \myBox{3em}{Choice\\Alt. 1:\\ped} &
  \myBox{3em}{Choice\\Alt. 2:\\bike} &
  \myBox{3em}{Choice\\Alt. 2:\\PT} &
  \myBox{3em}{Choice\\Alt. 3:\\ car} \\ \hline\hline
\unit[0-1]{km} & \unit[0.5]{km}    &  17 & 16 & 10 & 0\\[0.2em] \hline
\unit[1-2]{km} & \unit[1.5]{km}    &  9  & 23 & 20 & 2\\[0.2em] \hline
\unit[2-5]{km} & \unit[3.5]{km}    &  2  & 27 & 55 & 4\\[0.2em] \hline
\unit[5-10]{km} & \unit[7.5]{km}   &  0  & 7  & 42 & 7\\[0.2em] \hline
\unit[10-20]{km} & \unit[12.5]{km} &  0  & 0  & 18 & 7\\[0.2em] \hline
\end{tabular}}

}



%###################################################
\frame{\frametitle{Revealed Choice: fit quality}
%###################################################

\placebox[center]{0.40}{0.78}{\parbox{0.5\textwidth}{\small
$
\begin{array}{l}
V_1=\beta_1+\beta_4 r,\\ 
V_2=\beta_2+\beta_5 r,\\
V_3=\beta_3+\beta_6 r, \\
V_4=0
\end{array}
$
}}

\placebox[center]{0.37}{0.37}{
  \figSimple{0.73\textwidth}{\pathDiscrChoice/RC_skript_4alt_r_eng_fProb.png}
}

\placebox[center]{0.85}{0.40}{\parbox{0.30\textwidth}{\small
 $
 \begin{array}{l}
 \beta_1= 4.1\pm 0.6, \\
 \beta_2= 3.6\pm 0.5, \\
 \beta_3= 3.0\pm 0.5, \\
 \beta_4=-1.43\pm 0.26, \\
 \beta_5=-0.48\pm 0.08, \\
 \beta_6=-0.14\pm 0.05
 \end{array}
 $
}}

}

%###################################################
\frame{\frametitle{Revealed Choice: Modal split as a function of distance}
%###################################################

\placebox[center]{0.40}{0.78}{\parbox{0.5\textwidth}{\small
$
\begin{array}{l}
V_1=\beta_1+\beta_4 r,\\ 
V_2=\beta_2+\beta_5 r,\\
V_3=\beta_3+\beta_6 r, \\
V_4=0
\end{array}
$
}}

\placebox[center]{0.37}{0.37}{
  \figSimple{0.73\textwidth}{\pathDiscrChoice/RC_skript_4alt_r_eng_fProbKumDist.png}
}

\placebox[center]{0.85}{0.40}{\parbox{0.30\textwidth}{\small
 $
 \begin{array}{l}
  \beta_1= 4.1\pm 0.6, \\
 \beta_2= 3.6\pm 0.5, \\
 \beta_3= 3.0\pm 0.5, \\
 \beta_4=-1.43\pm 0.26, \\
 \beta_5=-0.48\pm 0.08, \\
 \beta_6=-0.14\pm 0.05
 \end{array}
 $
}}

}


%###################################################
\frame{\frametitle{Log-Likelihood: Sections through parameter space}
%###################################################
\figSimple{0.90\textwidth}{\pathDiscrChoice/RC_skript_4alt_r_eng_corrMatrix.png}

{\small
$V_i=\sum_{m=1}^3\beta_m\delta_{m,i}
+\sum_{m=1}^3\beta_{m+3}\ r\delta_{m,i}$
}

}


%###################################################
\frame{\frametitle{Likelihood and Log-Likelihood as $f(\beta_1,\beta_2)$}

\placebox[center]{0.50}{0.77}{\parbox{0.9\textwidth}{
$V_i=\sum_{m=1}^3\beta_m\delta_{m,i}
+\sum_{m=1}^3\beta_{m+3}\ r\delta_{m,i}$
}}

\placebox[center]{0.25}{0.50}{
 \figSimple{0.55\textwidth}{\pathDiscrChoice/RC_skript_4alt_r_eng_L_beta0_beta1.png}
}

\placebox[center]{0.75}{0.50}{
 \figSimple{0.55\textwidth}{\pathDiscrChoice/RC_skript_4alt_r_eng_lnL_beta0_beta1.png}
}

\placebox[center]{0.30}{0.20}{\parbox{0.5\textwidth}{
  Likelihoodfunktion\\
  $L(\beta_1,\beta_2,\hatbeta_3, ...)$
}}

\placebox[center]{0.80}{0.20}{\parbox{0.5\textwidth}{
  Log-Likelihoodfunktion\\
   $\tilL(\beta_1,\beta_2,\hatbeta_3, ...)$
}}

}





\subsection{4.9. Estimation errors}

%###################################################
\frame{\frametitle{4.9. Variance-covariance matrix of the estimation errors}
%###################################################

{\small
Since the log-lilkelihhod is maximized at $\hatvecbeta$, we have
\bdm
\ablpart{\tilL}{\vecbeta}=0 \ \Rightarrow \ 
\tilL(\vec{\beta}) \approx \tilL\sub{max} + \frac{1}{2}
\Delta \vec{\beta}^{\,T} \cdot \m{H} \cdot \Delta \vec{\beta}, \quad
\Delta \vec{\beta}=\vecbeta-\hatvecbeta
\edm
with the (negative definite) 
Hessian $H_{lm}=\ablpartmix{\tilL(\hatvecbeta)}{\beta_l}{\beta_m}$.

\pause
Compare $L(\vec{\beta})$ near
its maximum with the density $f(\vec{x})$ of the general multivariate normal
distribution with 
variance-covariance matrix $\vec{\Sigma}$:
\bdma
L(\vec{\beta}) &=& L\sub{max}\exp\left( 
\frac{1}{2} \Delta \vec{\beta}^{\,T} \cdot \m{H} \cdot \Delta
\vec{\beta}\right),\\[-1ex]
f(\vec{x}) &=& \left((2\pi)^{M} \text{Det}\vec{\Sigma}\right)^{-1/2} \ 
\exp\left(-\frac{1}{2} \vec{x}\tr\vec{\Sigma}^{-1}\, \vec{x}\right)
\edma
\vspace{-1ex}

\pause Identify $\Delta \vecbeta$ with $\vec{x}$, $\m{V}$ with $\vec{\Sigma}$,
  and assume the 
asymptotic limit (higher than quadratic terms in $\tilL(\hatvecbeta)$
negligible): \ $\Rightarrow$
}

\maindm{\m{V}=\text{Cov}(\hatvecbeta)
=E\left[\left(\vecbeta-\hatvecbeta\right)
\left(\vecbeta-\hatvecbeta\right)\tr\right] 
\approx -\m{H}^{-1}(\hatvecbeta)
}
}

%###################################################
\frame{\frametitle{Fisher's information matrix}
%###################################################
{\small
The variance-covariance matrix is related to 
\bfdef{Fisher's information matrix $\vec{\cal I}$}:
\bdm
\vec{\cal I}=-\m{H}, \quad
I_{lm}=-\ablpartmix{\tilL(\hatvecbeta)}{\beta_l}{\beta_m}
\edm
\bi
\item
Roughly speaking, information is missing uncertainty, so the higher
the main components of $\vec{\cal I}$, the lower the main components
of $\m{V}$
\vspace{1ex}
\pause \item
\bfdef{Cram{\'e}r-Rao inequality:} A lower bound for the
variance-covariance matrix is the inverse of Fisher's information
matrix
$\Rightarrow$ The ML estimator is \bfdef{asymptotically efficient}
\vspace{1ex}

\pause \item Comparison with the OLS estimator 
$\m{V}\sub{OLS}=2\sigma^2 \m{H}_S^{-1}$ of regression models:
\bdm
 \vec{\cal I}=-\m{H}=\m{H}_S/(2\sigma^2)=\m{X}\tr\m{X}/\sigma^2
\edm
The negative Hesse matrix of $\tilL(\vecbeta)$ 
 is proportional to the Hesse matrix of the regression SSE $S(\vecbeta)$.
\ei
}
}


\subsection{4.10. Significance tests}


%##############################################################
\frame{\frametitle{4.10. Significance tests I: parametric tests}
%##############################################################
\bi
\item The parameter test procedures are exactly the same as that of
regression models. Because we only consider the asymptotic limit, the test
  statistic is always Gaussian:

\pause \item Confidence interval of a parameter $\beta_m$:
\bdm
\text{CI}_{\alpha}(\beta_m)=[\hatbeta_m-\Delta_{\alpha},
  \hatbeta_m+\Delta_{\alpha}], \quad 
\Delta_{\alpha}=z_{1-\alpha/2}\sqrt{V_{mm}}
\edm

\pause \item Test of a parameter $\beta_m$ for 
$H_0: \beta_j=\beta_{j0}$, \ $\ge
  \beta_{j0}$, \ or $\le \beta_{j0}$:
\bdm
T=\frac{\hatbeta_j-\hatbeta_{j0}}{\sqrt{V_{jj}}}\sim N(0,1) \ | H_0^*
\edm

\pause \item $p$-values for 
$H_0: \beta_j=\beta_{j0}$, \ $\ge
  \beta_{j0}$, \ or $\le \beta_{j0}$, respectively:
{\small
\bdm
p_{=}=2\big(1-\Phi(|\hatbeta_j-\beta_{j0}|)\big), \quad
p_{\le}=1-\Phi(\hatbeta_j-\beta_{j0}), \quad
p_{\ge}=\Phi(\hatbeta_j-\beta_{j0})
\edm
}
\pause \item As in regression, a factor 4 of more data halves the error
\ei

}

%##############################################################
\frame{\frametitle{Significance tests II: Likelihood-ratio (LR) test}
%##############################################################
Like in regression (F-test), one sometimes wants to test $H_0$s
fixing
several parameters simultaneously to given values, i.e., $H_0$
corresponds to a \bfdef{restraint model}
\bi
\pause \item $H_0$: The restraint model with some fixed parameters and
  $M_r$ remaining parameters describes the data as well as the full
  model with $M$ parameters

\pause \item Test statistics: 
\vspace{-1em}

{\small
\maindm{
\lambda\sup{LR}=2\ln\left(
  \frac{L\left(\hatvecbeta\right)}{L\sup{r}\left(\hatvecbeta\sup{r}\right)}\right)
=2\left[
  \tilL\left(\hatvecbeta\right) 
 - \tilL\sup{r}\left(\hatvecbeta\sup{r}\right)
\right]
\sim \chi^2(M-M_r) \ \text{if}\ H_0
}
}
\vspace{1ex}

\pause \item Data realisation:
Calibrate both $M$ and $M_r$ and evaluate $\lambda\sup{LR}\sub{data}$

\pause \item Result:
Reject $H_0$ at $\alpha$ based on the
$1-\alpha$ quantile:
\bdm
\lambda\sup{LR}\sub{data}>\chi^2_{1-\alpha, M-M_r}
\edm
$p$-value: $p=1-F_{\chi^2(M-M_r)}\left(\lambda\sup{LR}\sub{data}\right)$
\ei
}

%##############################################################
\frame{\frametitle{Example: Mode choice for the route to this lecture}
%##############################################################

{\small
\begin{center}
\begin{tabular}{|l||r|r|r|} \hline
Distance class $n$ & \myBox{4em}{Distance $r_n$} &
$i=1$ (ped/bike) & $i=2$ (PT/car) \\
\hline\hline
$n=1$: 0-1 km    & \unit[0.5]{km}  & 7 & 1\\
$n=2$: 1-2 km    & \unit[1.5]{km}  & 6 & 4\\
$n=3$: 2-5 km    & \unit[3.5]{km}  & 6 & 12\\
$n=4$: 5-10 km   & \unit[7.5]{km}  & 1 & 10\\
$n=5$: 10-20 km  & \unit[15.0]{km} & 0 & 5\\ \hline
\end{tabular}
\end{center}
\vspace{-0.5em}

\bdma
V_{n1}(\beta_1,\beta_2) &=& \beta_1r_n+\beta_2,\\
V_{n2}(\beta_1,\beta_2) &=& 0
\edma
\vspace{-1.5em}

\bi
\item $\beta_1$: Difference in distance sensitivity (utility/km)
  for chosing ped/bike over PT/car (expected $<0$)
\item $\beta_2$: Utility difference ped/bike over PT/car at zero
  distance ($>0$)
\ei
\pause \bfAsk{Do the data allow to distinguish this model from the trivial
  model $V_{ni}=0$?}
}

}

%##############################################################
\frame{\frametitle{LR test for the corresponding Logit models}
%##############################################################

\fig{0.65\textwidth}{figsDiscr/BNL_Entfernung_smallSample_logL_eng.png}
\vspace{-1.5em}

{\small
\bi
\item $H_0$: The trivial model $V_{ni}=0$ describes the data as well
  as the full model
  $V_{n1}(\beta_1,\beta_2)=(\beta_1r_n+\beta_2)\delta_{i1}$
\pause \item Test statistics: \  $\lambda\sup{LR}=2\left[
  \tilL(\hatbeta_1,\hatbeta_2)-\tilL(0,0)\right] \sim \chi^2(2)|H_0$
\pause \item Data realisation (1 $\tilL$-unit per contour): \  
$\lambda\sup{LR}\sub{data}=2(-26.5+36.5)=20$
\pause \item Decision: \ Rejection range $\lambda\sup{LR}>\chi^2_{2,0.95}=5.99
  \ \Rightarrow$ \red{$H_0$ rejected.}
\ei
}

}

%##############################################################
\frame{\frametitle{Fit quality of the full model}
%##############################################################

\fig{0.60\textwidth}{figsDiscr/BNL_Entfernung_smallSample_relHaeuf_eng.png}
\vspace{-1.0em}
{\small
\bi
\item[\bfAsk{?}] What would be the modelled ped/bike modal split for
  the null model $V_{ni}=0$?
\pause \item[\bfAsk{?}] Read off from the $\tilL$ contour plot the parameter of
  the AC-only model $V_{ni}=\beta_2\delta_{i1}$ and give the modelled
  modal split
\pause \item[\bfAsk{?}] Motivate the negative correlation between the parameter errors
\ei
}

}

\subsection{4.11. Goodness-of-fit measures}

%##############################################################
\frame{\frametitle{4.11. Goodness-of-fit measures}
%##############################################################

\bi
\item The parameter tests for equality and the LR test are related to
  \bfdef{significance}: Is the more complicated of two nested models
  significantly better in describing the data?
\item This can be used to find the best model using the
  \bfdef{top-down ansatz}: \EinsteinPdflatex \emph{Make is as simple
    as possible but not simpler!}
\pause \item Problem: For very big samples, nearly any new parameter gives
  significance and the top-down ansatz fails
\pause \item More importantly: Significance/LR tests cannot give
evidence for missing 
  but relevant factors 
\pause \item A further problem: We cannot compare non-nested models
\pause \item Finally, in reality, one is interested in \bfdef{effect
  strength} (difference in the fit and validation quality), not
  significance
\ei

\pause 
\centering{\bfred{$\Rightarrow$ we need measures for absolute fit
    quality}}
}

%##############################################################
\frame{\frametitle{Information-based goodness-of-fit (GoF) measures}
%##############################################################


\bi
\item \bfdef{Akaike's information criterion}: % \vspace{-1ex}
  \bdm \text{AIC}=-2\tilL+2M\frac{N}{N-(M+1)} \edm

\item \bfdef{Bayesian information criterion:} % \vspace{-1ex}
   \bdm \text{BIC}=-2\tilL+M\ln N \edm
\ei


$N$: number of decisions; $M$: number of parameters

{\small
\bi
\pause \item Both criteria give the needed additional information (in
bit) to obtain the actual micro-data from the
  model's prediction, including an overfitting penalty: the lower, the better.
\pause \item Both the AIC and BIC are equivalent to the corresponding
GoF measures of regression.
\pause \item the BIC focusses more on parsimonious models (low $M$).
\pause \item For nested models satisfying the null hypothesis of the LR test
  and $N\gg M$,
  the expected AIC is the same (\bfAsk{verify!}). However, since the
  AIC is an absolute 
  measure, it allows comparing non-nested models.
\ei
}

}

%##############################################################
\frame{\frametitle{GoF measures corresponding to the coefficient of
    determination $R^2$ of linear models 
($\tilL\sup{0}$: log-likelihood\\of the
estimated AC-only or trivial model)}
%##############################################################

{\small
\bi
\item \bfdef{LR-Index} resp. \bfdef{McFaddens $R^2$}:  \vspace{-2ex}
   \bdm \rho^2=1-\frac{\tilL}{\tilL\sup{0}} \edm
 
\item \bfdef{Adjusted LR-Index/McFaddens $R^2$:}  \vspace{-1ex}
  \bdm \bar{\rho}^2=1-\frac{\tilL-M}{\tilL\sup{0}}\edm
\ei
}

{\footnotesize
\bi
\pause \item The LR-Index $\rho^2$ and the adjusted LR-Index $\bar{\rho}^2$
   correspond to the coefficient of
    determination $R^2$ and the adjusted coefficient $\tilde{R}^2$ of
    regression models, respectively: The higher, the better.
\pause \item In contrast to regression models, even the best-fitting model
  has $\rho^2$ and $\bar{\rho}^2$ values far from 1. Values as low as
  0.3 may characterize a good model, \emph{see
    \hyperlink{sec:SPcomparison}{\beamerbutton{the Example 4.8a}}},
  while $R^2=0.3$ means a really bad fit for a regression model.
\pause \item An overfitted model with $M$ parameters fitting
  $N=M$ decisions reaches the ``ideal'' LR-index value $\rho^2=1$ 
 while $\bar{\rho}^2$ is near
  zero.
\ei
}

}


%##############################################################
\frame{\frametitle{Questions on GoF metrics}
%##############################################################

\bi
\itemAsk Discuss the model to be tested, the AC-only model, and the
trivial model in the context of weather forecasts.\\[1em]
\pause \itemAsk Give the log-likelihood of the AC-only and trivial models
if there are $I$ alternatives and $N_i$ decisions for alternative $i$ 
(total number of decisions $N=\sum_{i=1}^IN_i$).\\[1em]
\pause \itemAsk Consider a binary choice situation where the $N/2$ persons with
short trips chose the pedestrian/bike
option with a probability of 3/4,  and the PT/car option with 1/4. The
other $N/2$ persons with long trips had the reverse modal split with
a ped/bike usage of \unit[25]{\%}, only.

What would be the LRindex for the ``perfect'' model exactly reproducing the
observed 3:1 and 1:3 modal splits for the short and long trips,
respectively? {\tiny (less than 0.18)} 
\ei


}


\end{document}
