
%\documentclass[mathserif,aspectratio=1610]{beamer}
\documentclass[mathserif,handout,aspectratio=1610]{beamer}
%\usepackage{beamerthemeshadow}

\input{$HOME/tex/inputs/defsSkript}%$
\input{$HOME/tex/inputs/styleBeamerVkoekMa}%$
%\input{styleBeamerVkoekMa}%$

\usepackage{graphicx}

%\newcommand{\pathDiscrChoice}{$HOME/vorlesungen/Verkehrsoekonometrie_Ma/discrChoice_cc_Levmar} %$
\newcommand{\pathDiscrChoice}{figsDiscr} 

%##############################################################

\begin{document}

\section{Chapter 9: Inferential Statistics of Discrete-Choice Models}

%###################################################
\frame{  %title layout
%###################################################
\makePale{1.00}{0.50}{0.70}{1.40}{1.40}

\placebox{0.50}{0.45}{
  \figSimple{1.40\textwidth}{figsDiscr/RC_skript_4alt_r_eng_corrMatrix.png}}
%makePale{opacity}{centerXrel}{centerYrel}{wrel}\{hrel} 
\makePale{0.80}{0.50}{0.70}{1.40}{1.40}

\placebox{0.50}{0.82}{\parbox{0.6\textwidth}{\myheading{
Chapter 9: Inferential Statistics of Discrete-Choice Models
}}}

\placebox{0.50}{0.44}{\parbox{0.6\textwidth}{
{\large
\bi
\item 9.1. Maximum-Likelihood Estimation
\item 9.2 Estimation Errors: Variance-Covariance Matrix
\bi
\item 9.2.1 Example 1: SP Survey in the Audience
\item 9.2.2 Example 2: RP Survey in the Audience
\ei
\item 9.3 Significance Tests
\item 9.4 Goodness-of-Fit Measures
\ei
}
}}

}




\subsection{9.1. Maximum-Likelihood Estimation}

%###################################################
\frame{\frametitle{9.1. Maximum-Likelihood Estimation: 
the likelihood function}
%###################################################
\bi
\item The \bfdef{maximum-likelihood (ML)} estimation is applicable for
general stochastic models where the probabilities depend on a
 parameter vector
$\vecbeta$
\pause \item The goal is to maximize the \bfdef{likelihood function
  $L(\vecbeta)$}, i.e., the probability that the model predicts
  \emph{all} data points $(\vec{y}_n, \vec{x}_n)$, $n=1, ..., N$:
\maindm{
L(\vecbeta)=P\big(\ \hat{\vec{y}}_1(\vecbeta)=\vec{y}_1, \quad ..., \quad
\hat{\vec{y}}_N(\vecbeta)=\vec{y}_N\ \big)
}
where $\hat{\vec{y}}_n=\hat{\vec{y}}(\vec{x}_n)$ 
gives the model estimate for $\vec{x}_n$
\pause \item For continuous endogenous variables, the likelihood function is
  given by the multi-dimensional probability density at the data
  points:
\bdm
L(\vecbeta)=f_{\hat{\vec{y}}_1(\vecbeta), ..., \hat{\vec{y}}_N(\vecbeta)}
(\vec{y}_1, ..., \vec{y}_N)
\edm

\pause \itemAsk \colAsk{Verify that the density formulation is equivalent to
  the probability definition by requiring the model estimations to
  be in small intervals around the data instead of hitting the data exactly.} 
\pause \itemAnswer \colAnswer{The multi-dimensional probability density $f(.)$
  is defined such that 
$\diff{P}=f_{\hat{\vec{y}}_1, ..., \hat{\vec{y}}_N}(\vec{y}) 
\text{d}^N \vec{y}$. Keeping $\text{d}^N \vec{y}$ small and constant,
$\diff{P}$ and thus $P$ is maximized if and only if $f(.)$ is maximized.}
\ei
}

%###################################################
\frame{\frametitle{Maximum-likelihood estimation}
%###################################################
\bi
\item The ML method maximizes the likelihood function:
\maindm{
\hatvecbeta=\text{arg} \max_{\vecbeta} L(\vecbeta)
}

\pause \item Equivalently, and often better, one maximizes the
  \bfdef{log-likelihood}:
\maindm{
\hatvecbeta=\text{arg} \max_{\vecbeta} \tilL(\vecbeta), \quad
\tilL(\vecbeta)=\ln L(\vecbeta)}
\vspace{2em}

\pause \itemAsk \colAsk{Why it does not matter whether to maximize the
  likelihood or the log-likelihood?}
\pause \itemAnswer \colAnswer{Since, as a probability or probability
  density, $L>0$ and the log function is defined and strictly monotonously
  increasing in this range. Since (i) in this case
\bdm
x>y \Leftrightarrow f(x) > f(y)
\edm
(ii) the maximum function is based on this inequality
  relation, the \emph{argument} of the maximum remains unchanged.}
\ei
}



%###################################################
\frame{\frametitle{Application 1: Regression models}
%###################################################

Besides OLS, the ML can also be used to estimate regression
models. Does it give the same result, at least if the statistical Gau\3-Markow
  conditions are  satisfied? 
\bdma
\visible<2->{L(\vecbeta) &\stackrel{\text{$\epsilon_n$ independent}}{=}& 
 \prod_{n=1}^N f_n(y_n)}
\visible<3->{ \stackrel{\epsilon_n\sim i.d. N(0,\sigma^2)}{=} 
 \prod_{n=1}^N\frac{1}{\sqrt{2\pi\sigma^2}}
\exp\left[-\frac{(y_n-\vec{\beta}\vec{x}_n)^2}{2\sigma^2}\right],\\}
\visible<4->{\tilL(\vecbeta) &=& \sum_{n=1}^N \ln f_n(y_n)
 = \sum_{n=1}^N\left\{
 -\frac{1}{2}(\ln 2\pi+\ln \sigma^2)
 -\left[\frac{(y_n-\vec{\beta}\vec{x}_n)^2}{2\sigma^2}\right]\right\}
 \nonumber\\}
 \visible<5->{&=& -\frac{N}{2}(\ln 2\pi+\ln \sigma^2)
 -\frac{1}{2\sigma^2}(\vec{y}-\m{X}\vec{\beta})\tr(\vec{y}-\m{X}\vec{\beta})}
\edma
\visible<6->{Except for the irrelevant additive and multiplicative
  constants, this is the SSE function  of the 
OLS method and therefore leads to the same estimator!}

\vspace{1em}

\bi
\visible<7->{\itemAsk \colAsk{Why it is possible 
to express $L(\vecbeta)$ as a product?}}
\visible<8->{\itemAnswer \colAnswer{Since the random terms $\epsilon_n
    \sim i.i.d 
  N(0,\sigma^2)$, particularly, they are \emph{independent} from each
  other}}
\ei
}

%###################################################
\frame{\frametitle{Application 2: Discrete-choice models}
%###################################################
\bi
\item Probability to predict the chosen alternative $i_n$ for a
  \emph{single} decision $n$:
\bdma
P\left(\hatvec{Y}_{n}=\vec{y}_n\right)
&=& P\left(\hat{Y}_{n1}=y_{n1}, \ ..., \ \hat{Y}_{nI}=y_{nI}\right)\\
\visible<2->{&=& \prod_{i=1}^I
\left[P_{ni}(\vecbeta)\right]^{y_{ni}}=P_{ni_n}(\vecbeta)
\edma
(this relies on the exclusivity/completeness
of ${\cal A}_n$ and of independent RUs)
% but the product expression is
%more suitable for the further development)
}
\\[1em]
\visible<3->{\pause \item Probability to predict \emph{all} the
  decisions correctly assuming
  independent  decisions:
\bdma
\visible<3->{L(\vecbeta) &=&
P \left(\vec{Y}\!_{1}(\vecbeta)=\vec{y}_1,
...,\vec{Y}\!_{N}(\vecbeta)=\vec{y}_N \right)\\}
\visible<4->{ &=&  \prod_{n=1}^N \prod_{i=1}^I
\left[P_{ni}(\vecbeta)\right]^{y_{ni}}}
\edma
}
\ei
\vspace{-2em}

\visible<5->{ML estimation:
\maindm{\hatvecbeta=\text{arg} \max_{\vec{\beta}}\tilL(\vecbeta), \quad
 \tilL(\vecbeta)
=\sum\limits_{n=1}^N  
\sum\limits_{i=1}^I y_{ni} \ln P_{ni}(\vec{\beta})
=\sum\limits_{n=1}^N \ln P_{ni_n}(\vec{\beta})
}}

}

%###################################################
\frame{\frametitle{Question}
%###################################################
\bi
\itemAsk \parbox[t]{0.65\textwidth}{\vspace{-0.9em}

\colAsk { Show that, in deriving the main ML result
$\tilL=\sum_n\sum_iy_{ni}\ln P_{ni}$, the random utilities need not to
be uncorrelated between alternatives, only between choices}}
\vspace{2em}

\pause \itemAnswer \parbox[t]{0.65\textwidth}{\vspace{-0.9em}

\colAnswer{Because of the exclusivity/completeness
  requirement for the alternatives, exactly one alternative can be
  chosen per decision so it is enough to maximize the corresponding
  probability (which, of course, depends on possible correlations)}}

\ei
}

%###################################################
\frame{\frametitle{Estimating models with only ACs}
%###################################################

If there are no exogenous variables, we are left with just the ACs
reflecting that people prefer certain alternatives over others for
unknown reasons:
\bdm
V_{ni}=\sum_{m=1}^{I-1}\beta_m\delta_{mi} \quad \text{or} \quad
V_{ni}=\beta_i \ \text{if} \ i\neq I, \ V_{nI}=0
\edm
This \bfdef{AC-only model} 
will be the ``reference case'' when estimating the
model quality, e.g., by the \bfdef{likelihood-ratio index}.

{\small
\bi
\pause \itemAsk \colAsk{ Show that the estimated models gives probabilities
  $P_{ni}=P_i$ that are equal to the observed choice fractions $N_i/N$.
  (\emph{Hint:} Lagrange multiplicators to satisfy $\sum_iP_i=1$)}

\pause \itemAnswer \colAnswer{we have 
$\tilL(\vec{P})=\sum_n \ln P_{i_n}=\sum_iN_i\ln P_i$; \ maximize under
  the constraint $\sum_iP_i=1$:
\bdm
\abl{}{P_i}\left(\tilL(\vec{P})-\lambda(\sum_iP_i-1)\right)\stackrel{!}{=}0 
\ \Rightarrow \ \frac{N_i}{P_i}=\lambda \Rightarrow \ P_i \propto N_i 
\edm
}

\pause \itemAsk \colAsk{ Based on this result $P_i=N_i/N$, give the
  parameters for the 
 AC-only MNL and for the binary i.i.d. Probit model}
\pause \colAnswer{Logit: $P_i/P_I=N_i/N_I=\exp(\beta_i)$ (notice that
  $I$ is the reference w/o AC)}
\ei
}

}



%###################################################
\frame{\frametitle{Exercise: simple binomial model with an AC and
    travel time}
%###################################################

\placebox{0.50}{0.84}{$V_{ni}=\beta_1\delta_{i1}+\beta_2T_{ni}$}


\visible<2->{
\placebox{0.25}{0.25}
 {\figSimple{0.51\textwidth}{figsDiscr/kalLogitBinom_beta1beta2.png}}

\placebox{0.75}{0.25}
 {\figSimple{0.51\textwidth}{figsDiscr/kalProbitBinom_beta1beta2.png}}
}


\visible<1->{
\placebox{0.50}{0.65}{\parbox{\textwidth}{
{\footnotesize
\begin{center}
\begin{tabular}{|c||c|c|c|c|} \hline
Choice set
 & $T\sub{ped}=T_1$ [min]
 & $T\sub{bike}=T_2$ [min]
 & \# chosen 1
 & \# chosen 2 \\ \hline
1 & 15 & 30 & 3 & 2 \\
2 & 10 & 15 & 2 & 3 \\
3 & 20 & 20 & 1 & 4 \\
4 & 30 & 25 & 1 & 4 \\
5 & 30 & 20 & 0 & 5 \\
6 & 60 & 30 & 0 & 5 \\ \hline
\end{tabular}
\end{center}
}}}}

}

%###################################################
\frame{\frametitle{I: Graphical solution}
%###################################################

\visible<1>{\placebox{0.26}{0.54}
 {\figSimple{0.54\textwidth}{figsDiscr/kalLogitBinom_beta1beta2.png}}}

\visible<2->{\placebox{0.26}{0.54}
 {\figSimple{0.54\textwidth}{figsDiscr/kalLogitBinom_beta1beta2_graph.png}}}

\visible<3>{\placebox{0.74}{0.54}
 {\figSimple{0.54\textwidth}{figsDiscr/kalProbitBinom_beta1beta2.png}}}

\visible<4->{\placebox{0.74}{0.54}
 {\figSimple{0.54\textwidth}{figsDiscr/kalProbitBinom_beta1beta2_graph.png}}}

\visible<1->{\placebox{0.50}{0.84}{$V_{ni}=\beta_1\delta_{i1}+\beta_2T_{ni}$}}

\visible<2->{\placebox{0.30}{0.14}{\parbox{0.4\textwidth}{
\textbf{Logit}:\\[1ex]
$\tilL=-12$\\
$\hatbeta_1=-1.3, \ \hatbeta_2=-0.14,$\\
$\text{AC in minutes: } -\, \frac{\hatbeta_1}{\hatbeta_2}=\unit[-9]{min}$
}}}

\visible<4->{\placebox{0.80}{0.14}{\parbox{0.4\textwidth}{
\textbf{Probit}:\\[1ex]
$\tilL=-12$\\
$\hatbeta_1=-1.1, \ \hatbeta_2=-0.12,$\\
$\text{AC in minutes: } -\, \frac{\hatbeta_1}{\hatbeta_2}=\unit[-9]{min}$
}}}

}



%###################################################
\frame{\frametitle{II. Numerical solution}
%###################################################

\bi
\item
Generally, we have a nonlinear optimization problem. 
\item For
parameter-linear utilities, we know for the MNL that a maximum exists
and is unique.
\pause \item Standard methods of nonlinear optimization are possible:
\bi
\item \bfdef{Newton's and quasi-Newton method}: Fast but may be unstable
\item \bfdef{Gradient/steepest descent methods}: slow but reliable
\item \bfdef{Broyden-Fletcher-Goldfarb-Shanno (BFGS)} or 
\bfdef{Levenberg-Marquardt algorithm} combining gradient and Newton
methods. Such methods are used in many software packages
\item \bfdef{genetic algorithms} if the \emph{objective function
  landscape} is complicated (nonlinear utilities).
\ei
\ei
}

%###################################################
\frame{\frametitle{Special case: estimating the MNL}
%###################################################

The special structure of the MNL with parameter-linear utilities,
$V_{ni}=\sum_m \beta_mX_{mni}$ allows for an intuitive formulation of
the estimation problem: 
\maintextbox{0.8\textwidth}{The observed and modeled 
 \bfdef{property sums} sums of the
  factors $X$ for a given parameter $m$ should be the same
\bdma
X_m\sup{MNL} &=& X_m\sup{data}, \\
\sum_{n, i} x_{mni}\, P_{ni}(\hatvecbeta) &=&
\sum_{n, i} x_{mni}\, y_{ni} = \sum_n x_{mni_n}
\edma
\vspace{-1em}
}

}


%###################################################
\frame{\frametitle{Example: four factors, two alternatives}
%###################################################
MNL model,
$V_{ni}=\beta_1T_{ni}+\beta_2C_{ni}+\beta_3 g_i\delta_{i1}+\beta_4\delta_{i1}$,
$g_{\malepng}=0$, $g_{\femalepng}=1$:
\vspace{1em}
{\small


\bi
\item $X_1=T$: Total travel time for the chosen alternatives:
\bdm
T\sup{MNL} = \sum_{n,i} P_{ni}(\vecbeta)T_{ni}, \quad
T\sup{data} = \sum_{n,i} y_{ni} T_{ni}=\sum_nT_{ni_n}
\edm

\pause \item $X_2=C$: Total money spent by the decision makers:
\bdm
C\sup{MNL} = \sum_{n,i} P_{ni}(\vecbeta)C_{ni}, \quad
C\sup{data} = \sum_{n,i} y_{ni} C_{ni}=\sum_nC_{ni_n}
\edm

\pause \item $X_3=N_{1,\femalepng}$: number of woman choosing alternative 1:
\bdm
N_{1,\femalepng}\sup{MNL} = \sum_{n}  P_{n1}(\vecbeta)g_n, \quad
N_{1,\femalepng}\sup{data} = \sum_{n} y_{n1} g_n 
\edm

\pause \item $X_4=N_1$: total number of persons choosing alternative 1:
\bdm
N_1\sup{MNL} = \sum_{n}  P_{n1}(\vecbeta), \quad
N_1\sup{data} = \sum_{n} y_{n1}
\edm

\ei
}


}

\subsection{9.2 Estimation errors}

%###################################################
\frame{\frametitle{9.2 Estimation Errors: Variance-Covariance Matrix}
%###################################################

{\small
Since the log-likelihood is maximized at $\hatvecbeta$, we have
\bdm
\ablpart{\tilL}{\vecbeta}=0 \ \Rightarrow \ 
\tilL(\vec{\beta}) \approx \tilL\sub{max} + \frac{1}{2}
\Delta \vec{\beta}^{\,T} \cdot \m{H} \cdot \Delta \vec{\beta}, \quad
\Delta \vec{\beta}=\vecbeta-\hatvecbeta
\edm
with the (negative definite) 
Hessian $H_{lm}=\left.\ablpartmix{\tilL(\vecbeta)}{\beta_l}{\beta_m}
\right|_{\vecbeta=\hatvecbeta}$

\pause
Compare $L(\vec{\beta})$ near
its maximum with the density $f(\vec{x})$ of the general multivariate normal
distribution with 
variance-covariance matrix $\vec{\Sigma}$:
\bdma
L(\vec{\beta}) &=& L\sub{max}\exp\left( 
\frac{1}{2} \Delta \vec{\beta}^{\,T} \cdot \m{H} \cdot \Delta
\vec{\beta}\right),\\[-1ex]
f(\vec{x}) &=& \left((2\pi)^{M} \text{Det}\vec{\Sigma}\right)^{-1/2} \ 
\exp\left(-\frac{1}{2} \vec{x}\tr\vec{\Sigma}^{-1}\, \vec{x}\right)
\edma
\vspace{-1ex}

\pause Identify $\Delta \vecbeta$ with $\vec{x}$, the sought-after
variance-covariance matrix $\m{V}$ with $\vec{\Sigma}$,
  and assume the 
asymptotic limit (higher than quadratic terms in $\tilL(\hatvecbeta)$
negligible): \ $\Rightarrow$
}

\maindm{\m{V}=\text{Cov}(\hatvecbeta)
=E\left[\left(\vecbeta-\hatvecbeta\right)
\left(\vecbeta-\hatvecbeta\right)\tr\right] 
\approx -\m{H}^{-1}(\hatvecbeta)
}
}

%###################################################
\frame{\frametitle{Fisher's information matrix}
%###################################################
{\small
The variance-covariance matrix is related to 
\bfdef{Fisher's information matrix $\vec{\cal I}$}:
\bdm
\vec{\cal I}=\m{V}^{-1}=-\m{H}, \quad
I_{lm}=-\ablpartmix{\tilL(\hatvecbeta)}{\beta_l}{\beta_m}
\edm
\bi
\item
Roughly speaking, information is missing uncertainty, so the higher
the main components of $\vec{\cal I}$, the lower the main components
of $\m{V}$
\vspace{1ex}
\pause \item
\bfdef{Cram{\'e}r-Rao inequality:} A lower bound for the
variance-covariance matrix is the inverse of Fisher's information
matrix
$\Rightarrow$ The ML estimator is \bfdef{asymptotically efficient}
\vspace{1ex}

\pause \item Comparison with the OLS estimator 
$\m{V}\sub{OLS}=2\sigma^2 \m{H}\sub{SSE}^{-1}$ of regression models:
\bdm
 \vec{\cal I}=-\m{H}=\m{H}\sub{SSE}/(2\sigma^2)=\m{X}\tr\m{X}/\sigma^2
\edm
The negative Hesse matrix of $\tilL(\vecbeta)$ 
 is proportional to the Hesse matrix of the regression SSE $S(\vecbeta)$.
\ei
}
}




\subsection{9.2.1 Example 1: SP Survey in the Audience}
%~/vorlesungen/Verkehrsoekonometrie_Ma/discrChoice_cc_Levmar/SC_WS1819_3alt_globalT_fProb

%###################################################
\frame{\label{sec:SP}\frametitle{9.2.1 Example 1 from past lecture:\\ 
SP Survey in the Audience WS18/19 \red{(red: bad weather, $\bfmath{W=1}$)}}
%###################################################

\vspace{0.5em}
{\small
\hspace{5em}
\begin{tabular}{|r||c|c|c||r|r|r|}
\hline 
\myBox{2.5em}{Choice\\Set} &
  \myBox{3em}{Alt. 1:\\Ped} &
  \myBox{3em}{Alt. 2:\\Bike} &
  \myBox{3em}{Alt. 3:\\ PT/Car} &
  Alt 1 & Alt 2 & Alt 3 \\ \hline\hline
1  & 30\,min & 20\,min & 20\,min+0\euro{} & 1  & 3 & 7 \\[0.1em] \hline
2  & 30\,min & 20\,min & 20\,min+2\euro{} & 2  & 9 & 2\\[0.1em] \hline
3  & 30\,min & 20\,min & 20\,min+1\euro{} & 1  & 5 & 7\\[0.1em] \hline
4  & 30\,min & 20\,min & 30\,min+0\euro{} & 2  & 9 & 3\\[0.1em] \hline
5  & 50\,min & 20\,min & 30\,min+0\euro{} & 0  & 9 & 4\\[0.1em] \hline
6  & 50\,min & 30\,min & 30\,min+0\euro{} & 0  & 3 & 9\\[0.1em] \hline
7  & 50\,min & 40\,min & 30\,min+0\euro{} & 0  & 2 & 10\\[0.1em] \hline
8  & 180\,min & 60\,min & 60\,min+2\euro{} & 0 & 4 & 11\\[0.1em] \hline
9  & 180\,min & 40\,min & 60\,min+2\euro{} & 0 & 9 & 6\\[0.1em] \hline
\red{10}  & \red{180\,min}  & \red{40\,min} & \red{60\,min+2\euro{}}
  & \red{0}  & \red{1}  & \red{14}                   \\[0.1em] \hline
11 & 12\,min & 8\,min & 10\,min+0\euro{} & 3  & 5 & 6\\[0.1em] \hline
12 & 12\,min & 8\,min & 10\,min+1\euro{} & 5  & 7 & 2\\[0.1em] \hline
\end{tabular}
}
}

%###################################################
\frame{\frametitle{Model specification for Model 1 of the past lecture}
%###################################################

\placebox{0.35}{0.43}{
 \figSimple{0.75\textwidth}{\pathDiscrChoice/SC_WS1819_3alt_globalT_fProb.png}
}

\placebox{0.82}{0.77}{\parbox{0.3\textwidth}{
{\small
$
\begin{array}{lll}
 V_i &= & \beta_0\delta_{i1} + \beta_1\delta_{i2}\\
& + & \beta_2 K_i + \beta_{3} T_i
\end{array}
$
}}}


\placebox{0.80}{0.50}{\parbox{0.20\textwidth}{\small\fbox{ 
  $
  \begin{array}{l}
 % \ln L\sub{init}=-176.9,\\
 % \ln L=-141.5, \\
  \beta_0=-0.95 \pm 0.37, \\
  \beta_1=-0.28 \pm 0.24, \\
  \beta_2=+0.17 \pm 0.19, \\
  \beta_3=-0.04 \pm 0.02
  \end{array}
 $
}}}

\placebox{0.82}{0.25}{{\small
   $
   \begin{array}{l}
   \frac{\beta_0}{-\beta_3}=\unit[-22.4]{min},\\[1ex]
   \frac{\beta_1}{-\beta_3}=\unit[-6.6]{min},\\[1ex]
   \frac{60\beta_3}{\beta_2}=\unit[-15]{\text{\euro{}}/h}
   \end{array}
   $
}}

\placebox{0.16}{0.048}{\parbox{0.3\textwidth}{\scriptsize
AIC=275,
BIC=303,\\
$\rho^2=0.200$,
$\tilde{\rho}^2=0.177$
}}

}


%###################################################
\frame{\frametitle{Likelihood and log-likelihood function for varying
    cost ($\beta_2$) and time ($\beta_3$) sensitivities}
%###################################################

\placebox{0.50}{0.76}{\parbox{1.0\textwidth}{\small
  $ 
  V_i= \beta_0\delta_{i1} + \beta_1\delta_{i2} + \beta_2 K
   + \beta_3 T
  $
}}

\placebox{0.25}{0.42}{\parbox{0.55\textwidth}{
  \begin{center}
    \figSimple{0.55\textwidth}{\pathDiscrChoice/SC_WS1819_3alt_globalT_L_beta2_beta3.png}\\
    Likelihood function\\
    $L(\beta_2,\beta_3|\hatbeta_0, \hatbeta_0)$
  \end{center}
}}


\placebox{0.75}{0.42}{\parbox{0.55\textwidth}{
  \begin{center}
    \figSimple{0.55\textwidth}{\pathDiscrChoice/SC_WS1819_3alt_globalT_lnL_beta2_beta3.png}\\
    Log-likelihood function\\
    $\tilde{L}(\beta_2,\beta_3|\hatbeta_0, \hatbeta_1)$
  \end{center}
}}

}
%###################################################
\frame{\frametitle{Log-likelihood function in parameter space}
%###################################################

\placebox{0.50}{0.425}{
 \figSimple{0.88\textwidth}{\pathDiscrChoice/SC_WS1819_3alt_globalT_corrMatrix.png}
}

\placebox{0.65}{0.72}{\small
  $V_i=\beta_0\delta_{i1} + \beta_1\delta_{i2} + \beta_2 K
   + \beta_3 T + \beta_4 W\delta_{i3}
  $
}

}



\subsection{9.2.2 Example 2: RP Survey in the Audience}
% ~/vorlesungen/Verkehrsoekonometrie_Ma/discrChoice_cc_Levmar/RC_skript_4alt_r_eng

%##############################################################
\frame{\frametitle{9.2.2 Example 2: RP Survey in the Audience}
%##############################################################

Distance classes for the trip home to university (cumulated till 2018)
\vspace{0.5em}

Weather: good
\vspace{2em}

{\small
\hspace{3em}
\begin{tabular}{|r|r||c|c|c|c|}
\hline 
\myBox{4em}{Distance} & 
  \myBox{4em}{Class-\\center} &
  \myBox{3em}{Choice\\Alt. 1:\\ped} &
  \myBox{3em}{Choice\\Alt. 2:\\bike} &
  \myBox{3em}{Choice\\Alt. 2:\\PT} &
  \myBox{3em}{Choice\\Alt. 3:\\ car} \\ \hline\hline
\unit[0-1]{km} & \unit[0.5]{km}    &  17 & 16 & 10 & 0\\[0.2em] \hline
\unit[1-2]{km} & \unit[1.5]{km}    &  9  & 23 & 20 & 2\\[0.2em] \hline
\unit[2-5]{km} & \unit[3.5]{km}    &  2  & 27 & 55 & 4\\[0.2em] \hline
\unit[5-10]{km} & \unit[7.5]{km}   &  0  & 7  & 42 & 7\\[0.2em] \hline
\unit[10-20]{km} & \unit[12.5]{km} &  0  & 0  & 18 & 7\\[0.2em] \hline
\end{tabular}}

}



%###################################################
\frame{\frametitle{Revealed Choice: fit quality}
%###################################################

\placebox{0.37}{0.44}{
  \figSimple{0.70\textwidth}{\pathDiscrChoice/RC_skript_4alt_r_eng_fProb.png}
}

\placebox{0.85}{0.72}{\parbox{0.3\textwidth}{\small
$
\begin{array}{l}
V_1=\beta_1+\beta_4 r,\\ 
V_2=\beta_2+\beta_5 r,\\
V_3=\beta_3+\beta_6 r, \\
V_4=0
\end{array}
$
}}

\placebox{0.85}{0.40}{\parbox{0.30\textwidth}{\small
 $
 \begin{array}{l}
 \beta_1= 4.1\pm 0.6, \\
 \beta_2= 3.6\pm 0.5, \\
 \beta_3= 3.0\pm 0.5, \\
 \beta_4=-1.43\pm 0.26, \\
 \beta_5=-0.48\pm 0.08, \\
 \beta_6=-0.14\pm 0.05
 \end{array}
 $
}}

}

%###################################################
\frame{\frametitle{Revealed Choice: Modal split as a function of distance}
%###################################################

\placebox{0.37}{0.44}{
  \figSimple{0.70\textwidth}{\pathDiscrChoice/RC_skript_4alt_r_eng_fProbKumDist.png}
}

\placebox{0.85}{0.72}{\parbox{0.3\textwidth}{\small
$
\begin{array}{l}
V_1=\beta_1+\beta_4 r,\\ 
V_2=\beta_2+\beta_5 r,\\
V_3=\beta_3+\beta_6 r, \\
V_4=0
\end{array}
$
}}

\placebox{0.85}{0.40}{\parbox{0.30\textwidth}{\small
 $
 \begin{array}{l}
 \beta_1= 4.1\pm 0.6, \\
 \beta_2= 3.6\pm 0.5, \\
 \beta_3= 3.0\pm 0.5, \\
 \beta_4=-1.43\pm 0.26, \\
 \beta_5=-0.48\pm 0.08, \\
 \beta_6=-0.14\pm 0.05
 \end{array}
 $
}}


}



%###################################################
\frame{\frametitle{Likelihood and Log-Likelihood as $f(\beta_1,\beta_2)$}

\placebox{0.50}{0.76}{
$V_i=\sum_{m=1}^3\beta_m\delta_{m,i}
+\sum_{m=1}^3\beta_{m+3}\ r\delta_{m,i}$
}

\placebox{0.25}{0.45}{
 \figSimple{0.50\textwidth}{\pathDiscrChoice/RC_skript_4alt_r_eng_L_beta0_beta1.png}
}

\placebox{0.75}{0.45}{
 \figSimple{0.50\textwidth}{\pathDiscrChoice/RC_skript_4alt_r_eng_lnL_beta0_beta1.png}
}

\placebox{0.30}{0.15}{\parbox{0.4\textwidth}{
  Likelihoodfunktion\\
  $L(\beta_1,\beta_2,\hatbeta_3, ...)$
}}

\placebox{0.80}{0.15}{\parbox{0.4\textwidth}{
  Log-Likelihoodfunktion\\
   $\tilL(\beta_1,\beta_2,\hatbeta_3, ...)$
}}

}



%###################################################
\frame{\frametitle{Log-Likelihood: Sections through parameter space}
%###################################################
\figSimple{0.89\textwidth}{\pathDiscrChoice/RC_skript_4alt_r_eng_corrMatrix.png}

{\small
$V_i=\sum_{m=1}^3\beta_m\delta_{m,i}
+\sum_{m=1}^3\beta_{m+3}\ r\delta_{m,i}$
}

}




\subsection{9.3 Significance Tests}


%##############################################################
\frame{\frametitle{9.3 Significance Tests: Parametric Tests}
%##############################################################
\bi
\item The parameter test procedures are exactly the same as that of
regression models. Because we only consider the asymptotic limit, the test
  statistic is always Gaussian:

\pause \item Confidence interval of a parameter $\beta_m$:
\bdm
\text{CI}_{\alpha}(\beta_m)=[\hatbeta_m-\Delta_{\alpha},
  \hatbeta_m+\Delta_{\alpha}], \quad 
\Delta_{\alpha}=z_{1-\alpha/2}\sqrt{V_{mm}}
\edm

\pause \item Test of a parameter $\beta_m$ for 
$H_0: \beta_j=\beta_{j0}$, \ $\ge
  \beta_{j0}$, \ or $\le \beta_{j0}$:
\bdm
T=\frac{\hatbeta_j-\hatbeta_{j0}}{\sqrt{V_{jj}}}\sim N(0,1) \ | H_0^*
\edm

\pause \item $p$-values for 
$H_0: \beta_j=\beta_{j0}$, \ $\ge
  \beta_{j0}$, \ or $\le \beta_{j0}$, respectively:
{\small
\bdm
p_{=}=2\big(1-\Phi(|t\sub{data}|)\big), \quad
p_{\le}=1-\Phi(t\sub{data}), \quad
p_{\ge}=\Phi(t\sub{data})
\edm
}
\pause \item As in regression, a factor 4 of more data halves the error
\ei

}

%##############################################################
\frame{\frametitle{Significance tests II: Likelihood-ratio (LR) test}
%##############################################################
Like in regression (F-test), one sometimes wants to test null hypotheses
fixing
several parameters \emph{simultaneously} to given values, i.e., $H_0$
corresponds to a \bfdef{restraint model}
\bi
\pause \item $H_0$: The restraint model with some fixed parameters and
  $M_r$ remaining parameters describes the data as well as the full
  model with $M$ parameters

\pause \item Test statistics: 

{\small
\maindm{
\lambda\sup{LR}=2\ln\left(
  \frac{L\left(\hatvecbeta\right)}{L\sup{r}\left(\hatvecbeta\sup{r}\right)}\right)
=2\left[
  \tilL\left(\hatvecbeta\right) 
 - \tilL\sup{r}\left(\hatvecbeta\sup{r}\right)
\right]
\sim \chi^2(M-M_r) \ \text{if}\ H_0
}
}
\vspace{1ex}

\pause \item Data realization:
calibrate both $M$ and $M_r$ and evaluate $\lambda\sup{LR}\sub{data}$

\pause \item Result:
reject $H_0$ at $\alpha$ based on the
$1-\alpha$ quantile:
\bdm
\lambda\sup{LR}\sub{data}>\chi^2_{1-\alpha, M-M_r}
\edm
$p$-value: $p=1-F_{\chi^2(M-M_r)}\left(\lambda\sup{LR}\sub{data}\right)$
\ei
}

%##############################################################
\frame{\frametitle{Example: Mode choice for the route to this lecture}
%##############################################################

{\small
\begin{center}
\begin{tabular}{|l||r|r|r|} \hline
Distance class $n$ & \myBox{4em}{Distance $r_n$} &
$i=1$ (ped/bike) & $i=2$ (PT/car) \\
\hline\hline
$n=1$: 0-1 km    & \unit[0.5]{km}  & 7 & 1\\
$n=2$: 1-2 km    & \unit[1.5]{km}  & 6 & 4\\
$n=3$: 2-5 km    & \unit[3.5]{km}  & 6 & 12\\
$n=4$: 5-10 km   & \unit[7.5]{km}  & 1 & 10\\
$n=5$: 10-20 km  & \unit[15.0]{km} & 0 & 5\\ \hline
\end{tabular}
\end{center}
\vspace{-0.5em}

\bdma
V_{n1}(\beta_1,\beta_2) &=& \beta_1r_n+\beta_2,\\
V_{n2}(\beta_1,\beta_2) &=& 0
\edma
\vspace{-1.5em}

\bi
\item $\beta_1$: Difference in distance sensitivity (utility/km)
  for choosing ped/bike over PT/car (expected $<0$)
\item $\beta_2$: Utility difference ped/bike over PT/car at zero
  distance ($>0$)
\ei
\pause \bfAsk{Do the data allow to distinguish this model from the trivial
  model $V_{ni}=0$?}
}

}

%##############################################################
\frame{\frametitle{LR test for the corresponding Logit models}
%##############################################################
\vspace{1ex}

\fig{0.56\textwidth}{figsDiscr/BNL_Entfernung_smallSample_logL_eng.png}
\vspace{-1em}

{\small
\bi
\item $H_0$: The trivial model $V_{ni}=0$ describes the data as well
  as the full model
  $V_{n1}(\beta_1,\beta_2)=(\beta_1r_n+\beta_2)\delta_{i1}$
\pause \item Test statistics: \  $\lambda\sup{LR}=2\left[
  \tilL(\hatbeta_1,\hatbeta_2)-\tilL(0,0)\right] \sim \chi^2(2)|H_0$
\pause \item Data realization (1 $\tilL$-unit per contour): \  
$\lambda\sup{LR}\sub{data}=2(-26.5+35.5)=18$
\pause \item Decision: \ Rejection range $\lambda\sup{LR}>\chi^2_{2,0.95}=5.99
  \ \Rightarrow$ \red{$H_0$ rejected.}
\ei
}

}

%##############################################################
\frame{\frametitle{Fit quality of the full model}
%##############################################################

\placebox{0.31}{0.45}{\figSimple{0.64\textwidth}
 {figsDiscr/BNL_Entfernung_smallSample_relHaeuf_eng.png}}

\placebox{0.77}{0.45}{\parbox{0.40\textwidth}{\small
\bi
\itemAsk \colAsk{ What would be the modeled ped/bike modal split for
  the null model $V_{ni}=0$?} \pause \colAnswer{50:50}
\pause \itemAsk \colAsk{ Read off from the $\tilL$ contour plot the parameter of
  the AC-only model $V_{ni}=\beta_2\delta_{i1}$ and give the modeled
  modal split} \pause \colAnswer{$\hatbeta_2=\ln(P_1/P_2)=-0.5$, OK
  with $P_1/P_2=e^{\hatbeta_2} \approx N_1/N_2=20/32$}
\pause \itemAsk \colAsk{ Motivate the negative correlation between the
  parameter errors} \pause \colAnswer{This makes at least sure that,
  in case of correlated errors,
about the same fraction chooses
  alternative~2 as for the calibrated model}
\ei
}}

}

\subsection{9.4 Goodness-of-Fit Measures}

%##############################################################
\frame{\frametitle{9.4 Goodness-of-Fit Measures}
%##############################################################

\bi
\item The parameter tests for equality and the LR test are related to
  \bfdef{significance}: Is the more complicated of two nested models
  significantly better in describing the data?
\item This can be used to find the best model using the
  \bfdef{top-down ansatz}: \\
  \EinsteinPdflatex \emph{Make is as simple
    as possible but not simpler!}
\pause \item Problem: For very big samples, nearly any new parameter gives
  significance and the top-down ansatz fails
\pause \item More importantly: Significance/LR tests cannot give
evidence for missing 
  but relevant factors 
\pause \item A further problem: We cannot compare non-nested models
\pause \item Finally, in reality, one often is interested in \bfdef{effect
  strength} (difference in the fit and validation quality), not
  significance
\ei

\pause 
\centering{\bfred{$\Rightarrow$ we need measures for absolute fit
    quality}}
}

%##############################################################
\frame{\frametitle{Information-based goodness-of-fit (GoF) measures}
%##############################################################


\bi
\item \bfdef{Akaike's information criterion}: % \vspace{-1ex}
  \bdm \text{AIC}=-2\tilL+2M\frac{N}{N-(M+1)} \edm

\item \bfdef{Bayesian information criterion:} % \vspace{-1ex}
   \bdm \text{BIC}=-2\tilL+M\ln N \edm
\ei


$N$: number of decisions; $M$: number of parameters

{\small
\bi
\pause \item Both criteria give the needed additional information (in
bit) to obtain the actual micro-data from the
  model's prediction, including an over-fitting penalty: the lower, the better.
\pause \item Both the AIC and BIC are equivalent to the corresponding
GoF measures of regression.
\pause \item the BIC focuses more on parsimonious models (low $M$).
\pause \item For nested models satisfying the null hypothesis of the LR test
  and $N\gg M$,
  the expected AIC is the same (\bfAsk{verify!}). However, since the
  AIC is an absolute 
  measure, it allows comparing non-nested models.
\ei
}

}

%##############################################################
\frame{\frametitle{GoF measures corresponding to the coefficient of
    determination $\bfmath{R^2}$ of linear models 
($\tilL\sup{0}$: log-likelihood of the
estimated AC-only or trivial model)}
%##############################################################

{\small
\bi
\item \bfdef{LR-Index} resp. \bfdef{McFadden's $R^2$}:  \vspace{-2ex}
   \bdm \rho^2=1-\frac{\tilL}{\tilL\sup{0}} \edm
 
\item \bfdef{Adjusted LR-Index/McFadden's $R^2$:}  \vspace{-1ex}
  \bdm \bar{\rho}^2=1-\frac{\tilL-M}{\tilL\sup{0}}\edm
\ei
}

{\footnotesize
\bi
\pause \item The LR-Index $\rho^2$ and the adjusted LR-Index $\bar{\rho}^2$
   correspond to the coefficient of
    determination $R^2$ and the adjusted coefficient $\tilde{R}^2$ of
    regression models, respectively: The higher, the better.
\pause \item In contrast to regression models, even the best-fitting model
  has $\rho^2$ and $\bar{\rho}^2$ values far from 1. Values as low as
  0.3 may characterize a good model, \emph{see
    \hyperlink{sec:SP}{\beamerbutton{the Example 9.2.1}}},
  while $R^2=0.3$ means a really bad fit for a regression model.
\pause \item An over-fitted model with $M$ parameters fitting
  $N=M$ decisions reaches the ``ideal'' LR-index value $\rho^2=1$ 
 while $\bar{\rho}^2$ is near
  zero.
\ei
}

}


%##############################################################
\frame{\frametitle{Questions on GoF metrics}
%##############################################################

\bi
\itemAsk \colAsk{Discuss the model to be tested, the AC-only model, and the
trivial model in the context of weather forecasts}\\
\pause \colAnswer{Full forecast info, info from climate table, 50:50}
\\[1em]
\pause \itemAsk \colAsk{Give the log-likelihood of the AC-only and trivial models
if there are $I$ alternatives and $N_i$ decisions for alternative $i$ 
(total number of decisions $N=\sum_{i=1}^IN_i$)}\\
\pause \colAnswer{Trivial model: 
$P_{ni}=1/I$, $\tilL=\sum_n \ln P_{i_n}=\sum_i N_i \ln P_i=-N\ln I$;\\
  AC-only model: $P_{ni}=N_i/N$, 
$\tilL=\sum_i N_i \ln P_i=N\ln N-\sum_iN_i\ln N_i$}
\\[1em]
\pause \itemAsk \colAsk{Consider a binary choice situation where the $N/2$ persons with
short trips chose the pedestrian/bike
option with a probability of 3/4,  and the PT/car option with 1/4. The
other $N/2$ persons with long trips had the reverse modal split with
a ped/bike usage of \unit[25]{\%}, only.

What would be the LR-index for the ``perfect'' model exactly reproducing the
observed 3:1 and 1:3 modal splits for the short and long trips,
respectively?} \\
\colAnswer{(less than 0.18)} 
\ei

}



\end{document}
