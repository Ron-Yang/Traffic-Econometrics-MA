%#############################################
% New pdflatex format,
% only png, jpg (or svg?) images but positioning now ok
% template in ~/tex/inputs/template_folien.tex
% [TODO now specialized to Vkoem_Ma; make general]
%#############################################


%\documentclass[mathserif]{beamer}
\documentclass[mathserif,handout]{beamer}
%\usepackage{beamerthemeshadow}
\input{$HOME/tex/inputs/defsSkript}%$
\input{$HOME/tex/inputs/styleBeamerVkoekMa}%$
\usepackage{graphicx}


%##############################################################

\begin{document}

\section{3. Classical Inferential Statistics}

%###################################################
\frame{  %title other layout
%###################################################
\placebox{0.50}{0.50}{
  \figSimple{1.70\textwidth}{figsRegr/regression3dFig.png}}

\makePale{0.85}{0.50}{0.55}{1.20}{1.50}

\placebox{0.50}{0.55}{\parbox{0.6\textwidth}{
\bi
\item[3.1] Expectation and Covariance Matrix of the OLS Estimator
\item[3.2] Confidence Intervals
\item[3.3] Type I and II Errors
\item[3.4] Significance Tests
\item[3.5] Model Selection Strategies
\ei
}}

\placebox{0.50}{0.90}{\myheading{
 Lecture 03: Classical Inferential Statistics}}
}


\subsection{3.1. OLS Expectation and Covariance}

%###################################################
\frame{\frametitle{3.1. OLS Expectation and Covariance}
%###################################################

\bi
\item Only stochasticity: residual errors
  $\veceps$ according to  $\vec{y}=\m{X}\vec{\beta}+\veceps$
\pause \item The OLS estimator is linear in $\vec{y}$:
\bdma
\hatvecbeta 
  &=&
    \left(\m{X}\tr\m{X}\right)^{-1}\m{X}\tr\vec{y} \\ 
 \visible<3->{ &=&
  \left(\m{X}\tr\m{X}\right)^{-1}\m{X}\tr(\m{X}\vec{\beta}+\veceps)\\}
 \visible<4->{ &=&
  \vec{\beta}+\left(\m{X}\tr\m{X}\right)^{-1} \m{X}\tr\veceps}
\edma
\ei

\visible<5->{\mysubheading{Expectation value}

\bdm
E(\hatvecbeta)=E(\vecbeta)
 +\left(\m{X}\tr\m{X}\right)^{-1} \m{X}\tr E(\veceps)=\vecbeta
\edm
}


\visible<6->{\maintextbox{0.8\textwidth}{
The OLS estimator of parameter-linear models is \bfdef{unbiased} under
the mild condition $E(\veceps)=\vec{0}$ for all the data points}}

}

 %###################################################
\frame{\frametitle{OLS estimation error II: variances and covariances}
%###################################################

\bi
\item Gau\3-Markow conditions $\to \epsilon \sim
  \text{i.i.d} N(0,\sigma^2) \to \hatvecbeta $ is normal distributed
\pause \item In this case, the complete errors are specified by the
  expectation value and the \bfdef{variance-covariance matrix}
$\m{V}_{\hatvecbeta}$

\ei

\pause
\bdma
\m{V}_{\hatvecbeta} &\stackrel{\text{def}}{=}&
 E\left( (\hatvecbeta-\vecbeta) (\hatvecbeta-\vecbeta)\tr\right)\\
  \visible<4->{\remark{insert
      $\hatvecbeta=\vec{\beta}
+\left(\m{X}\tr\m{X}\right)^{-1}\m{X}\tr\veceps$} 
     &=& 
    E\left( (\m{X}\tr\m{X})^{-1}\m{X}\tr\veceps
     \left( (\m{X}\tr\m{X})^{-1}\m{X}\tr\veceps\right)\tr\right) \\}
  \visible<5->{\remark{transpose rule} &=& 
     E\left( (\m{X}\tr\m{X})^{-1}\m{X}\tr \veceps\veceps\tr
     \m{X}(\m{X}\tr\m{X})^{-1}\right)\\}
  \visible<6->{\remark{$E(.)$ acts only on $\veceps$} &=& 
     (\m{X}\tr\m{X})^{-1}\m{X}\tr E(\veceps\veceps\tr)
     \m{X}(\m{X}\tr\m{X})^{-1}\\}
  \visible<7->{\remark{Gau\3-Markow} &=& 
     (\m{X}\tr\m{X})^{-1}\m{X}\tr\sigeps^2\m{X}(\m{X}\tr\m{X})^{-1}\\}
  \visible<8->{\remark{def inverse matrix} &=& 
     \sigeps^2(\m{X}\tr\m{X})^{-1} }
\edma
\visible<9->{The variance-covariance matrix depends only on the values of the
exogenous factors!}


}

%###################################################
\frame{\frametitle{Results summary}
%###################################################

{\small
\bi
\item Ordinary least squares (OLS) estimator:
\maindm{\hatvecbeta =\left(\m{X}\tr\m{X}\right)^{-1}\m{X}\tr\vec{y}}

\pause \item \bfdef{Variance-Covariance matrix} of the estimation errors
(provided the errors are i.i.d.) in terms of the \bfdef{Hesse matrix} $\m{H}$ of the 
objective function:

\maindm{
\begin{array}{rcl}
\m{V}_{\hatvecbeta}
 &=&  E\left( (\hatvecbeta-\vecbeta)(\hatvecbeta-\vecbeta)\tr\right)
 =\sigma^2 \left(\m{X}\tr\m{X}\right)^{-1}= 2\sigma^2 \m{H}^{-1},\\
H_{jk} &=& \left. \ablpartmix{S}{\beta_j}{\beta_k}\right|_{\vecbeta=\hatvecbeta}
=2\m{X}\tr\m{X}
\end{array}
}

\pause \item Variances of estimation errors: $V(\hatbeta_j)=V_{jj}$

\pause \item Correlation of estimation errors: $\text{Corr}(\hatbeta_j, \hatbeta_k)=\frac{V_{jk}}{\sqrt{V_{jj}V_{kk}}}$

\pause \item Distribution of the normalized estimation errors:
$
\frac{\hatbeta_j-\beta_j}{\sqrt{V_{jj}}} \sim N(0,1)
$
\ei
}

}
 
%###################################################
\frame{\frametitle{Estimation of the residual variance}
%###################################################
The above cannot be applied directly since the residual variance $\sigma^2$ is
unknown and must be estimated by the minimum $S(\hatbeta)$ of the sum
of squared 
errors: 
\maindm{
\hatsig^2 = \frac{1}{n-J-1}\sum_i \left(y_i-\hat{y}(\vec{x}_i)\right)^2
 = \frac{S(\hatbeta)}{n-J-1}
}
\bdm
\hatsig^2 =\frac{\vec{y}\tr
  \left(\m{1}-\m{X}(\m{X}\tr\m{X})^{-1}\m{X}\tr\right)\vec{y}} {n-J-1}
\edm

This results in following \emph{estimated} error statistics:

\bi
\pause \item Estimated variance-covariance matrix:

\maindm{\hat{\m{V}}_{\hatvecbeta}= 2\hatsig^2 \m{H}^{-1}
=\hatsig^2 \left(\m{X}\tr\m{X}\right)^{-1}
}

\pause \item The normalized 
approximate estimation errors are student-t distributed:
\bdm
\frac{\hatbeta_j-\beta_j}{\sqrt{\hat{V}_{jj}}} \sim T(n-1-J)
\edm
\ei
}
 
%###################################################
\frame{\frametitle{Multivariate distribution function of $\hatvecbeta$}
%###################################################

{\small
The distribution of the errors $\Delta
\hatvecbeta=\hatvecbeta-\vecbeta$ obeys a multivariate normal
distribution: 
\bdm
f_{\hatvecbeta}(\Delta \hatvecbeta)
 \propto \exp\left[-\frac{1}{2} 
 \Delta \hatvecbeta\tr \m{V}^{-1}\, \Delta \hatvecbeta\right]
   = \exp\left[-\frac{\Delta \hatvecbeta\tr \m{X}\tr\m{X}\Delta \hatvecbeta}
  {2\sigeps^2}\right].
\edm

\mysubsubheading{Relation to the maximum-likelihood-method ($\to$
  Lecture 06:)}

Expand the SSE $S(\vecbeta)$ around $\hatvecbeta$ to second order:
\bdm
S(\vecbeta)-S(\hatvecbeta)
\approx \frac{1}{2}\Delta \hatvecbeta\tr \m{H} \Delta \hatvecbeta
= \Delta \hatvecbeta\tr \m{X}\tr\m{X} \Delta \hatvecbeta
\edm
\vspace{1em}

\hspace{5em} $\Rightarrow \quad$ 
\vspace{-3em}

\maindm{f_{\hatvecbeta}(\Delta \hatvecbeta)
 \propto \exp\left[-\frac{S(\vecbeta)-S(\hatvecbeta)}{2\sigeps^2}\right] 
}

and with estimated residual variance $\hatsigeps^2=S(\hatvecbeta)/(n-J-1)$ 
\bdm
\hat{f}_{\hatvecbeta}(\vecbeta) \propto \exp \left[
-\frac{(n-J-1)}{2}\left(\frac{S(\vecbeta)}{S(\hatvecbeta)}-1\right)
\right]
\edm
}

}


%###################################################
\frame{\frametitle{Special case 1: No exogenous variables}
%###################################################
\bi
\item Model: $y=\beta_0+\epsilon$
\pause \item System matrix: $\m{X}=(1,1, ..., 1)\tr$
\pause \item OLS estimator: 
\bdma 
\left(\m{X}\tr\m{X}\right)^{-1}=\frac{1}{n}, \quad 
\m{X}\tr \vec{y}=\sum_iy_i=n\bar{y}, &&\\
\hatbeta_0=\hat{\mu}=\left(\m{X}\tr\m{X}\right)^{-1}\m{X}\tr \vec{y}=\bar{y}
\edma
\pause \item Variance:
  $V_{00}=V(\hat{\mu})=\sigma^2\left(\m{X}\tr\m{X}\right)^{-1}
=\frac{\sigma^2}{n}$, \quad
$\hat{V}_{00}=\frac{\hatsig^2}{n}$
\vspace{1em}

\pause \item Distribution of the estimator
 \red{(if $\epsilon \sim i.i.d N(\mu,\sigma^2)$)}
\bdm
\frac{\hatbeta_0-\beta_0}{\sqrt{V_{00}}}
=\frac{\bar{x}-\mu}{\sigma}\ \sqrt{n} \sim N(0,1), \quad
\frac{\bar{x}-\mu}{\hatsig}\ \sqrt{n} \sim T(n-1)
\edm
\ei
}

%###################################################
\frame{\frametitle{Special case 2: Simple linear regression}
%###################################################
\bi
\item Model (with $x_1=x$): $y=\beta_0+\beta_1x+\epsilon$

\pause \item System matrix: 
\bdm \m{X}=\myMatrixTwo{1 & x_1 \\ \vdots & \vdots \\ 1 & x_n},\quad
\m{X}\tr\m{X}=\myMatrixTwo{n & n\bar{x}\\n\bar{x} & \sum x_i^2}
\edm

\pause \item OLS estimator (with $s_x^2=1/n(\sum x_i^2-n\bar{x})$): 
\bdm
\left(\m{X}\tr\m{X}\right)^{-1}
 =\frac{1}{ns_x^2}
\myMatrixTwo{\frac{\sum x_i^2}{n} & -\bar{x} \\ -\bar{x} & 1}, \quad
\m{X}\tr \vec{y}=\myVector{n\bar{y}\\ \sum x_iy_i} 
\edm
\bdma
\hatbeta_1 &=& \left(-\frac{\bar{x}}{ns_x^2}, \frac{1}{ns_x^2}\right)
\myVector{n\bar{y}\\ \sum x_iy_i}
=\frac{\sum_ix_iy_i-n\bar{x}\bar{y}}{\sum x_i^2-n\bar{x}}
=\frac{s_{xy}}{s_x^2}, \\
\hatbeta_0 &=& \bar{y}-\hatbeta_1 \bar{x}
\edma
\ei
}

%###################################################
\frame{\frametitle{Simple linear regression (ctnd)}
%###################################################

\bi
\item Variance-covariance matrix (assuming w/o loss of generality
$\bar{x}=0$): 
\bdm
\m{V}(\hatvecbeta)=\sigma^2 \left(\m{X}\tr\m{X}\right)^{-1}
=\sigma^2 \myMatrixTwo{\frac{1}{n} & 0 \\ 0 & \frac{1}{ns_x^2}}
\edm

\pause \item Variance of the estimator $\hat{y}(x)$ ($x$ is deterministic):
\bdm
V(\hat{y}(x))=V(\hatbeta_0+\hatbeta_1x)
 = V_{00}+x^2  V_{11} +2xV_{01}
 = \frac{\sigma^2}{n}\left(1+\frac{x^2}{s_x^2}\right)
\edm

\pause \item Distribution of the estimator for $y(x)$:
\bdm
\hat{y}(x)\sim N\big(y(x), V(\hat{y}(x))\big)
\edm
If $\sigma^2$ has to be estimated by $\hatsig^2$, the normalized
estimators for $\beta_0$, $\beta_1$ and $y(x)$ are $\sim T(n-2)$.

\ei
}



%###################################################
\frame{\frametitle{Probability density for $\hat{y}(x)$ for simple 
linear regression}
%###################################################

\vspace{-0.5em}
\fig{0.8\textwidth}{figsRegr/regression3dFig.png}
\vspace{-2em}

\bi
\item If the Gau\3-Markov assumptions apply, the model
  estimation errors $\hat{y}(x)-y(x)$ are
  Gaussian distributed
\item The expectation and variance depends on $x$; the standard error
  is hyperbola-shaped.
\ei

}

\subsection{3.2 Confidence Intervals}

%###################################################
\frame{\frametitle{3.2. Confidence intervals and the
Student-t distribution}
%###################################################

\visible<1>{\placebox{0.489}{0.518}{
  \figSimple{1.02\textwidth}{figsRegr/KI_veranschaulichung_eng1.png}}}

\visible<2>{\placebox{0.48}{0.5}{
  \figSimple{1.0\textwidth}{figsRegr/KI_veranschaulichung_eng2.png}}}

\visible<3>{\placebox{0.502}{0.503}{
  \figSimple{1.05\textwidth}{figsRegr/KI_veranschaulichung_eng3.png}}}

\visible<4>{\placebox{0.502}{0.503}{
  \figSimple{1.05\textwidth}{figsRegr/KI_veranschaulichung_eng4.png}}}


}

%###################################################
\frame{\frametitle{Densities of standard normal 
\textit{vs.} Student-t distribution}
%###################################################
\vspace{-0.5em}
\fig{0.95\textwidth}{figsRegr/f_gaussStudent_eng.png}
}


%###################################################
\frame{\frametitle{Distributions of standard normal 
\textit{vs.} Student-t-distribution}
%###################################################
\vspace{-0.5em}

\fig{0.95\textwidth}{figsRegr/F_gaussStudent_eng.png}
}

%###################################################
\frame{\frametitle{Confidence intervals (CI)}
%###################################################

\placebox{0.50}{0.81}{\parbox{1.0\textwidth}{
\maindm{\text{CI}_{\beta_j}^{(\alpha)}: \beta_j \in \left[
 \, \hatbeta_j-\Delta \hatbeta_j,  \hatbeta_j+\Delta \hatbeta_j \,
\right], \quad
\Delta \hatbeta_j =t_{1-\alpha/2}^{(n-J-1)}\hatsig_{\hatbeta_j}.
}}}

\placebox{0.50}{0.70}{\parbox{1.0\textwidth}{
 {\small
 \bi
  \pause \item $t_{1-\alpha/2}$: Quantile (inverse of) the distribution
    function\\[-1ex]
  \pause \item CI ``uncertainty principle'': Higher sensitivity implies
    higher $\alpha$ error.
  \ei
}
}}

\visible<4>{\placebox{0.50}{0.31}
{\figSimple{0.70\textwidth}{figsRegr/f_student_KI_eng.png}}}

\visible<5>{\placebox{0.50}{0.31}
{\figSimple{0.70\textwidth}{figsRegr/F_student_KI_eng.png}}}



}




%###################################################
\frame{\frametitle{Significance tests: General four-step procedure}
%###################################################
\benum
\item Formulate a \bfblue{null hypothesis} $H_0$ such that their rejection
  gives insight, e.g. $\beta_j=\beta_{j0}$ (point hypothesis) or
  $\beta_j \le \beta_0$ (interval hypothesis): Notice: \emph{\red{One cannot confirm $H_0$}}
\pause \item Select a \bfblue{test function} or \bfblue{statistics} $T$
\bi
\item whose distribution is known provided $H^*_0$, i.e., the
  parameters lie at the margin of $H_0$ (of course, $H^*_0=H_0$ for a point null hypothesis)
\item which has distinct \bfblue{rejection regions} $R(\alpha)$ which
  are reached rarely 
  (with a probability $\le \alpha$) if $H_0$ but more often if
  $H_1=\overline{H_0}$ 
\ei

\pause \item Evaluate a realisation $t\sub{data}$ of $T$ from the data

\pause \item Check if $t\sub{data} \in R(\alpha)$. If yes, $H_0$ can be
  rejected at an error probability or \bfblue{significance level}
  $\alpha$. Otherwise, nothing can be said.
\pause \item[4a] Alternatively, calculate the \bfblue{$p$-value} as
the minimum $\alpha$ for which $H_0$ can be rejected.
\eenum
}

%###################################################
\frame{\frametitle{Examples for test statistics I}
%###################################################

{\small
\bi
\item Testing  \red{parameters} such as $H_0$: $\beta_j=\beta_{j0}$ or
  $\beta_j\ge \beta_{j0}$ or  $\beta_j\le \beta_{j0}$:

  The test
  function is the estimated deviation from the boundary of $H_0$ in
  units of the estimated error standard deviation:
\bdm 
T =\frac{\hatbeta_j-\beta_{j0}}{\sqrt{\hat{V}_{jj}}} \sim T(n-1-J)
\edm
\pause \item Testing \red{functions of parameters} such as
  $H_0$: $\beta_1/\beta_2=2$, $\le 2$ or $\ge 2$: Transform into a linear
  combination. Then, the normalized estimated deviation is student-t
  distributed under $H_0^*$. Here, the linear combination is
  $b=\beta_1-2\beta_2=0$: 
\bdma
\hat{b} &=& \hatbeta_1-2\hatbeta_1, \\
\hat{V}(\hat{b}) &=& \hat{V}_{11}+4\hat{V}_{22}-4\hat{V}_{12}, \\
T &=& \frac{\hat{b}}{\sqrt{\hat{V}(\hat{b})}} \sim T(n-1-J)
\edma
\ei
}

}

%###################################################
\frame{\frametitle{Examples for test statistics II}
%###################################################

\bi
\item Testing the \red{correlation coefficient} in an $xy$ scatter plot:
\bdm
\hat{\rho}=\frac{s_{xy}}{s_xs_y}, \quad H_0: \rho=0, \quad
T=\frac{\hat{\rho}}{\sqrt{1-\hat{\rho}^2}}\sqrt{n-2} \sim T(n-2)
\edm

{\footnotesize
\emph{Derivation:} $\rho \neq 0$ if, and only if, 
in a simple linear regression
$y=\beta_0+\beta_1x+\epsilon$, the slope parameter $\beta_1=0$, so
test for $\beta_1=0$: Under $H_0$, the test statistics
\bdm
T=\hatbeta_1/\sqrt{\hat{V}_{11}}=\frac{s_{xy}}{\hat{\sigma}s_x}\sqrt{n}
 \sim T(n-2)
\edm
Now insert $\hat{\sigma}$ which can, in the simple-regression case, be
explicitely calculated:
 $\hat{\sigma}^2=n (s_y^2 -s_{xy}^2/s_x^2)/(n-2)$
}

\vspace{1em}

\item Test for the \red{residual variance}, $H_0$: $\sigma^2=\sigma_0^2$, 
$\sigma^2 \ge \sigma_0^2$, and $\sigma^2 \le \sigma_0^2$:
\bdm
T=\frac{\hatsig^2}{\sigma_0^2} \ (n-1-J) \sim \chi^2(n-1-J)
\edm
\emph{Notice:} The one-parameter $\chi^2(m)=\sum_{i=1}^mZ_i^2$ is the
sum of squares of i.i.d. Gaussians; its density is not symmetric. 

\ei
}

%###################################################
\frame{\frametitle{Examples for test statistics III}
%###################################################

{\small
\bi

\item Tests of \red{simultaneous point null hypotheses}, e.g., $H_0$: 
$\beta_1=0$ AND $\beta_2=2$ using the
\red{Fisher-F test}:
\bdm
T=\frac{ (S_0-S)/(M-M_0)}{S/(n-M)} \sim F(M-M_0,n-M)
\edm
\bi
\item $S$: SSE of the estimated full model with $M=J+1$ parameters
\item $S_0$: SSE of the estimated restrained model under $H_0$ with
  $M_0$ free parameters
\ei

\item The \bfdef{Fisher-F} distribution essentially is the ratio of two
  independent $\chi^2$ distributed random variables,
\bdm 
F(n,d)=\frac{\chi_n^2/n}{\chi_d^2/d},
\edm
with $n$ numerator and $d$ denominator degrees of freedom

\itemAsk  Argue that always $S_0\ge S$ 
\itemAsk  Justify that there is only a left-sided rejection region:
  $H_0$ is rejected if $t\sub{data}>f^{(n,d)}_{1-\alpha}$

\ei
}
}

%###################################################
\frame{\frametitle{Equivalence of the F and T-tests for one parameter}
%###################################################
\vspace{1em}

{\small
With $M-M_0=1$, the F-test is equivalent to a parameter test for the
  parameter $j$ in question:
\bi
\item Parameter test:
$
T=\frac{\hatbeta_j-\beta_{j0}}{\sqrt{\hat{V}(\hatbeta_j)}}\sim T(n-1-J)
$

\item F-test:
$
T=(n-J-1)\frac{S_0-S}{S} \sim F(1,n-1-J)
$
\item There is a general relation (\bfAsk{prove!}) between the
  student-t and the F(1,d) distribution: 
\bdm
F\sim F(1,d) \ \text{and} \ T \sim T(d) \Rightarrow F=T^2
\edm
so we have
$\left( (\hatbeta_j-\beta_{j0})/\sqrt{\hat{V}(\hatbeta_j)}\right)^2 \sim
F(1, n-1-J)$
\item
{\scriptsize
In fact, using $\hat{\m{V}}=S/(n-1-J)(\m{X}\tr\m{X})^{-1}$ and 
expanding $S_0-S=(\hatvecbeta_r-\hatvecbeta)\tr
(\m{X}\tr\m{X}) (\hatvecbeta_r-\hatvecbeta)$ to second order, one can
show that, in fact
\bdm
(n-J-1)\frac{S_0-S}{S}=\frac{(\hatbeta_j-\beta_{j0})^2}{\hat{V}(\hatbeta_j)}
\edm
}
\ei
}

}

\subsection{Type I and II Errors}

%###################################################
\frame{\frametitle{3.3 Type I and II Errors}
%###################################################
\vspace{-1em}

\fig{0.6\textwidth}{figsRegr/fehlerErsterZweiterArt_eng.png}
{\small
\bi
\item We can control the Type I ($\alpha$) error probability
  $P(\text{$H_0$ rejected}|H_0) \le \alpha$ in \bfblue{significance tests}.
\pause \item Since the Type II ($\beta$) error probability 
  $P(\text{$H_0$ not rejected}|\overline{H_0})$ is unknown, the more serious
error type should be formulated as the $\alpha$ error.
\pause \item Fundamental problem: I want $P(H_0|\text{rejected})$ and
$P(H_0|\overline{\text{rejected}})$, not vice versa $\Rightarrow$
\bfblue{Bayesian statistics}
\ei
}
}


%###################################################
%\frame{\frametitle{Annahme und Ablehnungsbereiche}
\frame{\frametitle{Rejection\\regions}
%###################################################
\vspace{-2em}

\hspace{0.18\textwidth}
\includegraphics[width=0.8\textwidth]
 {figsRegr/annahmeAblehn_einZweiseit_eng.png}
\vspace{0.5em}

\bi
\item Shown is the density of the test function \emph{if $H^*_0$}
\item If $H_0$ is a point set, $H^*_0=H_0$. If it is an interval,
  replace the unequality operators $<, \le, >, \ge$ by ``='' to obtain $H^*_0$
\item $H_0$ is rejected if the realisation $t\sub{data}$ is in the red
  \emph{extreme region} or \bfblue{rejection region}
\ei
}

%###################################################
\frame{\frametitle{3.4 Significance tests: the $p$-value}
%###################################################
\bi
\item Obviously, it is not very efficient to test $H_0$ for a fixed
significance level $\alpha$ (one does not know \emph{how significant}
the result really is)
\pause \item Instead, one would like to know the \emph{minimum}
$\alpha$ for rejection, or the \bfblue{$p$-value}. 
\pause \item The most general definition is:
\maindm{p=\text{Prob}(T\in E\sub{data}|H_0^*))} 
where the \emph{extreme region} $E\sub{data}$ contains all
realisations of $T$ that are
further away from $H^*_0$ than $t\sub{data}$.
\vspace{1em}

\bi
\item $p\ge\unit[5]{\%}$: not significant (no star at the parameter
  $\beta$)
\item $p<\unit[5]{\%}$: significant (one star, $\beta^*$)
\item $p<\unit[1]{\%}$: very significant (two star, $\beta^{**}$)
\item $p<0.001$: highly significant (three stars, $\beta^{***}$)
\ei
\ei
}

%###################################################
\frame{\frametitle{Calculating $p$ for some basic tests}
%###################################################

\bi
\item Interval test $H_0: \beta \le \beta_0$ or $\beta < \beta_0$
\bdm
p=P(T>t\sub{data}|\beta=\beta_0)
=1-F_T(t\sub{data})
\edm



\pause \item Interval test $H_0: \beta \ge \beta_0$ or $\beta > \beta_0$
\bdm
p=P(T<t\sub{data}|\beta=\beta_0)=F_T(t\sub{data})
\edm

\pause \item Point test $H_0: \beta=\beta_0$ (symmetry of $f_T$ assumed)
\bdma
p &=& P\big( (T>|t\sub{data}|) \cup (T<-|t\sub{data}|)\big)\\
 &=& 1-F_T(|t\sub{data}|)+F_T(-|t\sub{data}|)\\
 &=& 1-F_T(|t\sub{data}|)+1-F_T(|t\sub{data}|)\\
 &=& 2(1-F_T(|t\sub{data}|))
\edma
\ei
}


%###################################################
\frame{\frametitle{Type I and II errors for ``$<$'' or
	``$\le$''-tests as a function\\of the true value relative to
    $H_0$, known variance} 
%###################################################
\vspace{0em}
\fig{0.90\textwidth}{figsRegr/fehler12art_leGauss_eng.png}
\vspace{-1.5em}
{\small
\bi
\item The maximum type-I error probability of $\alpha$ 
occurs if $\beta=\beta_0$, i.e., at the boundary of $H_0$.
\pause \item The maximum type-II error probability of $1-\alpha$
occurs if $\beta$ is just outside of $H_0$.
\ei
}
}

%###################################################
\frame{\frametitle{The same for unknown variance, 
df=2 degrees of freedom}
%###################################################
\vspace{0.0em}
\fig{0.90\textwidth}{figsRegr/fehler12art_leStudent_2FG_eng.png}
\bi
\item The \bfblue{power function} is defined as the rejection
  probability of $H_0$ as a function of the true value relative to $H_0$
\ei
}

%###################################################
\frame{\frametitle{Type I and II errors for ``$>$'' or
	``$\ge$''-tests, known variance}
%###################################################
\vspace{0em}
\fig{0.9\textwidth}{figsRegr/fehler12art_geGauss_eng.png}
\vspace{-1em}
\bi
\item Again, the maximum type I and II error probabilities of
  $\alpha$ and $1-\alpha$, respectively, are obtained if
  the true parameter(s) are at the boundary / very near outside of $H_0$.
\pause \item The maximum type-I error probability is also known as
significance level.
\ei
}

%###################################################
\frame{\frametitle{The same for unknown variance, 
df=2 degrees of freedom}
%###################################################
\vspace{-4em}
\fig{0.9\textwidth}{figsRegr/fehler12art_geStudent_2FG_eng.png}
}


%###################################################
\frame{\frametitle{Type I and II errors for two-sided (point-)tests \\
	(unkown variance, df=2)}
%###################################################
\vspace{0em}
\fig{0.9\textwidth}{figsRegr/fehler12art_eqStudent_2FG_eng.png}
\vspace{-1em}
\bi
\item Since $H_0$ is a point set here, the type-I error probability is always
  given by $\alpha$ (``significance level'')
\ei
}


\subsubsection{3.4.1 Example}

%###################################################
\frame{\frametitle{3.4.1 Example: modeling the demand for hotel rooms}
%###################################################

\placebox{0.26}{0.67}
{\figSimple{0.46\textwidth}{figsRegr/hotel_scatterplot_x1x2_eng.png}}


\placebox{0.26}{0.25}
{\figSimple{0.46\textwidth}{figsRegr/hotel_scatterplot_x1y_eng.png}}

\placebox{0.74}{0.25}
{\figSimple{0.46\textwidth}{figsRegr/hotel_scatterplot_x2y_eng.png}}

\placebox{0.74}{0.67}
{\parbox{0.53\textwidth}{\scriptsize
\bi
\item Exogenous variables $x_1$: quality [\# stars]; $x_2$: price
  [\euro{}/night].
\item Endogenous variable: booking rate [\%]
\item The exogenous variables are non-perfectly correlated: \OK
\item The demand is positively correlated with both the quality and
  the price (!)
\ei
}}

}


%###################################################
\frame{\frametitle{Visualization of the fit quality}
%###################################################


\hspace{-0.02\textwidth}
\includegraphics[width=0.50\textwidth]{figsRegr/hotel_scatter3d_1_eng.png}
\hspace{-0.02\textwidth}
\includegraphics[width=0.50\textwidth]{figsRegr/hotel_scatter3d_2_eng.png}

\bi
\item Surface: model
$\hat{y}(\vec{x})=\hat{\beta}_0+\hat{\beta}_1x_1+\hat{\beta}_2x_2$\\[0ex]

\pause \item Black bullets: data (right graphics: rotated clockwise by
90 degrees)

\pause \item OLS estimate: $\hatbeta_0=25.5$, $\hatbeta_1=38.2$, $\hatbeta_2=-0.953$.

\pause \item Blue or pink bars: residuals
$\epsilon_i$ ($\le 0$ if below the model plane)
\ei
}


%###################################################
\frame{\frametitle{Effect of the correlations between the exogenous
    variables} 
%###################################################

\fig{0.9\textwidth}{figsRegr/hotel_scatterplot_x2ycondx1_eng.png}
}

%###################################################
\frame{\frametitle{Effect of mis-fit parameters I}
%###################################################
\fig{0.9\textwidth}{figsRegr/hotel_scatterplot_x2ycondx1_notCal1_eng.png}
}
%###################################################
\frame{\frametitle{Effect of mis-fit parameters II}
%###################################################
\fig{0.9\textwidth}{figsRegr/hotel_scatterplot_x2ycondx1_notCal2_eng.png}
}
%###################################################
\frame{\frametitle{Effect of mis-fit parameters III}
%###################################################
\fig{0.9\textwidth}{figsRegr/hotel_scatterplot_x2ycondx1_notCal3_eng.png}
}
%###################################################
\frame{\frametitle{Effect of mis-fit parameters IV}
%###################################################
\fig{0.9\textwidth}{figsRegr/hotel_scatterplot_x2ycondx1_notCal4_eng.png}
}





%###################################################
\frame{\frametitle{Confidence interval for the appraisal for ``stars''
    $\beta_1$\\ \hspace*{0.65\textwidth}(full model)}
 %###################################################
\vspace{-2em}
\begin{tabular}{cc}
\parbox{0.5\textwidth}{
  \fig{0.48\textwidth}{figsRegr/hotel_f_hatbeta1_eng.png}
  \vspace{-2em}
  \fig{0.48\textwidth}{figsRegr/hotel_F_hatbeta1_eng.png}
}
&
\hspace{1em}
\parbox{0.48\textwidth}{Confidence interval (CI):
\vspace{1em}

{\small 
$\beta_1\in [\hat{\beta}_1-\Delta \hat{\beta}_1^{(\alpha)},
 \hat{\beta}_1+\Delta \hat{\beta}_1^{(\alpha)}]$

$\Delta \hat{\beta}_1^{(\alpha)}=t_{1-\alpha/2}^{(n-3)}
\sqrt{\hat{V}(\hat{\beta}_1)}$

$\hat{V}(\hat{\beta}_1)
  =\hatsigeps^2\left[\left(\m{X}\tr\m{X}\right)^{-1}\right]_{11}$

$\hatsigeps^2=\frac{1}{n-3}\sum\limits_{i=1}^n
 \left(\hat{y}_i-y_i\right)^2$
}
}
\end{tabular}
}


%###################################################
\frame{\frametitle{Confidence interval for the price sensitivity  $\beta_2$
 \\ \hspace*{0.65\textwidth}(full model)}
%###################################################
\vspace{-2em}
\begin{tabular}{cc}
\parbox{0.5\textwidth}{
  \fig{0.48\textwidth}{figsRegr/hotel_f_hatbeta2_eng.png}
  \vspace{-2em}
  \fig{0.48\textwidth}{figsRegr/hotel_F_hatbeta2_eng.png}
}
\end{tabular}


}


%###################################################
\frame{\frametitle{Linked tests and 2d regions of confidence for
    $\beta_1$ and $\beta_2$}
%###################################################
\fig{0.76\textwidth}{figsRegr/hotel_f2_hatbeta1_hatbeta2_eng.png}
\vspace{-1em}

{\scriptsize
\bi
\item The estimation errors are significantly correlated.

\pause \item Simple null hypotheses ($t$-test):
$H_{01}: \beta_1=\beta_{10}=27$,
$H_{02}: \beta_2=\beta_{20}=-1.4$

\pause \item Correlated null hypotheses ($t$-test), e.g.,
$\gamma_3=\beta_1+30\beta_2<0$

\pause \item Compound null hypotheses ($F$-test), e.g.,
% $\bullet=H_{01}: \beta_{10}=30 \cap \beta_{20}=-1$.
$\triangle=H_{03}: \beta_{10}=30 \cap \beta_{20}=-0.6$.
\ei
}
}

\subsection{3.5 Model Selection Strategies}

%###################################################
\frame{\frametitle{3.5 Model Selection Strategies I: Problem Statement}
%###################################################

\placebox{0.50}{0.45}
{\figSimple{0.60\textwidth}{figsRegr/elephantRaisedTrunkPale.jpg}}

\placebox{0.50}{0.45}
{\parbox{0.80\textwidth}{
\bi
\item With every additional parameter, the fit quality in terms of a
  reduced SSE or $\hat{\sigma}^2=\text{SSE}/(n-J-1)$ becomes better
\item However, the risk of overfitting increases. In the words of John
  Neumann: \textit{With four parameters I can fit an elephant, and
    with five I can make him wiggle [its] trunk.}
\item Overfitted models do not validate and can make neither
  statements nor predictions.
\item \red{$\Rightarrow$ we need selection criteria taking care of
  overfitting!}
\ei
}}

}

%###################################################
\frame{\frametitle{Model selection: Some standard criteria}
%###################################################
\bi
\item \bfdef{(1) Adjusted $R^2$:}
\bdm
\bar R^2 = 1 - \, \frac{n-1}{n-J-1}\,\left(1-R^2\right), \quad
R^2=1-\frac{S}{S_0}, 
\edm
\vspace{-1em}
\bdm
S =\text{SSE(full model)}, \quad S_0=\text{SSE(constant-only model)}.
\edm
\vspace{0.1em}

\item \bfdef{(2) Akaike information criterion AIC:}
\begin{equation*}
  \text{AIC} = \ln \hat\sigma\sub{descr}^2 + J\, \frac{2}{n}, 
\end{equation*}

\item \bfdef{(3) Bayes' Information criterion BIC:}
\begin{equation*}
   \text{BIC} = \ln \hat\sigma\sub{descr}^2 + J\, \frac{\ln n}{n} \,.
\end{equation*}
\ei
Notice that the descriptive $\hat{\sigma}\sub{descr}^2=S/n$ instead of
  the unbiased 
$\hat{\sigma}^2=S/(n-1-J)$ is taken in these criteria.
}

%###################################################
\frame{\frametitle{Model selection: Strategy {\`a} la ``Occam's Razor''}
\begin{itemize}
\item Identify $P$ possibly relevant exogenous factors (the constant
  is always included) and calculate
  $\bar{R}^2$, AIC, or BIC for all $2^P$ combinations of these
      factors.
\item The best model is that maximizing $\bar{R}^2$ or minimizing AIC
  or BIC. 
\item Since AIC and also $\bar{R}^2$ penalize complex models (with
  many parameters) too little, the BIC is usually the best bet.
\item Besides the mentioned \emph{brute-force} approach testing all $2^P$
  combinations, there are two standard strategies:
\bi
\item \bfdef{Top-down approach}: Start with all the $P$
  factors. In each round, eliminate a single factor such that the
  reduced model has the highest increase in $\bar{R}^2$ / decrease in
  AIC or BIC. Stop if there is no further improvement.
\item \bfdef{Bottom-up approach}: Start with the constant-only model
  $y=\beta_0$ and successively add factors until there is no further
  improvement.
\ei
\item Standard statistics packages contain all of these strategies. 
\end{itemize}
}

\end{document}

