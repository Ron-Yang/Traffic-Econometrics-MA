%#############################################
% New pdflatex format,
% only png, jpg (or svg?) images but positioning now ok
% template in ~/tex/inputs/template_folien.tex
% [TODO now specialized to Vkoem_Ma; make general]
%#############################################


%\documentclass[mathserif]{beamer}
\documentclass[mathserif,handout]{beamer}
%\usepackage{beamerthemeshadow}
\input{$HOME/tex/inputs/defsSkript}%$
\input{$HOME/tex/inputs/styleBeamerVkoekMa}%$
\usepackage{graphicx}


%##############################################################

\begin{document}

\section{3. Classical Inferential Statistics}

%###################################################
\frame{  %title other layout
%###################################################
\placebox{0.50}{0.50}{
  \figSimple{1.70\textwidth}{figsRegr/regression3dFig.png}}

\makePale{0.85}{0.50}{0.55}{1.20}{1.50}

\placebox{0.50}{0.50}{\parbox{0.6\textwidth}{
\bi
\item[3.1] Expectation and Covariance Matrix of the OLS Estimator
\item[3.2] Confidence Intervals
\item[3.3] Significance Tests
\bi
\item[3.3.1] General procedure
\item[3.3.2] Type I and II errors
\item[3.3.3] Specific test statistics
\item[3.3.4] The $p$-value
\ei
\item[3.4] Dependence on the True Parameter Value: Power Function
\item[3.5] Model Selection Strategies
\ei
}}

\placebox{0.50}{0.90}{\myheading{
 Lecture 03: Classical Inferential Statistics}}
}


\subsection{3.1. OLS Expectation and Covariance}

%###################################################
\frame{\frametitle{3.1. OLS Expectation and Covariance}
%###################################################

\bi
\item Only stochasticity: residual errors
  $\veceps$ according to  $\vec{y}=\m{X}\vec{\beta}+\veceps$
\pause \item The OLS estimator is linear in $\vec{y}$:
\bdma
\hatvecbeta 
  &=&
    \left(\m{X}\tr\m{X}\right)^{-1}\m{X}\tr\vec{y} \\ 
 \visible<3->{ &=&
  \left(\m{X}\tr\m{X}\right)^{-1}\m{X}\tr(\m{X}\vec{\beta}+\veceps)\\}
 \visible<4->{ &=&
  \vec{\beta}+\left(\m{X}\tr\m{X}\right)^{-1} \m{X}\tr\veceps}
\edma
\ei

\visible<5->{\mysubheading{Expectation value}

\bdm
E(\hatvecbeta)=E(\vecbeta)
 +\left(\m{X}\tr\m{X}\right)^{-1} \m{X}\tr E(\veceps)=\vecbeta
\edm
}


\visible<6->{\maintextbox{0.8\textwidth}{
The OLS estimator of parameter-linear models is \bfdef{unbiased} under
the mild condition $E(\veceps)=\vec{0}$ for all the data points}}

}

 %###################################################
\frame{\frametitle{OLS estimation error II: variances and covariances}
%###################################################

\bi
\item Gau\3-Markow conditions $\to \epsilon \sim
  \text{i.i.d} N(0,\sigma^2) \to \hatvecbeta $ is normal distributed
\pause \item In this case, the complete errors are specified by the
  expectation value and the \bfdef{variance-covariance matrix}
$\m{V}_{\hatvecbeta}$

\ei

\pause
\bdma
\m{V}_{\hatvecbeta} &\stackrel{\text{def}}{=}&
 E\left( (\hatvecbeta-\vecbeta) (\hatvecbeta-\vecbeta)\tr\right)\\
  \visible<4->{\remark{insert
      $\hatvecbeta=\vec{\beta}
+\left(\m{X}\tr\m{X}\right)^{-1}\m{X}\tr\veceps$} 
     &=& 
    E\left( (\m{X}\tr\m{X})^{-1}\m{X}\tr\veceps
     \left( (\m{X}\tr\m{X})^{-1}\m{X}\tr\veceps\right)\tr\right) \\}
  \visible<5->{\remark{transpose rule} &=& 
     E\left( (\m{X}\tr\m{X})^{-1}\m{X}\tr \veceps\veceps\tr
     \m{X}(\m{X}\tr\m{X})^{-1}\right)\\}
  \visible<6->{\remark{$E(.)$ acts only on $\veceps$} &=& 
     (\m{X}\tr\m{X})^{-1}\m{X}\tr E(\veceps\veceps\tr)
     \m{X}(\m{X}\tr\m{X})^{-1}\\}
  \visible<7->{\remark{Gau\3-Markow} &=& 
     (\m{X}\tr\m{X})^{-1}\m{X}\tr\sigeps^2\m{X}(\m{X}\tr\m{X})^{-1}\\}
  \visible<8->{\remark{def inverse matrix} &=& 
     \sigeps^2(\m{X}\tr\m{X})^{-1} }
\edma
\visible<9->{The variance-covariance matrix depends only on the values of the
exogenous factors!}


}

%###################################################
\frame{\frametitle{Results summary}
%###################################################

{\small
\bi
\item Ordinary least squares (OLS) estimator:
\maindm{\hatvecbeta =\left(\m{X}\tr\m{X}\right)^{-1}\m{X}\tr\vec{y}}

\pause \item \bfdef{Variance-Covariance matrix} of the estimation errors
(provided the errors are i.i.d.) in terms of the \bfdef{Hesse matrix} $\m{H}$ of the 
objective function:

\maindm{
\begin{array}{rcl}
\m{V}_{\hatvecbeta}
 &=&  E\left( (\hatvecbeta-\vecbeta)(\hatvecbeta-\vecbeta)\tr\right)
 =\sigma^2 \left(\m{X}\tr\m{X}\right)^{-1}= 2\sigma^2 \m{H}^{-1},\\
H_{jk} &=& \left. \ablpartmix{S}{\beta_j}{\beta_k}\right|_{\vecbeta=\hatvecbeta}
=2\m{X}\tr\m{X}
\end{array}
}

\pause \item Variances of estimation errors: $V(\hatbeta_j)=V_{jj}$

\pause \item Correlation of estimation errors: $\text{Corr}(\hatbeta_j, \hatbeta_k)=\frac{V_{jk}}{\sqrt{V_{jj}V_{kk}}}$

\pause \item Distribution of the normalized estimation errors:
$
\frac{\hatbeta_j-\beta_j}{\sqrt{V_{jj}}} \sim N(0,1)
$
\ei
}

}
 
%###################################################
\frame{\frametitle{Estimation of the residual variance}
%###################################################
The above cannot be applied directly since the residual variance $\sigma^2$ is
unknown and must be estimated by the minimum $S(\hatbeta)$ of the sum
of squared 
errors: 
\maindm{
\hatsig^2 = \frac{1}{n-J-1}\sum_i \left(y_i-\hat{y}(\vec{x}_i)\right)^2
 = \frac{S(\hatbeta)}{n-J-1}
}
\bdm
\hatsig^2 =\frac{\vec{y}\tr
  \left(\m{1}-\m{X}(\m{X}\tr\m{X})^{-1}\m{X}\tr\right)\vec{y}} {n-J-1}
\edm

This results in following \emph{estimated} error statistics:

\bi
\pause \item Estimated variance-covariance matrix:

\maindm{\hat{\m{V}}_{\hatvecbeta}= 2\hatsig^2 \m{H}^{-1}
=\hatsig^2 \left(\m{X}\tr\m{X}\right)^{-1}
}

\pause \item The normalized 
approximate estimation errors are student-t distributed:
\bdm
\frac{\hatbeta_j-\beta_j}{\sqrt{\hat{V}_{jj}}} \sim T(n-1-J)
\edm
\ei
}
 
%###################################################
\frame{\frametitle{Multivariate distribution function of $\hatvecbeta$}
%###################################################

{\small
The distribution of the errors $\Delta
\hatvecbeta=\hatvecbeta-\vecbeta$ obeys a multivariate normal
distribution: 
\bdm
f_{\hatvecbeta}(\Delta \hatvecbeta)
 \propto \exp\left[-\frac{1}{2} 
 \Delta \hatvecbeta\tr \m{V}^{-1}\, \Delta \hatvecbeta\right]
   = \exp\left[-\frac{\Delta \hatvecbeta\tr \m{X}\tr\m{X}\Delta \hatvecbeta}
  {2\sigeps^2}\right].
\edm

\mysubsubheading{Relation to the maximum-likelihood-method ($\to$
  Lecture 06:)}

Expand the SSE $S(\vecbeta)$ around $\hatvecbeta$ to second order:
\bdm
S(\vecbeta)-S(\hatvecbeta)
\approx \frac{1}{2}\Delta \hatvecbeta\tr \m{H} \Delta \hatvecbeta
= \Delta \hatvecbeta\tr \m{X}\tr\m{X} \Delta \hatvecbeta
\edm
\vspace{1em}

\hspace{5em} $\Rightarrow \quad$ 
\vspace{-3em}

\maindm{f_{\hatvecbeta}(\Delta \hatvecbeta)
 \propto \exp\left[-\frac{S(\vecbeta)-S(\hatvecbeta)}{2\sigeps^2}\right] 
}

and with estimated residual variance $\hatsigeps^2=S(\hatvecbeta)/(n-J-1)$ 
\bdm
\hat{f}_{\hatvecbeta}(\vecbeta) \propto \exp \left[
-\frac{(n-J-1)}{2}\left(\frac{S(\vecbeta)}{S(\hatvecbeta)}-1\right)
\right]
\edm
}

}

%###################################################
\frame{\frametitle{Example of correlated errors:
modeling the demand for hotel rooms}
%###################################################

\placebox{0.26}{0.64}
{\figSimple{0.46\textwidth}{figsRegr/hotel_scatterplot_x1x2_eng.png}}


\placebox{0.26}{0.22}
{\figSimple{0.46\textwidth}{figsRegr/hotel_scatterplot_x1y_eng.png}}

\placebox{0.74}{0.22}
{\figSimple{0.46\textwidth}{figsRegr/hotel_scatterplot_x2y_eng.png}}

\placebox{0.74}{0.64}
{\parbox{0.53\textwidth}{\scriptsize
\bi
\item The example of Lecture 02:
  $y=\beta_0+\beta_1x_1+\beta_2x_2+\epsilon$ with the factors: 
  $x_0=1$, $x_1$: quality [\# stars]; $x_2$: price 
  [\euro{}/night].
\item The exogenous variables/factors are non-perfectly correlated: \OK
\item Endogenous variable: booking rate [\%]
\item The demand is positively correlated with both the quality and
  the price (!)
\ei
}}

}

%###################################################
\frame{\frametitle{Residual errors for fitted parameters} 
%###################################################

\fig{0.9\textwidth}{figsRegr/hotel_scatterplot_x2ycondx1_eng.png}
}


%###################################################
\frame{\frametitle{Effect of mis-fit parameters I: small effect 
   if $\bfbeta_1$ and $\bfbeta_2$ have opposite misfits}
%###################################################
\fig{0.9\textwidth}{figsRegr/hotel_scatterplot_x2ycondx1_notCal2_eng.png}
}

%###################################################
\frame{\frametitle{Effect of mis-fit parameters II: small effect 
   if $\bfbeta_1$ and $\bfbeta_2$ have opposite misfits}
%###################################################
\fig{0.9\textwidth}{figsRegr/hotel_scatterplot_x2ycondx1_notCal4_eng.png}
}

%###################################################
\frame{\frametitle{Effect of mis-fit parameters III: large effect 
   if $\bfbeta_1$ and $\bfbeta_2$ have both positive misfits}
%###################################################
\fig{0.9\textwidth}{figsRegr/hotel_scatterplot_x2ycondx1_notCal1_eng.png}
}

%###################################################
\frame{\frametitle{Effect of mis-fit parameters IV: large effect 
   if $\bfbeta_1$ and $\bfbeta_2$ have both negative misfits}
%###################################################
\fig{0.9\textwidth}{figsRegr/hotel_scatterplot_x2ycondx1_notCal3_eng.png}
}


%###################################################
\frame{\frametitle{All this results in a negative correlation\\
    between the estimation errors for $\bfbeta_1$ and $\bfbeta_2$}
%###################################################

\vspace{1em}

\fig{0.90\textwidth}{figsRegr/hotel_f2_hatbeta1_hatbeta2_simple_eng.png}

}

%###################################################
\frame{\frametitle{Special case 1: No exogenous variables}
%###################################################
\bi
\item Model: $y=\beta_0+\epsilon$
\pause \item System matrix: $\m{X}=(1,1, ..., 1)\tr$
\pause \item OLS estimator: 
\bdma 
\left(\m{X}\tr\m{X}\right)^{-1}=\frac{1}{n}, \quad 
\m{X}\tr \vec{y}=\sum_iy_i=n\bar{y}, &&\\
\hatbeta_0=\hat{\mu}=\left(\m{X}\tr\m{X}\right)^{-1}\m{X}\tr \vec{y}=\bar{y}
\edma
\pause \item Variance:
  $V_{00}=V(\hat{\mu})=\sigma^2\left(\m{X}\tr\m{X}\right)^{-1}
=\frac{\sigma^2}{n}$, \quad
$\hat{V}_{00}=\frac{\hatsig^2}{n}$
\vspace{1em}

\pause \item Distribution of the estimator
 \red{(if $\epsilon \sim i.i.d N(\mu,\sigma^2)$)}
\bdm
\frac{\hatbeta_0-\beta_0}{\sqrt{V_{00}}}
=\frac{\bar{x}-\mu}{\sigma}\ \sqrt{n} \sim N(0,1), \quad
\frac{\bar{x}-\mu}{\hatsig}\ \sqrt{n} \sim T(n-1)
\edm
\ei
}

%###################################################
\frame{\frametitle{Special case 2: Simple linear regression}
%###################################################
\bi
\item Model (with $x_1=x$): $y=\beta_0+\beta_1x+\epsilon$

\pause \item System matrix: 
\bdm \m{X}=\myMatrixTwo{1 & x_1 \\ \vdots & \vdots \\ 1 & x_n},\quad
\m{X}\tr\m{X}=\myMatrixTwo{n & n\bar{x}\\n\bar{x} & \sum x_i^2}
\edm

\pause \item OLS estimator (with $s_x^2=1/n(\sum x_i^2-n\bar{x})$): 
\bdm
\left(\m{X}\tr\m{X}\right)^{-1}
 =\frac{1}{ns_x^2}
\myMatrixTwo{\frac{\sum x_i^2}{n} & -\bar{x} \\ -\bar{x} & 1}, \quad
\m{X}\tr \vec{y}=\myVector{n\bar{y}\\ \sum x_iy_i} 
\edm
\bdma
\hatbeta_1 &=& \left(-\frac{\bar{x}}{ns_x^2}, \frac{1}{ns_x^2}\right)
\myVector{n\bar{y}\\ \sum x_iy_i}
=\frac{\sum_ix_iy_i-n\bar{x}\bar{y}}{\sum x_i^2-n\bar{x}}
=\frac{s_{xy}}{s_x^2}, \\
\hatbeta_0 &=& \bar{y}-\hatbeta_1 \bar{x}
\edma
\ei
}

%###################################################
\frame{\frametitle{Simple linear regression (ctnd)}
%###################################################

\bi
\item Variance-covariance matrix (assuming w/o loss of generality
$\bar{x}=0$): 
\bdm
\m{V}(\hatvecbeta)=\sigma^2 \left(\m{X}\tr\m{X}\right)^{-1}
=\sigma^2 \myMatrixTwo{\frac{1}{n} & 0 \\ 0 & \frac{1}{ns_x^2}}
\edm

\pause \item Variance of the estimator $\hat{y}(x)$ ($x$ is deterministic):
\bdm
V(\hat{y}(x))=V(\hatbeta_0+\hatbeta_1x)
 = V_{00}+x^2  V_{11} +2xV_{01}
 = \frac{\sigma^2}{n}\left(1+\frac{x^2}{s_x^2}\right)
\edm

\pause \item Distribution of the estimator for $y(x)$:
\bdm
\hat{y}(x)\sim N\big(y(x), V(\hat{y}(x))\big)
\edm
If $\sigma^2$ has to be estimated by $\hatsig^2$, the normalized
estimators for $\beta_0$, $\beta_1$ and $y(x)$ are $\sim T(n-2)$.

\ei
}



%###################################################
\frame{\frametitle{Probability density for $\hat{y}(x)$ for simple 
linear regression}
%###################################################

\vspace{-0.5em}
\fig{0.8\textwidth}{figsRegr/regression3dFig.png}
\vspace{-2em}

\bi
\item If the Gau\3-Markov assumptions apply, the model
  estimation errors $\hat{y}(x)-y(x)$ are
  Gaussian distributed
\item The expectation and variance depends on $x$; the standard error
  is hyperbola-shaped.
\ei

}

\subsection{3.2 Confidence Intervals}

%###################################################
\frame{\frametitle{3.2. Confidence Intervals:\\
 where the
Student-t distribution comes from}
%###################################################

\visible<1>{\placebox{0.489}{0.468}{
  \figSimple{1.02\textwidth}{figsRegr/KI_veranschaulichung_eng1.png}}}

\visible<2>{\placebox{0.48}{0.45}{
  \figSimple{1.0\textwidth}{figsRegr/KI_veranschaulichung_eng2.png}}}

\visible<3>{\placebox{0.502}{0.453}{
  \figSimple{1.05\textwidth}{figsRegr/KI_veranschaulichung_eng3.png}}}

\visible<4>{\placebox{0.502}{0.453}{
  \figSimple{1.05\textwidth}{figsRegr/KI_veranschaulichung_eng4.png}}}


}

%###################################################
\frame{\frametitle{Densities of standard normal 
\textit{vs.} Student-t distribution}
%###################################################
\vspace{-0.5em}
\fig{0.95\textwidth}{figsRegr/f_gaussStudent_eng.png}
}


%###################################################
\frame{\frametitle{Distributions of standard normal 
\textit{vs.} Student-t-distribution}
%###################################################
\vspace{-0.5em}

\fig{0.95\textwidth}{figsRegr/F_gaussStudent_eng.png}
}

%###################################################
\frame{\frametitle{Calculation of the confidence intervals (CI)}
%###################################################

\placebox{0.50}{0.81}{\parbox{1.0\textwidth}{
\maindm{\text{CI}_{\beta_j}^{(\alpha)}: \beta_j \in \left[
 \, \hatbeta_j-\Delta \hatbeta_j,  \hatbeta_j+\Delta \hatbeta_j \,
\right], \quad
\Delta \hatbeta_j =t_{1-\alpha/2}^{(n-J-1)}\hatsig_{\hatbeta_j}.
}}}

\placebox{0.50}{0.70}{\parbox{1.0\textwidth}{
 {\small
 \bi
  \pause \item $t_{1-\alpha/2}$: Quantile (inverse of) the distribution
    function\\[-1ex]
  \pause \item CI ``uncertainty principle'': Higher sensitivity implies
    higher $\alpha$ error.
  \ei
}
}}

\visible<4>{\placebox{0.50}{0.31}
{\figSimple{0.70\textwidth}{figsRegr/f_student_KI_eng.png}}}

\visible<5>{\placebox{0.50}{0.31}
{\figSimple{0.70\textwidth}{figsRegr/F_student_KI_eng.png}}}



}


%###################################################
\frame{\frametitle{Hotel example: CI for the
    appraisal for ``stars'' 
    $\beta_1$\\ \hspace*{0.65\textwidth}(full model)}
 %###################################################
\vspace{-2em}
\begin{tabular}{cc}
\parbox{0.5\textwidth}{
  \visible<1->{\fig{0.48\textwidth}{figsRegr/hotel_f_hatbeta1_eng.png}}
  \vspace{-2em}
  \visible<2->{\fig{0.48\textwidth}{figsRegr/hotel_F_hatbeta1_eng.png}}
}
&
\hspace{1em}
\parbox{0.48\textwidth}{

Model: $y(\vec{x})=\sum_j\beta_jx_j+\epsilon$
\vspace{1em}

Factors:\\[-0.5ex]
{\small $x_0=1$, $x_1$: \#stars, $x_2$: price}
\vspace{1em}

Confidence interval (CI):
\vspace{0.5em}

{\small 
$\beta_1\in [\hat{\beta}_1-\Delta \hat{\beta}_1^{(\alpha)},
 \hat{\beta}_1+\Delta \hat{\beta}_1^{(\alpha)}]$

\pause $\Delta \hat{\beta}_1^{(\alpha)}=t_{1-\alpha/2}^{(n-3)}
\sqrt{\hat{V}(\hat{\beta}_1)}$

\pause $\hat{V}(\hat{\beta}_1)
  =\hatsigeps^2\left[\left(\m{X}\tr\m{X}\right)^{-1}\right]_{11}$

\pause $\hatsigeps^2=\frac{1}{n-3}\sum\limits_{i=1}^n
 \left(\hat{y}_i-y_i\right)^2$
}
}
\end{tabular}
}



\subsection{3.3 Significance tests}
\subsubsection{3.3.1 General four-step procedure}

%###################################################
\frame{\frametitle{3.3 Significance Tests:\\1. General four-step procedure}
%###################################################
\benum
\item Formulate a \bfblue{null hypothesis} $H_0$ such that their rejection
  gives insight, e.g. $\beta_j=\beta_{j0}$ (point hypothesis) or
  $\beta_j \le \beta_0$ (interval hypothesis): Notice: \emph{\red{One cannot confirm $H_0$}}
\pause \item Select a \bfblue{test function} or \bfblue{statistics} $T$
\bi
\item whose distribution is known provided $H^*_0$, i.e., the
  parameters lie at the margin of $H_0$ (of course, $H^*_0=H_0$ for a point null hypothesis)
\item which has distinct \bfblue{rejection regions} $R(\alpha)$ which
  are reached rarely 
  (with a probability $\le \alpha$) if $H_0$ but more often if
  $H_1=\overline{H_0}$ 
\ei

\pause \item Evaluate a realisation $t\sub{data}$ of $T$ from the data

\pause \item Check if $t\sub{data} \in R(\alpha)$. If yes, $H_0$ can be
  rejected at an error probability or \bfblue{significance level}
  $\alpha$. Otherwise, nothing can be said.
\pause \item[4a] Alternatively, calculate the \bfblue{$p$-value} as
the minimum $\alpha$ for which $H_0$ can be rejected.
\eenum
}

%###################################################
\frame{\frametitle{3.3.2 Type I and II errors}
%###################################################

\visible<1>{\placebox{0.5}{0.68}{
 \figSimple{0.6\textwidth}{figsRegr/fehlerErsterZweiterArt1_eng.png}}}

\visible<2>{\placebox{0.5}{0.68}{
 \figSimple{0.6\textwidth}{figsRegr/fehlerErsterZweiterArt2_eng.png}}}

\visible<3>{\placebox{0.5}{0.68}{
 \figSimple{0.6\textwidth}{figsRegr/fehlerErsterZweiterArt3_eng.png}}}

\visible<4->{\placebox{0.5}{0.68}{
 \figSimple{0.6\textwidth}{figsRegr/fehlerErsterZweiterArt4_eng.png}}}

\placebox{0.5}{0.25}{\parbox{\textwidth}{
{\small
\bi
\visible<3->{\item We can control the Type I ($\alpha$) error probability
  $P(\text{$H_0$ rejected}|H_0) \le \alpha$ in \bfblue{significance tests}}
\visible<4->{\item Since the Type II ($\beta$) error probability 
  $P(\text{$H_0$ not rejected}|\overline{H_0})$ is unknown, the more serious
error type should be formulated as the $\alpha$ error}
\visible<5->{\item Fundamental problem: I want $P(H_0|\text{rejected})$ and
$P(H_0|\overline{\text{rejected}})$, not vice versa $\Rightarrow$
\bfblue{Bayesian statistics}}
\ei
}}}

}


\subsubsection{3.3.3 Specific test statistics}
%###################################################
\frame{\frametitle{3.3.3 Specific test statistics I}
%###################################################

{\small
\bi
\item Testing  \red{parameters} such as $H_0$: $\beta_j=\beta_{j0}$ or
  $\beta_j\ge \beta_{j0}$ or  $\beta_j\le \beta_{j0}$:

  The test
  function is the estimated deviation from the boundary of $H_0$ in
  units of the estimated error standard deviation:
\bdm 
T =\frac{\hatbeta_j-\beta_{j0}}{\sqrt{\hat{V}_{jj}}} \sim T(n-1-J)
\edm
\pause \item Testing \red{functions of parameters} such as
  $H_0$: $\beta_1/\beta_2=2$, $\le 2$ or $\ge 2$: Transform into a linear
  combination. Then, the normalized estimated deviation is student-t
  distributed under $H_0^*$. Here, the linear combination is
  $b=\beta_1-2\beta_2=0$: 
\bdma
\hat{b} &=& \hatbeta_1-2\hatbeta_1, \\
\hat{V}(\hat{b}) &=& \hat{V}_{11}+4\hat{V}_{22}-4\hat{V}_{12}, \\
T &=& \frac{\hat{b}}{\sqrt{\hat{V}(\hat{b})}} \sim T(n-1-J)
\edma
\ei
}

}

%###################################################
\frame{\frametitle{Specific test statistics II}
%###################################################

\bi
\item Testing the \red{correlation coefficient} in an $xy$ scatter plot:
\bdm
\hat{\rho}=\frac{s_{xy}}{s_xs_y}, \quad H_0: \rho=0, \quad
T=\frac{\hat{\rho}}{\sqrt{1-\hat{\rho}^2}}\sqrt{n-2} \sim T(n-2)
\edm

{\footnotesize
\emph{Derivation:} $\rho \neq 0$ if, and only if, 
in a simple linear regression
$y=\beta_0+\beta_1x+\epsilon$, the slope parameter $\beta_1=0$, so
test for $\beta_1=0$: Under $H_0$, the test statistics
\bdm
T=\hatbeta_1/\sqrt{\hat{V}_{11}}=\frac{s_{xy}}{\hat{\sigma}s_x}\sqrt{n}
 \sim T(n-2)
\edm
Now insert $\hat{\sigma}$ which can, in the simple-regression case, be
explicitely calculated:
 $\hat{\sigma}^2=n (s_y^2 -s_{xy}^2/s_x^2)/(n-2)$
}

\vspace{1em}

\item Test for the \red{residual variance}, $H_0$: $\sigma^2=\sigma_0^2$, 
$\sigma^2 \ge \sigma_0^2$, and $\sigma^2 \le \sigma_0^2$:
\bdm
T=\frac{\hatsig^2}{\sigma_0^2} \ (n-1-J) \sim \chi^2(n-1-J)
\edm
\emph{Notice:} The one-parameter $\chi^2(m)=\sum_{i=1}^mZ_i^2$ is the
sum of squares of i.i.d. Gaussians; its density is not symmetric. 

\ei
}

%###################################################
\frame{\frametitle{Specific test statistics III}
%###################################################

{\small
\bi

\item Tests of \red{simultaneous point null hypotheses}, e.g., $H_0$: 
$\beta_1=0$ AND $\beta_2=2$ using the
\red{Fisher-F test}:
\bdm
T=\frac{ (S_0-S)/(M-M_0)}{S/(n-M)} \sim F(M-M_0,n-M)
\edm
\bi
\item $S$: SSE of the estimated full model with $M=J+1$ parameters
\item $S_0$: SSE of the estimated restrained model under $H_0$ with
  $M_0$ free parameters
\ei

\item The \bfdef{Fisher-F} distribution essentially is the ratio of two
  independent $\chi^2$ distributed random variables,
\bdm 
F(n,d)=\frac{\chi_n^2/n}{\chi_d^2/d},
\edm
with $n$ numerator and $d$ denominator degrees of freedom

\itemAsk  Argue that always $S_0\ge S$ 
\itemAsk  Justify that there is only a left-sided rejection region:
  $H_0$ is rejected if $t\sub{data}>f^{(n,d)}_{1-\alpha}$

\ei
}
}


%###################################################
\frame{\frametitle{Example: modeling the demand for hotel rooms}
%###################################################

{\small 
The already well-known example: 
\bdm
y=\beta_0x_0+\beta_1x_1+\beta_2x_2+\epsilon
\edm
where $x_0=1$, $x_1$: proxy for quality [\# stars]; $x_2$: price
  [\euro{}/night], 
\bdm
\hatbeta_0=25.5, \quad
\hatbeta_1=38.2, \quad
\hatbeta_2=-0.952
\edm
and
\bdm
\hat{\m{V}}=\myMatrixThree{
28.0 &	-6.40 &	-0.119\\
-6.40 & 26.0 & -0.941\\
-0.119 & -0.941 & 0.0397}
\edm

\bi
\itemAsk \colAsk{Formulate and test the null hypothesis at
  $\alpha=\unit[5]{\%}$ that the stars
  do not matter}
\itemAnswer \scriptsize{\colAnswer{$H_{01}: \beta_1=0$, point t-test
    with $T=\hatbeta_1/\sqrt{\hat{V}_{11}} \sim T(12-9)$, i.e. df=9
    degrees of freedom, $t\sub{data}=7.49$,
    $t^{(9)}_{0.975}=2.26<|t\sub{data}|$ $\Rightarrow$ $H_0$ rejected,
    stars matter}}
\itemAsk \colAsk{Do people favour more stars (at $\alpha=\unit[5]{\%}$)?}  
\itemAnswer \scriptsize{\colAnswer{$H_{02}: \beta_1<=0$, interval
     test with same $T$ and $t\sub{data}$ as above,
     $t^{(9)}_{0.95}=1.83<t\sub{data} \Rightarrow H_{02}
     \text{ rejected}$, more stars are better}}
\ei
}

}

%###################################################
\frame{\frametitle{Example: modeling the demand for hotel rooms (ctned)}
%###################################################

\bi
\itemAsk \colAsk{\small{Does each \euro{} more per night decrease the
    occupancy 
  by at most \unit[1]{\%}?}}

\itemAnswer \scriptsize{\colAnswer{$H_{03}: \beta_2<-1$ ($H_{03}$ is the
    complement event!),
    $t\sub{data}=(\hat{\beta}_2+1)/\sqrt{\hat{V}_{22}}=0.24
    \stackrel{!}{>} t^{(9)}_{0.95}=1.83$ $\Rightarrow$ $H_{03}$
    not rejected\\[0.5ex]
$\Rightarrow$ the hotel manager might risk losing more than one
    percent point of customers
}}


\itemAsk {\small \colAsk{Is it worth renovating my hotel thereby gaining one
  star so that\\[0.5ex] I can ask for \unit[30]{\euro{}} more per
  night without  losing guests?}}

\itemAnswer \scriptsize{\colAnswer{Again, define the complement event as 
$H_{04}: \beta_1\le -30\beta_2$ or $\gamma=\beta_1+30\beta_2\le 0$
\bdma
\hat{\gamma} &=& \hatbeta_1+30\hatbeta_2=9.63,\\
\hat{V}(\hat{\gamma}) &=&\hat{V}_{11}+900 \hat{V}_{22}+2*1*30\hat{V}_{12}=5.27
\edma
So, $t\sub{data}=\hat{\gamma}/\sqrt{\hat{V}(\hat{\gamma})}=4.20>
t^{(9)}_{0.95}=1.83$
$\Rightarrow \ H_{04}$ rejected at \unit[5]{\%}  
$\Rightarrow$ 
the risk of losing customers is less than \unit[5]{\%}}}

\itemAsk {\small \colAsk{Can it be simultaneously true that
    $\beta_1=30$ and $\beta_2=-1$?}}

\itemAnswer \scriptsize{\colAnswer{Full model: $\hatbeta_0=25.5,$,
$\hatbeta_1=38.2$, $\hatbeta_2=-0.952$, $S(\hatvecbeta)=498.2$; 
Reduced model with fixed $\beta_1=30$, $\beta_2=1$: $\hatbeta_0=49.0$, 
$S_0=S( (\hatbeta_0,30,1)')=1808$, $M-M_0=\unit[2]{dof}$,
$n-M=\unit[9]{dof}$, $T\sim F(2,9)$,
$t\sub{data}=9/2 \ (S_0-S)/S = 11.8>f^{(2.9)}_{0.95}$ $\Rightarrow$
$H_0$ rejected}} 

\ei


}


%###################################################
\frame{\frametitle{Equivalence of the F and T-tests for one parameter}
%###################################################
\vspace{1em}

{\small
With $M-M_0=1$, the F-test is equivalent to a parameter test for the
  parameter $j$ in question:
\bi
\item Parameter test:
$
T=\frac{\hatbeta_j-\beta_{j0}}{\sqrt{\hat{V}(\hatbeta_j)}}\sim T(n-1-J)
$

\item F-test:
$
T=(n-J-1)\frac{S_0-S}{S} \sim F(1,n-1-J)
$
\itemAsk \colAsk{\scriptsize{Regarding the rhs., show following
    general relation between the 
  student-t and the F(1,d) distributions: 
$
F\sim F(1,d) \ \text{and} \ T \sim T(d) \Rightarrow F=T^2
$
}}

\itemAnswer \colAnswer{\tiny{By definition, Fisher's F is a
    ratio of $\chi^2$ distributions and, by definition of $\chi^2$, a
    ratio of sums of i.i.d. standardnormal distributed random variables:
\bdm
F(1,d)=\chi_1^2/(\chi_d^2/d)=Z^2/(\chi_d^2/d)
\edm
where $Z\sim N(0,1)$ and $\chi_d^2$ and $Z$ are independent from each
other. 
The definition of the student-t distribution is 
$T(d)=Z/\sqrt{\chi_d^2/d}$, so $F(1,d)=T_d^2$.}}

\item One can show (difficult!) that following is exactly valid for
  the lhs.:
 \bdm
 (n-J-1)\frac{S_0-S}{S}
=\frac{(\hatbeta_j-\beta_{j0})^2}{\hat{V}(\hatbeta_j)}
=\frac{(\hatbeta_j-\beta_{j0})^2}{\hat{V}_{jj}}
\edm
where $S_0$ is the (minimum) SSE for the
calibrated restrained model


\ei
}

}


%###################################################
\frame{\frametitle{Rejection region for $\vec{H}_0$: ``$<$''
or ``$\le$'' (interval hypothesis)}
%###################################################

\placebox{0.5}{0.58}{
 \figSimple{0.7\textwidth}{figsRegr/annahmeAblehn_einZweiseit1_eng.png}}

\placebox{0.5}{0.16}{\parbox{\textwidth}{
{\small
\bi
 \item Shown is the density of the test function \emph{if $H^*_0$: ``=''}
   (boundary of $H_0$)
 \pause \item $H_0$ is rejected if the realisation $t\sub{data}$ is in the red
  \emph{extreme region} or \bfblue{rejection region}
% \pause \item If $H_0$ is a point set, $H^*_0=H_0$. If it is an interval,
%  replace the unequality operators $<, \le, >, \ge$ by ``='' to obtain
%  $H^*_0$ 
\ei
}}}

}


%###################################################
\frame{\frametitle{Rejection region for $\vec{H}_0$: ``$>$''
or ``$\ge$'' (interval hypothesis)}
%###################################################

\placebox{0.5}{0.58}{
 \figSimple{0.7\textwidth}{figsRegr/annahmeAblehn_einZweiseit2_eng.png}}

\placebox{0.5}{0.16}{\parbox{\textwidth}{
{\small
\bi
 \item Shown is the density of the test function \emph{if $H^*_0$: ``=''}
   (boundary of $H_0$)
 \pause \item $H_0$ is rejected if the realisation $t\sub{data}$ is in the red
  \emph{extreme region} or \bfblue{rejection region}
% \pause \item If $H_0$ is a point set, $H^*_0=H_0$. If it is an interval,
%  replace the unequality operators $<, \le, >, \ge$ by ``='' to obtain
%  $H^*_0$ 
\ei
}}}

}

%###################################################
\frame{\frametitle{Rejection region for $\vec{H}_0$: ``='' (point hypothesis)
or ``$\le$''}
%###################################################

\placebox{0.5}{0.58}{
 \figSimple{0.7\textwidth}{figsRegr/annahmeAblehn_einZweiseit3_eng.png}}

\placebox{0.5}{0.16}{\parbox{\textwidth}{
{\small
\bi
 \item Shown is the density of the test function \emph{if $H^*_0$}
   (boundary of $H_0$)
 \pause \item If $H_0$ is a point set, $H^*_0=H_0$. If it is an interval,
  replace the unequality operators $<, \le, >, \ge$ by ``='' to obtain
  $H^*_0$ 
\ei
}}}

}

\subsubsection{3.3.4 The $p$-value}

%###################################################
\frame{\frametitle{3.3.4 The $\bfmath{p}$-value}
%###################################################
\bi
\item Obviously, it is not very efficient to test $H_0$ for a fixed
significance level $\alpha$ (one does not know \emph{how significant}
the result really is)
\pause \item Instead, one would like to know the \emph{minimum}
$\alpha$ for rejection, or the \bfblue{$p$-value}. 
\pause \item The most general definition is:
\maindm{p=\text{Prob}(T\in E\sub{data}|H_0^*))} 
where the \emph{extreme region} $E\sub{data}$ contains all
realisations of $T$ that are
further away from $H^*_0$ than $t\sub{data}$.
\vspace{1em}

\bi
\pause \item $p\ge\unit[5]{\%}$: not significant (no star at the parameter
  $\beta$, sometimes a ``+'' if between \unit[5]{\%} and \unit[10]{\%})
\pause \item $p<\unit[5]{\%}$: significant (one star, $\beta^*$)
\pause \item $p<\unit[1]{\%}$: very significant (two star, $\beta^{**}$)
\item $p<0.001$: highly significant (three stars, $\beta^{***}$)
\ei
\ei
}

%###################################################
\frame{\frametitle{Calculating $\bfmath{p}$ for some basic tests}
%###################################################

\visible<1->{\placebox{0.84}{0.75}{
 \figSimple{0.30\textwidth}{figsRegr/p_intervalLeft_eng.png}}}

\visible<2->{\placebox{0.84}{0.47}{
 \figSimple{0.30\textwidth}{figsRegr/p_intervalRight_eng.png}}}

\visible<3->{\placebox{0.84}{0.12}{
 \figSimple{0.30\textwidth}{figsRegr/p_point_eng.png}}}

\placebox{0.34}{0.42}{\parbox{0.65\textwidth}{
\bi
\item Interval test $H_0: \beta \le \beta_0$ or $\beta < \beta_0$
\vspace{-1em}
 \bdm
p=P(T>t\sub{data}|\beta=\beta_0)
=1-F_T(t\sub{data})
\edm\\[2em]
\pause \item Interval test $H_0: \beta \ge \beta_0$ or $\beta > \beta_0$
\vspace{-1em}
\bdm
p=P(T<t\sub{data}|\beta=\beta_0)=F_T(t\sub{data})
\edm\\[2em]
\pause \item Point test $H_0: \beta=\beta_0$ (symmetry of $f_T$ assumed)
\vspace{-1em}
\bdma
p &=& P\big( (T>|t\sub{data}|) \cup (T<-|t\sub{data}|)\big)\\
 &=& 1-F_T(|t\sub{data}|)+F_T(-|t\sub{data}|)\\
 &=& 1-F_T(|t\sub{data}|)+1-F_T(|t\sub{data}|)\\
 &=& 2(1-F_T(|t\sub{data}|))
\edma
\ei
}}

}

%###################################################
\frame{\frametitle{$\bfmath{p}$-values for the null hypotheses of the
    hotel example}
%###################################################

\bdm
y=\beta_0x_0+\beta_1x_1+\beta_2x_2+\epsilon
\edm
where $x_0=1$, $x_1$: proxy for quality [\# stars]; $x_2$: price

\bi
\item $H_{01}:$ point hypothesis $\beta_1=0$ \\ $t\sub{data}=7.49, \ 
 p= 2(1-F_T^{(9)})(|t\sub{data}|)=3.7E-5^{***}$
\item $H_{02}:$ interval hypothesis $\beta_1<0$ \\ $t\sub{data}=7.49,\
p= 1-F_T^{(9)}(t\sub{data})=1.9E-5^{***}$
\item $H_{03}:$ interval hypothesis $\beta_2<-1$ \\ $t\sub{data}=0.24,\
p=1-F_T^{(9)}(t\sub{data})=\unit[40]{\%} $
\item $H_{04}:$ function interval hypothesis
  $\gamma=\beta_1+30\beta_2<0$\\ $t\sub{data}=4.20,\ 
p= 1-F_T^{(9)}(t\sub{data})=\unit[0.12]{\%}^{**}$
\item $H_{05}:$ compound point hypothesis $(\beta_1=30) \cap
  (\beta_2=-1)$\\ $t\sub{data}=11.8,\ 
p=1-F_F^{(2,9)}(t\sub{data})=\unit[0.30]{\%}^{**}$
\ei
}


%###################################################
\frame{\frametitle{Visualization}
%###################################################
\fig{0.82\textwidth}{figsRegr/hotel_f2_hatbeta1_hatbeta2_eng.png}
\vspace{-1em}

{\scriptsize
\bi
\pause \item Simple null hypotheses ($t$-test):
$H_0: \beta_1=45$

\pause \item Correlated null hypotheses ($t$-test), e.g.,
$H_0=H_{04}: \gamma=\beta_1+30\beta_2<0$

\pause \item Compound null hypotheses ($F$-test), e.g.,
$\bullet$: \ $H_0=H_{05}: (\beta_1=30) \cap \beta_2=-1)$,
or
$\triangle$: \ $H_0: (\beta_1=30) \cap (\beta_2=-0.6)$.
\ei
}
}

\subsection{3.4 Dependence on the True Parameter Value}

%###################################################
\frame{\frametitle{3.4 Dependence on the True Parameter Value}
%###################################################
All statistical tests, including the $p$-values, are based on some
\emph{null hypothesis} which is supposed to be \emph{marginally}
fulfilled, $\beta=\beta_0 \in H_0^*$.  What if the true parameter
values take on other values? 
\bi
\item Since regression parameters are continuous, the probability
  $P(H_0^*)=0$ exactly, so the tests and $p$-values \emph{do not
  reflect reality}
\item What happens for other values $\beta \notin H_0^*$? This is
  quantified by following conditional probability called
  \bfdef{statistical power
    function}: 
\maindm{\pi(\beta)=\text{Pr}(\text{test rejected} | \beta)}
\item If $\beta\notin H_0$, then $\pi(\beta)$ indicates the
  \bfdef{statistical power} or \bfdef{specificity} of a test and 
 $1-\pi(\beta)$ its probability for a type-II error
\item If $\beta\in H_0$, then $\pi(\beta)$ is the type-I ($\alpha$)
  error and $1-\pi(\beta)$ the \bfdef{sensitivity} of a test
\ei
}

%###################################################
\frame{\frametitle{Calculating the statistical power function}
%###################################################

\bi
\item If $\beta\neq \beta_0 \in H_0^*$, then the usual test function, e.g.,
$(\hatbeta_j-\beta_{j0})/\sqrt{\hat{V}_{jj}}$ does \emph{no longer} obey
a standard statistical distribution such as standardnormal or
student-t \\[2em]
\pause \item However, $T=(\hatbeta_j-\beta_j)/\sqrt{\hat{V}_{jj}}$ does:
\bdm
T=\frac{\hatbeta_j-\beta_j}{\sqrt{\hat{V}_{jj}}}
 = \frac{\hatbeta_j-\beta_{j0}}{\sqrt{\hat{V}_{jj}}}
 +\frac{\beta_{j0}-\beta_j}{\sqrt{\hat{V}_{jj}}}
 = \frac{\hatbeta_j-\beta_{j0}}{\sqrt{\hat{V}_{jj}}}-\Delta T
\edm \\[2em]
\pause \item The new independent variable is the standardized difference 
$\Delta T=(\beta_j-\beta_{j0})/\sqrt{\hat{V}_{jj}}$


\ei

}

%###################################################
\frame{\frametitle{Example I: Interval test for $<$ and $\le$}
%###################################################

\bdma
\pi^{\le}(\Delta T)
 &=& P\left(\frac{\hatbeta_j-\beta_{j0}}{\sqrt{\hat{V}_{jj}}}
    >t_{1-\alpha}\right)\\
 &=& P(T+\Delta T>t_{1-\alpha}) \\
 &=& P(T>-\Delta T+t_{1-\alpha}) \\
 &=& 1-P(T<-\Delta T+t_{1-\alpha})\\
 & \stackrel{\text{symm.}}{=} & P(T<\Delta T-t_{1-\alpha})\\
 & \stackrel{\text{def distr.}}{=} & \uu{F_T(\Delta T-t_{1-\alpha})}
\edma

\bi
\itemAsk \colAsk{\small{Test this expression by calculating
    $\pi^{\le}(0)$ and $\pi'^{\le}(0)$}}
\itemAnswer \colAnswer{\scriptsize{Just insert $\Delta T=0$:
\bdma
\pi^{\le}(0) &=& F_T(-t_{1-\alpha})\\
  &=& F_T(t_{\alpha})\\
  &\stackrel{\text{def quantile}}{=} & \alpha \ \OK \\
\pi'^{\le}(0) &=& f_T(0)>0 \ \OK
\edma
}}

\ei
}

%###################################################
\frame{\frametitle{Type I and II errors for ``$<$'' or
	``$\le$''-tests as a function\\of the true value relative to
    $H_0$, known variance} 
%###################################################
\placebox{0.5}{0.55}{
  \figSimple{0.90\textwidth}{figsRegr/fehler12art_leGauss_eng.png}}

\placebox{0.5}{0.15}{\parbox{0.9\textwidth}{
{\small
\bi
\item The maximum type-I error probability of $\alpha$ 
occurs if $\beta=\beta_0$, i.e., at the boundary of $H_0$.
\pause \item The maximum type-II error probability of $1-\alpha$
occurs if $\beta$ is just outside of $H_0$.
\ei
}}}

}

%###################################################
\frame{\frametitle{The same for unknown variance, 
df=2 degrees of freedom}
%###################################################

\placebox{0.5}{0.55}{
  \figSimple{0.90\textwidth}{figsRegr/fehler12art_leStudent_2FG_eng.png}}

\placebox{0.5}{0.25}{\parbox{0.9\textwidth}{
{\small
\bi
\item The increase with $\Delta T$ is steeper but $\pi(0)=\alpha$ is
  unchanged 
\ei
}}}

}


%###################################################
\frame{\frametitle{Example II: Interval test for for $>$ and $\ge$}
%###################################################

\bdma
\pi^{\ge}(\Delta T)
 &=& P\left(\frac{\hatbeta_j-\beta_{j0}}{\sqrt{\hat{V}_{jj}}}
    <t_{\alpha}\right)\\
 &=& P(T+\Delta T<t_{\alpha}) \\
 &=& P(T<-\Delta T+t_{\alpha}) \\
 & \stackrel{\text{def distr.}}{=} & \uu{F_T(t_{\alpha})-\Delta T}
\edma

\bi
\itemAsk \colAsk{\small{Test this expression by calculating
    $\pi^{\ge}(0)$ and $\pi'^{\ge}(0)$}}
\itemAnswer \colAnswer{\scriptsize{Just insert $\Delta T=0$:
\bdma
\pi^{\ge}(0) &\stackrel{\text{def quantile}}{=} & \alpha \ \OK \\
\pi'^{\ge}(0) &=& -f_T(0)<0 \ \OK
\edma
}}

\ei
}

%###################################################
\frame{\frametitle{Type I and II errors for ``$>$'' or
	``$\ge$''-tests, known variance}
%###################################################
\vspace{0em}
\fig{0.9\textwidth}{figsRegr/fehler12art_geGauss_eng.png}
\vspace{-1em}
\bi
\item Again, the maximum type I and II error probabilities of
  $\alpha$ and $1-\alpha$, respectively, are obtained if
  the true parameter(s) are at the boundary / very near outside of $H_0$.
\pause \item The maximum type-I error probability is also known as
significance level.
\ei
}

%###################################################
\frame{\frametitle{The same for unknown variance, 
df=2 degrees of freedom}
%###################################################
\vspace{-4em}
\fig{0.9\textwidth}{figsRegr/fehler12art_geStudent_2FG_eng.png}
}


%###################################################
\frame{\frametitle{Example III: Point test for ``=''}
%###################################################
\bdma
\pi\sup{\,eq}(\Delta T) 
 &=& P\left(\left|\frac{\hatbeta_j-\beta_{j0}}{\hatsig_{\hatbeta_j}}\right|
      > t_{1-\alpha/2}\right) \\
&=& P(|T+\Delta T| > t_{1-\alpha/2}) \\
&=& P(T+\Delta T > t_{1-\alpha/2}) + P(T+\Delta T <-t_{1-\alpha/2})\\
&=& 1-P(T+\Delta T \le t_{1-\alpha/2})+P(T+\Delta T <-t_{1-\alpha/2})\\
& \stackrel{\text{def distr.}}{=} & 
 1-F_T(t_{1-\alpha/2}-\Delta T) + F_T(-t_{1-\alpha/2}-\Delta T)\\
& \stackrel{\text{symm.}}{=} & 
 \uu{2-F_T(t_{1-\alpha/2}-\Delta T) - F_T(t_{1-\alpha/2}+\Delta T)} 
\edma


\bi
\itemAsk \colAsk{\small{Test this expression by calculating
    $\pi^{\le}(0)$}}
\itemAnswer \colAnswer{\scriptsize{Just insert $\Delta T=0$:
\bdm
\pi\sup{\,eq}(0) = 2-(1-\alpha/2)-(1-\alpha/2)=\alpha \ \OK
\edm
}}

\ei
}

%###################################################
\frame{\frametitle{Type I and II errors for two-sided (point-)tests \\
	(unkown variance, df=2)}
%###################################################
\vspace{0em}
\fig{0.9\textwidth}{figsRegr/fehler12art_eqStudent_2FG_eng.png}
\vspace{-1em}
\bi
\item Since $H_0$ is a point set here, the type-I error probability is always
  given by $\alpha$ (``significance level'')
\ei
}





\subsection{3.5 Model Selection Strategies}

%###################################################
\frame{\frametitle{3.5 Model Selection Strategies I: Problem Statement}
%###################################################

\placebox{0.50}{0.45}
{\figSimple{0.60\textwidth}{figsRegr/elephantRaisedTrunkPale.jpg}}

\placebox{0.50}{0.45}
{\parbox{0.80\textwidth}{
\bi
\item With every additional parameter, the fit quality in terms of a
  reduced SSE or $\hat{\sigma}^2=\text{SSE}/(n-J-1)$ becomes better
\item However, the risk of overfitting increases. In the words of John
  Neumann: \textit{With four parameters I can fit an elephant, and
    with five I can make him wiggle [its] trunk.}
\item Overfitted models do not validate and can make neither
  statements nor predictions.
\item \red{$\Rightarrow$ we need selection criteria taking care of
  overfitting!}
\ei
}}

}

%###################################################
\frame{\frametitle{Model selection: Some standard criteria}
%###################################################
\bi
\item \bfdef{(1) Adjusted $R^2$:}
\bdm
\bar R^2 = 1 - \, \frac{n-1}{n-J-1}\,\left(1-R^2\right), \quad
R^2=1-\frac{S}{S_0}, 
\edm
\vspace{-1em}
\bdm
S =\text{SSE(full model)}, \quad S_0=\text{SSE(constant-only model)}.
\edm
\vspace{0.1em}

\item \bfdef{(2) Akaike information criterion AIC:}
\begin{equation*}
  \text{AIC} = \ln \hat\sigma\sub{descr}^2 + J\, \frac{2}{n}, 
\end{equation*}

\item \bfdef{(3) Bayes' Information criterion BIC:}
\begin{equation*}
   \text{BIC} = \ln \hat\sigma\sub{descr}^2 + J\, \frac{\ln n}{n} \,.
\end{equation*}
\ei
Notice that the descriptive $\hat{\sigma}\sub{descr}^2=S/n$ instead of
  the unbiased 
$\hat{\sigma}^2=S/(n-1-J)$ is taken in these criteria.
}

%###################################################
\frame{\frametitle{Model selection: Strategy {\`a} la ``Occam's Razor''}
\begin{itemize}
\item Identify $P$ possibly relevant exogenous factors (the constant
  is always included) and calculate
  $\bar{R}^2$, AIC, or BIC for all $2^P$ combinations of these
      factors.
\item The best model is that maximizing $\bar{R}^2$ or minimizing AIC
  or BIC. 
\item Since AIC and also $\bar{R}^2$ penalize complex models (with
  many parameters) too little, the BIC is usually the best bet.
\item Besides the mentioned \emph{brute-force} approach testing all $2^P$
  combinations, there are two standard strategies:
\bi
\item \bfdef{Top-down approach}: Start with all the $P$
  factors. In each round, eliminate a single factor such that the
  reduced model has the highest increase in $\bar{R}^2$ / decrease in
  AIC or BIC. Stop if there is no further improvement.
\item \bfdef{Bottom-up approach}: Start with the constant-only model
  $y=\beta_0$ and successively add factors until there is no further
  improvement.
\ei
\item Standard statistics packages contain all of these strategies. 
\end{itemize}
}

\end{document}

