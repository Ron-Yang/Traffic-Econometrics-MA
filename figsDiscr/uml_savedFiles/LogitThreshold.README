Hallo Andy,

ich habe nun mal explizit geschaut, ob bin\"ares Logit mit echt nichtlinearen
Funktionen, welche Thresholds modellieren, funktioniert. 

Meine Nutzenfunktionen in Abh\"angigkeit der Zeitdifferenz dT in
Minuten war

V1(dT,b0,b1,b2) =b0+b1*(dT-b2*tanh(dT/b2)) 

b0 = alt.-spez. Konstante
b1 = linearen Asymptote
b2 = Plateauweite

Die Funktion modelliert ein horizontales Plateau bei dT=0, welches
fuer dT>>b2 in eine lineare Funktion mit Steigung b1 \"ubergeht.

Meine erfundenen Daten f\"ur 13 Zeitdifferenzen und jeweils 10
unabh\"angige Entscheidungen pro Zeitdifferenz sind wie folgt:

n=10. # n unabh. Entscheidungen fuer jede Zeitdifferenz
dT1=0;   y11=5; # y11=Zahl der Ja-Antworten beim ersten Choice set (y21=n-y11 usw)
dT2=5;   y21=5; # falls y21=3 und y31=7, kein Threshold!
dT3=-5;  y31=5;
dT4=10;  y41=4;
dT5=-10; y51=6;
dT6=15;  y61=2;
dT7=-15; y71=9;
dT8=20;  y81=1;
dT9=-20; y91=9;
dT10=30; y101=0;
dT11=-30; y111=9;
dT12=40; y121=0;
dT13=-40; y131=10;


In Abb. LogitThreshold_lnL.png plottete ich die Log-Likelihoodfunktion
des Logitmodells (ohne Konstanten) f\"ur diese Daten. Ergebnis: Es
gibt eine Schwelle der Breite von etwa 6 Minuten. Der dazugeh\"orige
lineare Teil entspricht einer Zufallsnutzen-Standardabweichung
-1/beta1*pi/sqrt(6) von etwa 9 Minuten, was ebenfalls plausibel
ist. Allerdings ist bei den wenigen Daten die Schwelle nicht
signifikant, da ein Likelihood-Ratio-Test mit dem kalibrierten linearen Modell
V1=b0+b1*dT (und beta1=-0.1, siehe unten an der Grafik) eine nur etwa
um 1.5 geringere Log-Likelihood aufweist. Aber das ist wohl nur eine
Frage der Datenzahl (h\"atte ich 50 statt 10 Entscheidungen pro Zeitdifferenz
genommen, w\"are b2 signifikant).

In Abb. LogitNoThreshold_lnL.png ver\"anderte  ich nur die
Entscheidungsh\"aufigkeiten f\"ur dT=-5 und +5 auf 7 statt 5 bzw 3
statt 5, d.h. ich beseitigte die Indifferenz f\"ur +/- 5 Minuten
Zeitdifferenz. Nun kalibriert das Modell auf beta2=0, also keine
Schwelle, was plausibel ist!

Allerdings ist die log-Likelihhod Funktion i.A. nicht mehr konvex
f\"ur beliebige Parameter, weshalb viele nichtlineare
Optimierungsmethoden versagen. Enthalten die Daten keinen wirklich
linearen Teil in der Nutzenfunktion, gibt es evtl. nicht einmal ein
sinnvolles globales Maximum. Setze ich z.B. y111=y1(dT=-30 min)=10 statt 9,
entspricht das Optimum einer rein nichtlinearen Fkt ohne lineare
Asymptote, da nun beta2 groesser als die maximale Zeitdifferenz
wird. In diesem Fall sind die kalibrierten Werte (au\3erhalb des
Bildes) und das Maximum ist extrem flach. Vielleicht ist das der Grund
f\"ur das Versagen? Allerdings zeigt die Erfahrung, dass solche
Pathologien bei mehr Daten verschwinden.
